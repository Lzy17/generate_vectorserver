<!-- Autogenerated by mlir-tblgen; don't manually edit -->
<h3><code>-cluster-tf-ops-by-host</code>: Cluster the TensorFlow ops by host so that each function only contains ops placed on the same host</h3>
<h3><code>-constant-op-device-assignment</code>: Assign device for tf.Const ops</h3>
<h3><code>-convert-tf-control-flow-to-scf</code>: Convert TensorFlow control flow to SCF.</h3>
<p>This pass can be used for all direct control flow lowerings from the TensorFlow
dialect to the SCF dialect.</p>
<h3><code>-prepare-tpu-computation-for-tf-export</code>: Prepare TPU computation to be legal for export to TensorFlow</h3>
<p>Prepares TPU computation module attached to _TPUCompileMlir op for
TensorFlow graph export by making transformation such as replacing or
removing MLIR or XLA specific attributes that are not legal in TensorFlow
graph.</p>
<h3><code>-tf-batch-matmul-to-tf-einsum</code>: Replace TF BatchMatMul op by TF Einsum op.</h3>
<h3><code>-tf-broadcast-fold</code>: Fold explicit broadcasts into the following operations if they support implicit broadcasting on their operand.</h3>
<h3><code>-tf-canonicalize-compile-and-replicate-attributes</code>: Canonicalize compilation and replication attributes.</h3>
<p>A pass that converts existing compilation and replication attributes into
unified attributes. For example, <code>_tpu_replicate="cluster"</code> in the
following code</p>
<p><code>mlir
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_tpu_replicate = "cluster", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_tpu = true, use_spmd_for_xla_partitioning = false} : () -&gt; ()</code></p>
<p>wll be replaced by <code>_replication_info="cluster"</code> and  <code>_xla_compile_device_type="TPU"</code>.</p>
<p><code>mlir
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_replication_info = "cluster", _xla_compile_device_type = "TPU", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_spmd_for_xla_partitioning = false, use_tpu = true} : () -&gt; ()</code></p>
<p><code>_XlaMustCompile=true</code> in the following code</p>
<p><code>mlir
%outputs_67, %control_68 = tf_executor.island wraps "tf.PartitionedCall"(%arg0, %outputs_0) {_XlaMustCompile = true, _collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\00\0A\07\0A\03TPU\10\02\0A\0E\0A\0ATPU_SYSTEM\10\012\02J\008\01\82\01\05h\01\88\01\01", device = "", executor_type = "", f = @__inference__jit_compiled_convolution_op_1510} : (tensor&lt;4x32x32x8xf32&gt;, tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;</code></p>
<p>will be replaced by <code>_xla_compile_device_type</code>, with its value set to the value of <code>device</code>.</p>
<p><code>mlir
%outputs_67, %control_68 = tf_executor.island wraps "tf.PartitionedCall"(%arg0, %outputs_0) {_collective_manager_ids = [], _read_only_resource_inputs = [], _xla_compile_device_type = "", config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\00\0A\07\0A\03TPU\10\02\0A\0E\0A\0ATPU_SYSTEM\10\012\02J\008\01\82\01\05h\01\88\01\01", device = "", executor_type = "", f = @__inference__jit_compiled_convolution_op_1510} : (tensor&lt;4x32x32x8xf32&gt;, tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;</code></p>
<h3><code>-tf-convert-to-legacy-compile-and-replicate-attributes</code>: Convert unified compilation and replication attributes back to legacy attributes.</h3>
<p>This transformation pass converts unified compilation and replication
attributes (<code>_replication_info</code> and <code>_xla_compile_device_type</code>) into legacy
attributes. This ensures the unified attributes do not get exposed outside
of the MLIR bridge with V1 pipeline in some cases. The pass expects to have
either none or both of the unified attributes present in an op for the
conversion to happen. Otherwise it will fail.</p>
<p>For example, <code>_replication_info="cluster"</code> and
<code>_xla_compile_device_type="TPU"</code> in the following code</p>
<p><code>mlir
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_replication_info = "cluster", _xla_compile_device_type = "TPU", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_spmd_for_xla_partitioning = false, use_tpu = true} : () -&gt; ()</code></p>
<p>wll be replaced by <code>_tpu_replicate="cluster"</code> as follows,</p>
<p><code>mlir
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_tpu_replicate = "cluster", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_tpu = true, use_spmd_for_xla_partitioning = false} : () -&gt; ()</code></p>
<h3><code>-tf-data-optimization</code>: Performs tf.data optimizations</h3>
<h3><code>-tf-decompose-reduce-dataset</code>: Decomposes ReduceDataset op into dataset operations.</h3>
<p>Decomposes ReduceDataset op into a while loop that iterates the dataset and calls
  into the reduction function.  This decomposition is only done if the
  ReduceDataset op is marked for compilation with the _xla_compile_device_type
  attribute.</p>
<p>For example, for the following function the ReduceDataset op:</p>
<p><code>mlir
  func.func @single_state_single_dataset_type_no_arguments(
    %arg0: tensor&lt;!tf_type.variant&gt;,
    %arg1: tensor&lt;i64&gt;
  ) {
    %1 = "tf.ReduceDataset"(%arg0, %arg1) {
      Targuments = [],
      Tstate = [i64], device = "",
      f = @__reduce_func_1, f._tf_data_function = true,
      output_shapes = [#tf_type.shape&lt;&gt;],
      output_types = [i64], use_inter_op_parallelism = true, _xla_compile_device_type="TPU"} :
 (tensor&lt;!tf_type.variant&gt;, tensor&lt;i64&gt;) -&gt; (tensor&lt;i64&gt;)
    func.return
 }</code></p>
<p>with the following reduction function:</p>
<p><code>mlir
 func.func private @__reduce_func_1(%arg0: tensor&lt;i64&gt; {tf._user_specified_name = "args_0"},
   %arg1: tensor&lt;32xf32&gt; {tf._user_specified_name = "args_1"}) -&gt; (tensor&lt;i64&gt;)
   attributes {tf._tf_data_function = true, tf.signature.is_stateful} {
     %0 = "tf.JustPretend"(%arg1) : (tensor&lt;32xf32&gt;) -&gt; (tensor&lt;i64&gt;)
     func.return %0 : tensor&lt;i64&gt;
 }</code></p>
<p>will be transformed into:</p>
<p><code>mlir
 func.func @single_state_single_dataset_type_no_arguments(%arg0: tensor&lt;!tf_type.variant&gt;, %arg1: tensor&lt;i64&gt;) {
  %0 = "tf.AnonymousIteratorV3"() {output_shapes = [#tf_type.shape&lt;32&gt;], output_types = [f32]} : () -&gt; tensor&lt;!tf_type.resource&gt;
  "tf.MakeIterator"(%arg0, %0) : (tensor&lt;!tf_type.variant&gt;, tensor&lt;!tf_type.resource&gt;) -&gt; ()
  %cst = "tf.Const"() {value = dense&lt;true&gt; : tensor&lt;i1&gt;} : () -&gt; tensor&lt;i1&gt;
  %1:2 = "tf.WhileRegion"(%cst, %arg1) ({
  ^bb0(%arg2: tensor&lt;i1&gt;, %arg3: tensor&lt;i64&gt;):
    "tf.Yield"(%arg2) : (tensor&lt;i1&gt;) -&gt; ()
  }, {
  ^bb0(%arg2: tensor&lt;i1&gt;, %arg3: tensor&lt;i64&gt;):
    %2 = "tf.IteratorGetNextAsOptional"(%0) {output_shapes = [#tf_type.shape&lt;32&gt;], output_types = [f32]} : (tensor&lt;!tf_type.resource&gt;) -&gt; tensor&lt;!tf_type.variant&gt;
    %3 = "tf.OptionalHasValue"(%2) : (tensor&lt;!tf_type.variant&gt;) -&gt; tensor&lt;i1&gt;
    %4 = "tf.IfRegion"(%3) ({
      %5 = "tf.OptionalGetValue"(%2) : (tensor&lt;!tf_type.variant&gt;) -&gt; tensor&lt;32xf32&gt;
      %6 = func.call @__reduce_func_1(%arg3, %5) {_xla_compile_device_type = "TPU"} : (tensor&lt;i64&gt;, tensor&lt;32xf32&gt;) -&gt; tensor&lt;i64&gt;
      "tf.Yield"(%6) : (tensor&lt;i64&gt;) -&gt; ()
    }, {
      "tf.Yield"(%arg3) : (tensor&lt;i64&gt;) -&gt; ()
    }) {_lower_using_switch_merge = true, is_stateless = false} : (tensor&lt;i1&gt;) -&gt; tensor&lt;i64&gt;
    "tf.Yield"(%3, %4) : (tensor&lt;i1&gt;, tensor&lt;i64&gt;) -&gt; ()
  }) {_lower_using_switch_merge = true, is_stateless = false, parallel_iterations = 10 : i64} : (tensor&lt;i1&gt;, tensor&lt;i64&gt;) -&gt; (tensor&lt;i1&gt;, tensor&lt;i64&gt;)
  return
}</code></p>
<h3><code>-tf-device-assignment-by-func-attr</code>: Device assignment in TF dialect using the device specified in the function attribute.</h3>
<h3><code>-tf-device-cluster-formation</code>: Form clusters from instructions assigned to same device</h3>
<p>Clusters operations with the same device assignment id. For each
cluster, creates a "tf_device.device_launch" op with a Region containing the
ops in each cluster and replaces the ops with the new launch op.</p>
<p>For example, given the following program:</p>
<p><code>mlir
  %2 = "tf.A"(%arg0) : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  %3 = "tf.B"(%2) {device = "tpu0"} : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  %4 = "tf.C"(%2, %3) {device = "tpu0"} : (tensor&lt;?xi32&gt;, tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  %5 = "tf.D"(%4) : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;</code></p>
<p>After the pass, we will have:</p>
<p><code>mlir
  %0 = "tf.A"(%arg0) : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  %1 = "tf_device.launch"() ( {
    %3 = "tf.B"(%0) : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;
    %4 = "tf.C"(%0, %3) : (tensor&lt;?xi32&gt;, tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;
    tf_device.return %4 : tensor&lt;?xi32&gt;
  }) {device = "tpu0"} : () -&gt; tensor&lt;?xi32&gt;
  %2 = "tf.D"(%1) : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  return %2 : tensor&lt;?xi32&gt;</code></p>
<h3><code>-tf-device-cluster-outlining</code>: Outlines regions of tf_device.cluster operations</h3>
<p>This pass outlines the body of a <code>tf_device.cluster</code> into a function and
replaces the <code>tf_device.cluster</code> op with an equivalent <code>tf_device.cluster_func</code>
op. Implicit operands will be captured and materialized as explicit arguments to
the newly created functions and associated <code>tf_device.cluster_func</code> ops.</p>
<p>For example, the following:</p>
<p><code>mlir
func @computation(%arg0: tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt; {
  %cluster = "tf_device.cluster"() ( {
    %identity = "tf.Identity"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %identity : tensor&lt;i32&gt;
  }) : () -&gt; (tensor&lt;i32&gt;)
  return %cluster : tensor&lt;i32&gt;
}</code></p>
<p>will be transformed into:</p>
<p>```mlir
func @computation(%arg0: tensor<i32>) -&gt; tensor<i32> {
  %cluster = "tf_device.cluster_func"(%arg0) {func = @_func} : (tensor<i32>) -&gt; tensor<i32>
  return %cluster : tensor<i32>
}</p>
<p>func @_func(%arg0: tensor<i32>) -&gt; tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -&gt; tensor<i32>
  return %identity : tensor<i32>
}
```</p>
<h4>Options</h4>
<p><code>-globally-unique-func-names : If true, the pass adds extra identifiers to make function names globally unique within a process, not just within a module.</code></p>
<h3><code>-tf-device-constant-sinking</code>: Sinks constants implicitly captured in a tf_device.cluster region.</h3>
<p>This pass sinks implicitly captured constants (<code>tf.Const</code> ops) used by and into
a <code>tf_device.cluster</code> region. Performing this prior to outlining will reduce the
number of arguments of the outlined function.</p>
<p>For example, the following:</p>
<p><code>mlir
func @cluster() -&gt; tensor&lt;i32&gt; {
  %const = "tf.Const"() {value = dense&lt;0&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %cluster = "tf_device.cluster"() ( {
    %identity = "tf.Identity"(%const) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %identity : tensor&lt;i32&gt;
  }) : () -&gt; (tensor&lt;i32&gt;)
  return %cluster : tensor&lt;i32&gt;
}</code></p>
<p>will be transformed into:</p>
<p><code>mlir
func @cluster() -&gt; tensor&lt;i32&gt; {
  %cluster = "tf_device.cluster"() ( {
    %const = "tf.Const"() {value = dense&lt;0&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
    %identity = "tf.Identity"(%const) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %identity : tensor&lt;i32&gt;
  }) : () -&gt; (tensor&lt;i32&gt;)
  return %cluster : tensor&lt;i32&gt;
}</code></p>
<h3><code>-tf-device-convert-launch-func-to-tf-call</code>: Rewrites tf_device::LaunchFuncOp to TF::PartitionedCallOp</h3>
<p>This pass converts tf_device::LaunchFuncOp into an equivalent
TF::PartitionedCallOp so that it can be exported to TensorFlow GraphDef.</p>
<h3><code>-tf-device-index-selector</code>: Fold tf.DeviceIndex to constant.</h3>
<h3><code>-tf-device-launch-outlining</code>: Outlines regions of tf_device.launch operations</h3>
<p>This pass outlines the body of a <code>tf_device.launch</code> into a function and
replaces the <code>tf_device.launch</code> op with an equivalent <code>tf_device.launch_func</code>
op. Implicit operands will be captured and materialized as explicit arguments to
the newly created functions and associated <code>tf_device.launch_func</code> ops. The
<code>device</code> attribute from the <code>launch</code> op is transferred to <code>launch_func</code>.</p>
<p>For example, the following:</p>
<p><code>mlir
func @computation(%arg0: tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt; {
  %launch = "tf_device.launch"() ( {
    %identity = "tf.Identity"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %identity : tensor&lt;i32&gt;
  }) {device = "some_device"} : () -&gt; (tensor&lt;i32&gt;)
  return %launch : tensor&lt;i32&gt;
}</code></p>
<p>will be transformed into:</p>
<p>```mlir
func @computation(%arg0: tensor<i32>) -&gt; tensor<i32> {
  %launch = "tf_device.launch_func"(%arg0) {device = "some_device", func = @_func} : (tensor<i32>) -&gt; tensor<i32>
  return %launch : tensor<i32>
}</p>
<p>func @_func(%arg0: tensor<i32>) -&gt; tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -&gt; tensor<i32>
  return %identity : tensor<i32>
}
```</p>
<h4>Options</h4>
<p><code>-globally-unique-func-names : If true, the pass adds extra identifiers to make function names globally unique within a process, not just within a module.</code></p>
<h3><code>-tf-device-mark-input-output-aliases</code>: Marks device cluster inputs-output pairs that read/write to the same variable as aliases</h3>
<p>This pass analyzes the inputs and outputs to device cluster and marks those
input-output pairs as aliases (using <code>tf.aliasing_output</code> attribute) which read
and write to the same resource. This aliasing information can then be propagated
to XLA compiler for input/output buffer space optimizations.</p>
<h3><code>-tf-drop-while-shape-invariant</code>: Drop <code>shape_invariant</code> attribute from While/WhileRegion ops.</h3>
<p>Drop <code>shape_invariant</code> attribute from tf.While and tf.WhileRegion op. This
would allow shape inference pass to further refine operand/result shapes of
these ops. This is only safe to do when compiling to XLA.</p>
<h3><code>-tf-drop-while-shape-invariant-in-device-cluster</code>: Drop <code>shape_invariant</code> attribute from While/WhileRegion ops inside device cluster.</h3>
<p>Drop <code>shape_invariant</code> attribute from tf.While and tf.WhileRegion op only
inside device cluster. This would allow shape inference pass to further
refine operand/result shapes of these ops. This is only safe to do when
compiling to XLA.</p>
<h3><code>-tf-einsum</code>: Transform Einsum to other TF Ops for the supported variants</h3>
<h3><code>-tf-embedding-pipelining</code>: Rewrite graph for embedding pipelining</h3>
<p>For architectures that support accelerated embedding lookups, this pass will
rewrite the graph to use pipelining for better device utilization.</p>
<h3><code>-tf-executor-break-up-islands</code>: Transform from TF control dialect to TF executor dialect.</h3>
<h3><code>-tf-executor-check-control-dependencies</code>: Checks control dependencies</h3>
<p>This pass analyzes control dependencies between islands and warns about
dependencies that are not explainable by side effects of the involved ops.
More precisely, for every minimal unexplainable control dependency path
we emit op warnings for all involved ops. The pass does not report
intermediate dummy ops for grouping control dependencies (Identity, NoOp),
unless they are part of an unexplainable path between other ops.
This pass is useful to understand control dependency conservatism for a
given MLIR module.</p>
<p>For example, the following function
<code>mlir
func.func @path_with_intermediate_ops(
  %arg0: tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;,
  %arg1: tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;,
  %arg2: tensor&lt;f32&gt;) -&gt; () {
  tf_executor.graph {
    %island1 = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg2) : (tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;, tensor&lt;f32&gt;) -&gt; ()
    %island2 = tf_executor.island(%island1) wraps "tf.NoOp"() : () -&gt; ()
    %island3 = tf_executor.island(%island2) wraps "tf.NoOp"() : () -&gt; ()
    %island4 = tf_executor.island(%island3) wraps "tf.AssignVariableOp"(%arg1, %arg2) : (tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;, tensor&lt;f32&gt;) -&gt; ()
    tf_executor.fetch
  }
  func.return
}</code>
produces the following warnings
<code>mlir
  6:45: warning: unexpected control dependency path: path 0, node 0 (source)
  %island1 = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg2) : (tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;, tensor&lt;f32&gt;) -&gt; ()
                                      ^
  6:45: note: see current operation: %control = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg2) : (tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;, tensor&lt;f32&gt;) -&gt; ()
  7:55: warning: unexpected control dependency path: path 0, node 1 (intermediate)
  %island2 = tf_executor.island(%island1) wraps "tf.NoOp"() : () -&gt; ()
                                                ^
  7:55: note: see current operation: %control_0 = tf_executor.island(%control) wraps "tf.NoOp"() : () -&gt; ()
  8:55: warning: unexpected control dependency path: path 0, node 2 (intermediate)
  %island3 = tf_executor.island(%island2) wraps "tf.NoOp"() : () -&gt; ()
                                                ^
  8:55: note: see current operation: %control_1 = tf_executor.island(%control_0) wraps "tf.NoOp"() : () -&gt; ()
  9:55: warning: unexpected control dependency path: path 0, node 3 (target)
  %island4 = tf_executor.island(%island3) wraps "tf.AssignVariableOp"(%arg1, %arg2) : (tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;, tensor&lt;f32&gt;) -&gt; ()
                                                ^
  9:55: note: see current operation: %control_2 = tf_executor.island(%control_1) wraps "tf.AssignVariableOp"(%arg1, %arg2) : (tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;, tensor&lt;f32&gt;) -&gt; ()</code>
because the first and last <code>AssignVariableOp</code>s access different resources
and therefore should be independent. Note that the <code>NoOp</code>s are considered
as intermediate ops for control dependency grouping.</p>
<h3><code>-tf-executor-convert-control-to-data-outputs</code>: Chain control outputs of while loop body</h3>
<p>This pass converts the control outputs of a while loop body function to data
outputs. Thus, inter iteration control dependencies are transformed to
data dependencies. Since data dependencies can express which particular
operations in the while loop body are dependent on which inputs, it captures
inter iteration parallelism in while loop. Control dependencies on the other
hand create a barrier at the end of while loop body thus blocking any
parallelism across iterations.</p>
<p>For example, the following while loop body has a <code>%barrier</code> at the end.
Although there is no data/control dependency between <code>tf.AssignVariableOp</code>
for <code>%arg0</code> to <code>tf.AssignVariableOp</code> for <code>%arg1</code> across any iteration, the
while loop body has a control barrier (<code>%barrier</code>) at the end which forces
a dependency and the two assign variable ops must wait for each other to
complete before starting the next iteration. Transforming these control
outputs to data outputs removes the dependency between the two assign
variable ops, thus allowing them to run in parallel across iterations.</p>
<p>Before:</p>
<p><code>mlir
!tf_res = type tensor&lt;!tf_type.resource&lt;tensor&lt;f32&gt;&gt;&gt;
func @while_body(%arg0: !tf_res, %arg1: !tf_res, %arg2: tensor&lt;f32&gt;, %arg3: tensor&lt;f32&gt;) -&gt; (!tf_res, !tf_res, tensor&lt;f32&gt;, tensor&lt;f32&gt;) {
  %graph:4 = tf_executor.graph {
    %assign_0_control = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg2) : (!tf_res, tensor&lt;f32&gt;) -&gt; ()
    %assign_1_control = tf_executor.island wraps "tf.AssignVariableOp"(%arg1, %arg3) : (!tf_res, tensor&lt;f32&gt;) -&gt; ()
    %add_out, %add_control = tf_executor.island wraps "tf.Add"(%arg2, %arg3) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
    %mul_out, %mul_control = tf_executor.island wraps "tf.Mul"(%arg2, %arg3) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
    %barrier = tf_executor.island(%assign_0_control, %assign_1_control, %add_control, %mul_control) wraps "tf.NoOp"() : () -&gt; ()
    tf_executor.fetch %arg0, %arg1, %add_out, %mul_out, %barrier : !tf_res, !tf_res, tensor&lt;f32&gt;, tensor&lt;f32&gt;, !tf_executor.control
  }
  return %graph#0, %graph#1, %graph#2, %graph#3 : !tf_res, !tf_res, tensor&lt;f32&gt;, tensor&lt;f32&gt;
}</code></p>
<p>After:</p>
<p><code>mlir
func @while_body(%arg0: !tf_res, %arg1: !tf_res, %arg2: tensor&lt;f32&gt;, %arg3: tensor&lt;f32&gt;, %chain_0: tensor&lt;i32&gt;, %chain_1: tensor&lt;i32&gt;) -&gt; (!tf_res, !tf_res, tensor&lt;f32&gt;, tensor&lt;f32&gt;, tensor&lt;i32&gt;, tensor&lt;i32&gt;) {
  %graph:6 = tf_executor.graph {
    %_, %chain_0_src = tf_executor.island wraps "tf.Identity"(%chain_0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %_, %chain_1_src = tf_executor.island wraps "tf.Identity"(%chain_1) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %assign_0_control = tf_executor.island(%chain_0_src) wraps "tf.AssignVariableOp"(%arg0, %arg2) : (!tf_res, tensor&lt;f32&gt;) -&gt; ()
    %assign_1_control = tf_executor.island(%chain_1_src) wraps "tf.AssignVariableOp"(%arg1, %arg3) : (!tf_res, tensor&lt;f32&gt;) -&gt; ()
    %add_out, %add_control = tf_executor.island wraps "tf.Add"(%arg2, %arg3) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
    %mul_out, %mul_control = tf_executor.island wraps "tf.Mul"(%arg2, %arg3) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
    %chain_0_sink, %_ = tf_executor.island(%assign_0_control) wraps "tf.Identity"(%chain_0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %chain_1_sink, %_ = tf_executor.island(%assign_1_control) wraps "tf.Identity"(%chain_1) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_executor.fetch %arg0, %arg1, %add_out, %mul_out, %chain_0_sink, %chain_1_sink : !tf_res, !tf_res, tensor&lt;f32&gt;, tensor&lt;f32&gt;, tensor&lt;i32&gt;, tensor&lt;i32&gt;
  }
  return %graph#0, %graph#1, %graph#2, %graph#3, %graph#4, %graph#5 : !tf_res, !tf_res, tensor&lt;f32&gt;, tensor&lt;f32&gt;, tensor&lt;i32&gt;, tensor&lt;i32&gt;
}</code></p>
<h3><code>-tf-executor-graph-pruning</code>: Prunes unreachable ops in a tf_executor.graph</h3>
<p>This pass removes ops from a <code>tf_executor.graph</code> that are not transitively, via
data or control dependencies, connected to the associated <code>tf_executor.fetch</code>
op. The order of ops will be preserved. Functions named <code>main</code> with no
<code>tf.entry_function</code> attribute will not be pruned, as such graphs/functions may
have been imported from a V1 TensorFlow graph, where feeds/fetches/targets are
not provided at certain stages of IR transformation (e.g. pre-placement).</p>
<p>Option <code>ops-to-preserve</code> allows to specify ops that should not be pruned,
regardless of their reachability.</p>
<p>For example, the following:</p>
<p><code>mlir
func @graph(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt; {
  %graph = tf_executor.graph {
    %transitive_reachable_data:2 = tf_executor.island wraps "tf.Identity"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %reachable_data:2 = tf_executor.island wraps "tf.Identity"(%transitive_reachable_data#0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %unreachable_data:2 = tf_executor.island wraps "tf.Const"() {value = dense&lt;0&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
    %transitive_reachable_control = tf_executor.island wraps "tf.NoOp"() : () -&gt; ()
    %reachable_control = tf_executor.island(%transitive_reachable_control) wraps "tf.NoOp"() : () -&gt; ()
    %unreachable_control = tf_executor.island wraps "tf.NoOp"() : () -&gt; tensor&lt;i32&gt;
    tf_executor.fetch %reachable_data#0, %reachable_control : tensor&lt;i32&gt;, !tf_executor.control
  }
  return %graph : tensor&lt;i32&gt;
}</code></p>
<p>will be transformed into:</p>
<p><code>mlir
func @graph(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt; {
  %graph = tf_executor.graph {
    %transitive_reachable_data:2 = tf_executor.island wraps "tf.Identity"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %reachable_data:2 = tf_executor.island wraps "tf.Identity"(%transitive_reachable_data#0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %transitive_reachable_control = tf_executor.island wraps "tf.NoOp"() : () -&gt; ()
    %reachable_control = tf_executor.island(%transitive_reachable_control) wraps "tf.NoOp"() : () -&gt; ()
    tf_executor.fetch %reachable_data#0, %reachable_control : tensor&lt;i32&gt;, !tf_executor.control
  }
  return %graph : tensor&lt;i32&gt;
}</code></p>
<h4>Options</h4>
<p><code>-ops-to-preserve : Comma separated list of ops that should not be pruned regardless of reachability</code></p>
<h3><code>-tf-executor-island-coarsening</code>: Walks tf_executor::GraphOp and merges individual tf_executor::IslandOps.</h3>
<p>This pass performs whole graph analysis for a graph encapsulated into tf_executor::GraphOp.
The analysis identifies all IslandOps within the graph which could be merged together.
The goal is to merge as many islands as possible.
Once analysis is completed, the pass merges all IslandOps in a single scan.</p>
<p>For example given the following program with two disjunct islands:</p>
<p><code>mlir
  func @test(%arg0 : tensor&lt;i1&gt;) -&gt; tensor&lt;f32&gt; {
    %0 = tf_executor.graph {
      %1:2 = tf_executor.island {
        %3 = "tf.opA"(%arg0) : (tensor&lt;i1&gt;) -&gt; tensor&lt;i1&gt;
        tf_executor.yield %3 : tensor&lt;i1&gt;
      }
      %2:2 = tf_executor.island(%1#1) {
        %4 = "tf.opB"() : () -&gt; tensor&lt;f32&gt;
        tf_executor.yield %4 : tensor&lt;f32&gt;
      }
      tf_executor.fetch %2#0 : tensor&lt;f32&gt;
    }
    return %0 : tensor&lt;f32&gt;
  }</code></p>
<p>After running this pass, the two islands are merged:</p>
<p><code>mlir
  func @test(%arg0: tensor&lt;i1&gt;) -&gt; tensor&lt;f32&gt; {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island {
        %1 = "tf.opA"(%arg0) : (tensor&lt;i1&gt;) -&gt; tensor&lt;i1&gt;
        %2 = "tf.opB"() : () -&gt; tensor&lt;f32&gt;
        tf_executor.yield %2 : tensor&lt;f32&gt;
      }
      tf_executor.fetch %outputs : tensor&lt;f32&gt;
    }
    return %0 : tensor&lt;f32&gt;
  }</code></p>
<h3><code>-tf-executor-split-into-island-per-op</code>: Transform from TF control dialect to TF executor dialect.</h3>
<p>Splits an island with multiple ops into multiple islands (one per op). Does
not create any control dependencies between new islands, and does not
propagate control dependencies that potentially existed between the old
islands into the new islands. Maintains existing data dependencies between
ops wrapped by the new islands.</p>
<p>Example: original program:</p>
<p><code>mlir
    func.func @dangling_print(%arg0: tensor&lt;*xi32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; (tensor&lt;*xi32&gt;, tensor&lt;*xi32&gt;) {
      %graph:2 = tf_executor.graph {
        %island1:3 = tf_executor.island {
          %add1 = "tf.Add"(%arg0, %arg1) : (tensor&lt;*xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;*xi32&gt;
          %add2 = "tf.Add"(%add1, %arg1) : (tensor&lt;*xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;*xi32&gt;
          %res = "tf.Print"(%add2) { message = "add result" } : (tensor&lt;*xi32&gt;) -&gt; (tensor&lt;*xi32&gt;)
          tf_executor.yield %add1, %add2 : tensor&lt;*xi32&gt;, tensor&lt;*xi32&gt;
        }
        tf_executor.fetch %island1#0, %island1#1 : tensor&lt;*xi32&gt;, tensor&lt;*xi32&gt;
      }
      func.return %graph#0, %graph#1 : tensor&lt;*xi32&gt;, tensor&lt;*xi32&gt;
    }</code></p>
<p>will be converted by this pass into:</p>
<p><code>mlir
    func.func @dangling_print(%arg0: tensor&lt;*xi32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; (tensor&lt;*xi32&gt;, tensor&lt;*xi32&gt;) {
      %0:2 = tf_executor.graph {
        %outputs, %control = tf_executor.island wraps "tf.Add"(%arg0, %arg1) : (tensor&lt;*xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;*xi32&gt;
        %outputs_0, %control_1 = tf_executor.island wraps "tf.Add"(%outputs, %arg1) : (tensor&lt;*xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;*xi32&gt;
        %outputs_2, %control_3 = tf_executor.island wraps "tf.Print"(%outputs_0) {message = "add result"} : (tensor&lt;*xi32&gt;) -&gt; tensor&lt;*xi32&gt;
        tf_executor.fetch %outputs, %outputs_0 : tensor&lt;*xi32&gt;, tensor&lt;*xi32&gt;
      }
      return %0#0, %0#1 : tensor&lt;*xi32&gt;, tensor&lt;*xi32&gt;
    }</code></p>
<h3><code>-tf-executor-to-functional-conversion</code>: Lifts tf_executor.island inner ops from a tf_executor.graph</h3>
<p>This pass converts tf_executor.graphs consisting of only tf_executor.islands and
a tf_executor.fetch into a sea of nodes consisting of TensorFlow Dialect ops by
lifting such ops out of a tf_executor.graph's tf_executor.islands. If V1 control
flow ops are present in a tf_executor.graph, an error will be returned.</p>
<p>For example, the following:</p>
<p><code>mlir
func @my_fn(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;) {
  %graph_results:2 = tf_executor.graph {
    %island_0_result, %island_0_control = tf_executor.island {
      %identity = "tf.Identity"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
      tf_executor.yield %identity : tensor&lt;i32&gt;
    }
    %island_1_result, %island_1_control = tf_executor.island {
      %identity_n:2 = "tf.IdentityN"(%arg1, %island_0_result) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;)
      tf_executor.yield %identity_n#0
    }
    tf_executor.fetch %island_0_result, %island_1_result : tensor&lt;i32&gt;, tensor&lt;i32&gt;
  }
  return %graph_results#0, %graph_results#1 : tensor&lt;i32&gt;, tensor&lt;i32&gt;
}</code></p>
<p>will be transformed into:</p>
<p><code>mlir
func @my_fn(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;) {
  %identity = "tf.Identity"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
  %identity_n:2 = "tf.IdentityN"(%arg1, %identity) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;)
  return %identity, %identity_n#0 : tensor&lt;i32&gt;, tensor&lt;i32&gt;
}</code></p>
<h3><code>-tf-executor-tpu-v1-island-coarsening</code>: Merges TPU clusters IslandOps, intended for V1 compatibility mode</h3>
<p>This pass is a variant of ExecutorIslandCoarseningPass that is limited to
TPU-annotated operations and intended to preserve backward compatibility with
TFv1.</p>
<h3><code>-tf-executor-tpu-v1-island-inlining</code>: Inline calls to the nested TPU module.</h3>
<p>This pass inlines the islands calling into the nested module that was
outlined, thus reversing the effect of the
<code>-tf-executor-tpu-v1-island-outlining</code> pass.</p>
<p>For example, the following:
<code>mlir
module {
  func @foo(%arg0: tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt; {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.PartitionedCall"(%arg0) {f = @_tpu_v1_compat_outlined::@bar} : (tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
      tf_executor.fetch %outputs : tensor&lt;f32&gt;
    }
    return %0 : tensor&lt;f32&gt;
  }
  module @_tpu_v1_compat_outlined {
    func nested @bar(%arg0: tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt; {
      %0 = "tf.opA"(%arg0) : (tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
      return %0 : tensor&lt;f32&gt;
    }
  }
}</code></p>
<p>will be transformed into:</p>
<p><code>mlir
module  {
  func @foo(%arg0: tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt; {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island {
        %1 = "tf.opA"(%arg0) : (tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
        tf_executor.yield %1 : tensor&lt;f32&gt;
      }
      tf_executor.fetch %outputs : tensor&lt;f32&gt;
    }
    return %0 : tensor&lt;f32&gt;
  }
}</code></p>
<h3><code>-tf-executor-tpu-v1-island-outlining</code>: Outline TPU clusters from island into a nested module, so it can be processed like a V2 module, intended for V1 compatibility mode</h3>
<p>Extract the islands containing a TPU cluster computation into an outlined
function in a nested module. This will allow to run the usual bridge on this
nested module which now exhibits a more friendly "V2-like" structure.
This is only intended for V1 compatibility mode where the bridge runs without
feed/fetches on session create/extend.</p>
<p>So given e.g.</p>
<p><code>mlir
  func @test() -&gt; tensor&lt;i32&gt; {
    %0 = tf_executor.graph {
      %output, %control = tf_executor.island {
        ...
        tf_executor.yield %result : tensor&lt;i32&gt;
      }
      tf_executor.fetch %output : tensor&lt;i32&gt;
    }
    return %0
  }</code></p>
<p>This pass will create an additional function containing the code in
tf_executor.island:</p>
<p><code>mlir
  func nested @_tpu_v1_compat_outlined_func0() -&gt; tensor&lt;i32&gt; {
    ...
  }</code></p>
<p>and will then replace the island with the wrapped call:</p>
<p><code>mlir
  func @test() -&gt; tensor&lt;i32&gt; {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.PartitionedCall"() {
          f = @_tpu_v1_compat_outlined::@_tpu_v1_compat_outlined_func0
      } : () -&gt; tensor&lt;i32&gt;
      tf_executor.fetch %outputs : tensor&lt;i32&gt;
    }
    return %0 : tensor&lt;i32&gt;
  }</code></p>
<h3><code>-tf-executor-update-control-dependencies</code>: Computes and applies all necessary control dependencies based on side effect analysis.</h3>
<p>This pass is intended to run after the split_into_island_per_op
pass. That pass splits up multi-op islands into multiple individual islands
wrapping a single op without applying any control deps between the new
islands. So, this pass is needed in order to make preservation of the
semantic ordering relationships between ops as determined by side effect
analysis explicit in the IR.</p>
<p>Example: original program:</p>
<p><code>mlir
    func.func @example(%arg0: tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;, %arg1: tensor&lt;32xf32&gt;) -&gt; (tensor&lt;32xf32&gt;) {
      %graph = tf_executor.graph {
        %read0, %read0_control = tf_executor.island wraps "tf.ReadVariableOp"(%arg0) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;) -&gt; tensor&lt;32xf32&gt;
        %assign0_control = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg1) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;, tensor&lt;32xf32&gt;) -&gt; ()
        %read1, %read1_control = tf_executor.island wraps "tf.ReadVariableOp"(%arg0) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;) -&gt; tensor&lt;32xf32&gt;
        %print, %print_control = tf_executor.island wraps "tf.Print"(%read1) { message = "read1 value" } : (tensor&lt;32xf32&gt;) -&gt; (tensor&lt;32xf32&gt;)
        tf_executor.fetch %read1#0 : tensor&lt;32xf32&gt;
      }
      func.return %graph : tensor&lt;32xf32&gt;
    }</code></p>
<p>will be converted by this pass into:</p>
<p><code>mlir
    func.func @example(%arg0: tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;, %arg1: tensor&lt;32xf32&gt;) -&gt; tensor&lt;32xf32&gt; {
      %0 = tf_executor.graph {
        %read0, %read0_control = tf_executor.island wraps "tf.ReadVariableOp"(%arg0) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;) -&gt; tensor&lt;32xf32&gt;
        %assign0_control = tf_executor.island(%read0_control) wraps "tf.AssignVariableOp"(%arg0, %arg1) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;, tensor&lt;32xf32&gt;) -&gt; ()
        %read1, %read1_control = tf_executor.island(%assign0_control) wraps "tf.ReadVariableOp"(%arg0) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;) -&gt; tensor&lt;32xf32&gt;
        %print, %print_control = tf_executor.island(%read1_control) wraps "tf.Print"(%read1) {message = "read1 value"} : (tensor&lt;32xf32&gt;) -&gt; tensor&lt;32xf32&gt;
        tf_executor.fetch %read1, %print_control : tensor&lt;32xf32&gt;, !tf_executor.control
      }
      return %0 : tensor&lt;32xf32&gt;
    }</code></p>
<h3><code>-tf-extract-head-tail-outside-compilation</code>: Extracts head or tail outside compilation to separate host launches before/after device cluster.</h3>
<p>This pass extracts a CPU computation cluster with <code>_xla_outside_compilation</code>
annotation from the head or tail of a Device cluster.</p>
<p>For example:</p>
<p><code>mlir
  %cluster = "tf_device.cluster"() ( {
    %a = "tf.A"(%arg0) {_xla_outside_compilation = "cluster1"} : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %b = "tf.B"(%a) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    %c = "tf.C"(%b) {_xla_outside_compilation = "cluster1"} : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %c : tensor&lt;i32&gt;
  }) {num_cores_per_replica = 1, step_marker_location = "", padding_map = [], topology = "", device_assignment = []} : () -&gt; tensor&lt;i32&gt;
  return %cluster : tensor&lt;i32&gt;</code></p>
<p>becomes:</p>
<p>```mlir
%0 = "tf_device.launch"() ( {
  %3 = "tf.A"(%arg0) : (tensor<i32>) -&gt; tensor<i32>
  tf_device.return %3 : tensor<i32>
}) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -&gt; tensor<i32>
%1 = "tf_device.cluster"() ( {
  %3 = "tf.B"(%0) : (tensor<i32>) -&gt; tensor<i32>
  tf_device.return %3 : tensor<i32>
}) {device_assignment = [], num_cores_per_replica = 1 : i64, padding_map = [], step_marker_location = "", topology = ""} : () -&gt; tensor<i32>
%2 = "tf_device.launch"() ( {
  %3 = "tf.C"(%1) : (tensor<i32>) -&gt; tensor<i32>
  tf_device.return %3 : tensor<i32>
}) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -&gt; tensor<i32>
return %2 : tensor<i32></p>
<p>```</p>
<h3><code>-tf-extract-outside-compilation</code>: Extracts device outside compilation computation to a separate tf_device.parallel_execute region.</h3>
<p>This pass extracts a CPU computation cluster with <code>_xla_outside_compilation</code>
annotation, which denotes ops that should be run on CPU/host, from a device cluster.
Each outside compilation cluster is moved to
a tf_device.parallel_execute region. The device cluster is also moved to a
tf_device.parallel_execute region. Communication ops between device and host are
added to pass inputs/outputs to/from the outside compiled region.</p>
<p>For example, the following tf_device.cluster with an op marked for <code>xla_outside_compilation</code>:</p>
<p><code>mlir
func @outside_compilation() -&gt; tensor&lt;f32&gt; {
  %0 = "tf_device.cluster"() ( {
    %1 = "tf.Const"() {_xla_outside_compilation = "0", value = dense&lt;1.0&gt; : tensor&lt;f32&gt;} : () -&gt; (tensor&lt;f32&gt;)
    %2 = "tf.Identity"(%1) {_xla_outside_compilation = "0"} : (tensor&lt;f32&gt;) -&gt; (tensor&lt;f32&gt;)
    %3 = "tf.AddV2"(%1, %2) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; (tensor&lt;f32&gt;)
    tf_device.return %3 : tensor&lt;f32&gt;
  }) {num_cores_per_replica = 1, topology =  "", device_assignment =  []} : () -&gt; tensor&lt;f32&gt;
  return %0 : tensor&lt;f32&gt;
}</code></p>
<p>will become a tf_device.parallel_execute op with a CPU/host region and
a tf_device.cluster with communication ops to send data to/from device/host:</p>
<p><code>mlir
func @outside_compilation() -&gt; tensor&lt;f32&gt; {
  %0 = "tf_device.parallel_execute"() ( {
    "tf_device.launch"() ( {
      %1 = "tf._TPUCompileMlirPlaceholderProgramKey"() : () -&gt; tensor&lt;3x!tf_type.string&gt;
      %2 = "tf._XlaRecvAtHost"(%1) {device_ordinal = 0 : i64, key = "host_compute_channel_0_0_args"} : (tensor&lt;3x!tf_type.string&gt;) -&gt; tensor&lt;f32&gt;
      %3 = "tf.Identity"(%2) : (tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
      "tf._XlaSendFromHost"(%3, %1) {device_ordinal = 0 : i64, key = "host_compute_channel_0_0_retvals"} : (tensor&lt;f32&gt;, tensor&lt;3x!tf_type.string&gt;) -&gt; ()
      tf_device.return
    }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -&gt; ()
    tf_device.return
  },  {
    %1 = "tf_device.cluster"() ( {
      %2 = "tf.Const"() {value = dense&lt;1.000000e+00&gt; : tensor&lt;f32&gt;} : () -&gt; tensor&lt;f32&gt;
      %3 = "tf._XlaHostComputeMlir"(%2) {recv_key = "host_compute_channel_0_0_retvals", send_key = "host_compute_channel_0_0_args", tpu_core = 0 : i64} : (tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
      %4 = "tf.AddV2"(%2, %3) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
      tf_device.return %4 : tensor&lt;f32&gt;
    }) {device_assignment = [], num_cores_per_replica = 1 : i64, topology = ""} : () -&gt; tensor&lt;f32&gt;
    tf_device.return %1 : tensor&lt;f32&gt;
  }) : () -&gt; tensor&lt;f32&gt;
  return %0 : tensor&lt;f32&gt;
}</code></p>
<h3><code>-tf-extract-tpu-copy-with-dynamic-shape-op</code>: Extract the TPUCopyWithDynamicShapeOp out of the host launch and place it on device launch</h3>
<p>This pass looks for TPUCopyWithDynamicShapeOp which wraps in a
<code>tf_device.launch</code> with host device attribute. It extracts the ops and wrap
them in <code>tf_device.launch</code> with tpu device attribute so that ops can be
run on TPU instead of CPU while still being compiled on host.</p>
<h3><code>-tf-functional-control-flow-to-cfg</code>: Transform functional control flow Ops to MLIR Control Form Graph (CFG) form</h3>
<h3><code>-tf-functional-control-flow-to-regions</code>: Transforms functional control flow operations to their region-based counterparts</h3>
<p>This pass transforms functional control flow operations in the TensorFlow
dialect to their region-based counterparts, i.e., <code>tf.If</code> is transformed to
<code>tf.IfRegion</code> and <code>tf.While</code> is transformed to <code>tf.WhileRegion</code>.</p>
<p>For example, this functional operation</p>
<p><code>mlir
  %0 = "tf.If"(%arg0, %arg1) {
    then_branch = @then_branch_func, else_branch = @else_branch_func, is_stateless = false
  } : (tensor&lt;i1&gt;, tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;</code></p>
<p>will be transformed into this region-based operation</p>
<p><code>mlir
    %0 = "tf.IfRegion"(%arg0) ( {
      %1 = call @then_branch_func(%arg1) : (tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;
      "tf.Yield"(%1) : (tensor&lt;*xf32&gt;) -&gt; ()
    },  {
      %1 = call @else_branch_func(%arg1) : (tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;
      "tf.Yield"(%1) : (tensor&lt;*xf32&gt;) -&gt; ()
    }) {is_stateless = false} : (tensor&lt;i1&gt;) -&gt; tensor&lt;*xf32&gt;</code></p>
<h3><code>-tf-functional-to-executor-conversion</code>: Transform from func op to TF executor dialect.</h3>
<h3><code>-tf-fused-kernel-matcher</code>: Matches computations corresponding to optimized fused kernels</h3>
<h3><code>-tf-gpu-op-fusion</code>: Fusion optimization for GPU targets</h3>
<p>This pass is performing fusion specific to GPU targets. This is an ad-hoc
pass for now, but should be integrated with some notion of "target" in the
MLIR pipeline in the future.</p>
<h3><code>-tf-group-by-dialect</code>: Groups ops into functions that only contain one dialect.</h3>
<p>Factors operations into subroutines such that all functions only
contain a single dialect. Which of the dialects are allowed in the
"top" function is configurable.</p>
<p>For example, the code
  x.a()
  x.b()
  %c = y.c()
  x.d(%c)
would be transformed into something like
  call @x_1()
  %c = call @y_1()
  call @x_2(%c)
with @x_1, @x_2 and @y_1 filled in.</p>
<h3><code>-tf-guarantee-all-funcs-one-use</code>: Guarantee all FuncOp's have only a single use.</h3>
<h3><code>-tf-hoist-loop-invariant</code>: Hoists loop invariant ops to the outside of the loop</h3>
<p>Hoists loop invariant to the outside of the loop. The pass is similar to
   LoopInvariantCodeMotion pass, but it also hoists ReadVariableOps,
   if the variable is read only.</p>
<p>For example, the following pseudo MLIR code (types are left out for
   brevity)
   <code>mlir
     func.func @hoist_loop_invariant(%arg0, %arg1) {
%var = "tf.VarHandleOp"() {container="", shared_name="var_name", device = "/device:CPU:0"}
       %results:2 = "tf.WhileRegion"(%arg0, %arg1) ({
       ^bb0(%arg2, %arg3):
         %0 = "tf.OpA"() {is_stateless = true}
         "tf.Yield"(%0)
       }, {
       ^bb0(%arg2, %arg3):
  %1 = "tf.ReadVariableOp"(%var)
         %2 = "tf.OpB"(%1) {is_stateless = true}
         %3 = "tf.OpC"(%arg2, %2) {is_stateless = true}
         %4 = "tf.OpD"(%arg3, %2) {is_stateless = true}
         "tf.Yield"(%3, %4)
       }) {is_stateless = true}
       return %results#0, %results#1
     }</code>
   would be transformed to
   <code>mlir
    func.func @hoist_loop_invariant(%arg0, %arg1) {
%var = "tf.VarHandleOp"() {container="", shared_name="var_name", device = "/device:CPU:0"}
%1 = "tf.ReadVariableOp"(%var)
       %2 = "tf.OpB"(%1) {is_stateless = true}
       %results:2 = "tf.WhileRegion"(%arg0, %arg1) ({
       ^bb0(%arg2, %arg3):
         %0 = "tf.OpA"() {is_stateless = true}
         "tf.Yield"(%0)
       }, {
       ^bb0(%arg2, %arg3):
         %3 = "tf.OpC"(%arg2, %2) {is_stateless = true}
         %4 = "tf.OpD"(%arg3, %2) {is_stateless = true}
         "tf.Yield"(%3, %4)
       }) {is_stateless = true}
       return %results#0, %results#1
     }</code>
   The <code>tf.ReadVariableOp</code> and <code>tf.OpB</code> can be hoisted to the outside of
   the loop.</p>
<h3><code>-tf-hoist-replicate-invariant-resource-writes</code>: Hoists writes to replicate invariant resource variables.</h3>
<p>This pass hoists replicate invariant resource variable writes outside
tf_device.replicate op. These may have been inserted by other passes such as
resource op lifting. However, if the resource variable is not replicated, writes
to such variables for each replica are redundant and can be replaced by writing
a single value from first replica.</p>
<p>The benefit of this optimization is reduced memory requirement on host. For
multiple writes (one from each replica) to such variables, the host would
allocate buffer space to receive the device output from all replicas, which is
not required. We can use the output of first replica in such cases.</p>
<h3><code>-tf-init-text-file-to-import</code>: convert InitializeTableFromTextFileV2 ops to LookupTableImportV2Op to remove the dependency on asset files</h3>
<h4>Options</h4>
<p><code>-tf-saved-model-dir : Directory containing the model exported as a TensorFlow SavedModel. If your model is not based on the TensorFlow SavedModel, use an empty value.</code></p>
<h3><code>-tf-layout-assignment</code>: Layout assignment pass.</h3>
<h4>Options</h4>
<p><code>-force-data-format : Force data format for all layout sensitive ops.</code></p>
<h3><code>-tf-legalize-hlo</code>: Legalize from HLO to the TF dialect</h3>
<h3><code>-tf-localize-var-handles</code>: Creates VarHandleOps next to the operations that use them.</h3>
<p>Creates VarHandleOps right next to the operations that use them, one
per operation.
This is useful for transformations that only end up with a few small
snippets of remaining TF code, and wish for those snippets to be
self-contained.
For example, this would transform</p>
<p>"tf_saved_model.global_tensor"() { sym_name = "v" ... }
func @f(%arg0 {tf_saved_model.bound_input = @v}) {
  %1 = "tf.ReadVariableOp"(%arg0)
  ...
}</p>
<p>to</p>
<p>func @f(%arg0 {tf_saved_model.bound_input = @v}) {
  %0 = "tf.VarHandleOp"(sym_name = "v")
  %1 = "tf.ReadVariableOp"(%0)
  ...
}</p>
<p>Note that this pass might leave behind unused values
(like e.g. %arg0 in the example above), which can later be
pruned using DCE.</p>
<h3><code>-tf-lower-quantized</code>: Lowers ops that require quantized input or output.</h3>
<p>This pass rewrites all ops that have at least one input or output that must
be a quantized type to ops whose inputs and outputs allow non-quantized
types. Examples of quantized types are TF_Qint8 or TF_Quint8.</p>
<p>An example is TF_DequantizeOp, which converts a quantized type to a float.
This op is rewritten to generic ops that perform the scale and shift
and can operate on non-quantized types.</p>
<p>Currently, TF_DequantizeOp is the only op with a lowering that falls
in this category. When more lowerings are added (e.g. QuantizeV2Op),
they should be added to this pass.</p>
<h3><code>-tf-mark-ops-for-outside-compilation</code>: Marks ops in device cluster for outside compilation if they are unsupported on device.</h3>
<p>This pass marks unsupported ops in a device cluster with
<code>_xla_outside_compilation</code> attribute so the operations will run on the host
instead of the device. Unsupported ops are ops that can not be code
generated to run on the device for the cluster including:</p>
<ol>
<li>String operations on TPUs.</li>
<li>Operations that don't have a kernel defined for the device.</li>
</ol>
<p>This pass is conservative in that it will mark all ops for outside compilation
that can not be compiled for the device.  Exceptions for this are added for ops
that will be rewritten or decomposed before compiling on device.</p>
<p>For example, tf_device.cluster op with an unsupported op, tf.UnsupportedOp:</p>
<p><code>mlir
func @unsupported_op() -&gt; tensor&lt;i32&gt; {
  %0 = "tf_device.cluster"() ( {
    %1 = "tf.UnsupportedOp"() : () -&gt; tensor&lt;i32&gt;
    %2 = "tf.Identity"(%1) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %2 : tensor&lt;i32&gt;
  }) {allow_soft_placement = true, num_cores_per_replica = 1, topology =  "", device_assignment =  []} : () -&gt; tensor&lt;i32&gt;
  return %0 : tensor&lt;i32&gt;
}</code></p>
<p>will mark tf.UnsupportedOp with <code>_xla_outside_compilation</code> attribute:</p>
<p><code>mlir
func @unsupported_op() -&gt; tensor&lt;i32&gt; {
  %0 = "tf_device.cluster"() ( {
    %1 = "tf.UnsupportedOp"() {_xla_outside_compilation = "auto0"} : () -&gt; tensor&lt;i32&gt;
    %2 = "tf.Identity"(%1) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %2 : tensor&lt;i32&gt;
  }) {allow_soft_placement = true, device_assignment = [], num_cores_per_replica = 1 : i64, topology = ""} : () -&gt; tensor&lt;i32&gt;
  return %0 : tensor&lt;i32&gt;
}</code></p>
<h3><code>-tf-materialize-passthrough-op</code>: Materialize the MlirPassthroughOp by replacing it with the MLIR module attached as an attribute</h3>
<p>A pass that replaces MlirPassthrough ops with the code they have in
their <code>mlir_module</code> string attribute.</p>
<h3><code>-tf-merge-control-flow</code>: Merges IfRegion ops together with a common predicate.</h3>
<p>This pass merges IfRegion ops together if they have the same predicate and it
is safe to do so (there are no intermediate dependencies, they are in the
same block, etc).</p>
<p>For example:</p>
<p><code>mlir
"tf.IfRegion"(%0) ( {
  %2 = "tf.A"() : () -&gt; (tensor&lt;f32&gt;)
  "tf.Yield"() : () -&gt; ()
  }, {
  "tf.Yield"() : () -&gt; ()
 }) { is_stateless = true } : (tensor&lt;i1&gt;) -&gt; ()
"tf.IfRegion"(%0) ( {
  %2 = "tf.B"() : () -&gt; (tensor&lt;f32&gt;)
  "tf.Yield"() : () -&gt; ()
  }, {
  "tf.Yield"() : () -&gt; ()
  }) { is_stateless = true } : (tensor&lt;i1&gt;) -&gt; ()</code></p>
<p>Would be transformed to:</p>
<p><code>mlir
"tf.IfRegion"(%0) ( {
  %2 = "tf.A"() : () -&gt; (tensor&lt;f32&gt;)
  %3 = "tf.B"() : () -&gt; (tensor&lt;f32&gt;)
  "tf.Yield"() : () -&gt; ()
  }, {
  "tf.Yield"() : () -&gt; ()
  }) { is_stateless = true } : (tensor&lt;i1&gt;) -&gt; ()</code></p>
<h3><code>-tf-move-transposes</code>: Move transposes pass.</h3>
<h4>Options</h4>
<p><code>-fold-transpose-in-ops : Whether to fold transposes in ops which can support folding.
-direction             : Move transposes to the beginning or the end of the block where they are defined.</code></p>
<h3><code>-tf-name-anonymous-iterators</code>: Converts anonymous iterators to named iterators</h3>
<p>This converts AnonymousIterator ops to Iterator, thus giving them a name.
For example, this will convert
  %0 = "tf.AnonymousIteratorV3"() {...}
to
  %0 = "tf.Iterator"() {shared_name = "_iterator1", ...}</p>
<h3><code>-tf-optimize</code>: Optimize TensorFlow module</h3>
<h3><code>-tf-order-by-dialect</code>: Reorders ops so ops of the same dialect are next to each other.</h3>
<p>Performs a reordering of ops so that
  (a) ops of the same dialect are next to each other
  (b) order within a dialect is preserved
.
For example, this would transform
  %a = "x.f"()
  %b = "y.f"(%a)
  %c = "x.f"(%a)
to
  %a = "x.f"()
  %c = "x.f"(%a)
  %b = "y.f"(%a)
so that the two "x" dialect instructions are next to each other.</p>
<h3><code>-tf-outside-compiled-to-host-launch</code>: Wraps each op with the _xla_outside_compiled attribute in a separate tf_device.launch on replicated host device.</h3>
<p>This pass wraps ops with the same <code>_xla_outside_compilation</code>
attribute value in a tf_device.launch op with host device assignment.</p>
<p>A simple example:</p>
<p><code>mlir
  "tf_device.cluster"() ( {
    "tf.A"()
    "tf.B"() {_xla_outside_compilation = "cluster1"}
    "tf.C"()
    tf_device.return
  }) {num_cores_per_replica = 1, topology =  "", device_assignment =  []}</code></p>
<p>Would become the following ops (unimportant attribute, type are omitted):</p>
<p><code>mlir
  "tf_device.cluster"() ( {
    "tf.A"()
    "tf_device.launch"() {
      "tf.B"() {_xla_outside_compilation = "cluster1"}
      tf_device.return
    } {device = "TPU_REPLICATED_HOST_0"} : () -&gt; ()
    "tf.C"()
    tf_device.return
  }) {num_cores_per_replica = 1, topology =  "", device_assignment =  []}</code></p>
<h3><code>-tf-parallel-execute-to-islands</code>: Lowers device parallel_execute to executor islands</h3>
<h4>Options</h4>
<p><code>-legacy-graph-export : Determines whether or not this pass should execute logic that is reserved for the legacy graph export pipeline to maintain expected invariants. In the case of this pass, that means manually propagating controls to lifted parallel execute regions to the graph fetch to ensure the ops execute.</code></p>
<h3><code>-tf-promote-resources-to-args</code>: Promote resources reads/writes to function inputs/outputs.</h3>
<p>This pass promotes resource accesses in function(s) (by default, the main)
to input arguments and outputs of the function(s).</p>
<p>Two types of resources are supported:
(1) A function argument of TF::ResourceType type (this pass).
(2) A VarHandleOp in the function (tf-promote-var-handles-to-args).</p>
<p>After the pass,</p>
<p>. The function will have an input argument for each resource that is
   already provided as an input argument or is read. The type of the input
   argument will become the shape of the value represented by the resource.</p>
<p>. The function will have an output for each resource that is written. The
   type of the output will become the shape of the resource.</p>
<p>The information of variable identification and input-output alising is
recorded as named attributes of the input argument or output:</p>
<p>. 'tf.resource_name' matches 'shared_name' of VarHandleOp, which represents
   the identifier of the corresponding resource. This attribute is added to
   an input argument if the initial value of the resource is read, or to the
   output if the initial value is not read.</p>
<p>. 'tf.aliasing_output' is the index of the function output that is an alias
   of the input argument. This attribute is added only to the input argument
   when the initial value of the corresponding resource is read, and the
   resource is written later.</p>
<p>Assumption of this pass:
 . Compound resource operations have already been decomposed.
 . Dead functions have already been removed, as resource arguments in dead
   functions can cause the pass to fail.</p>
<h4>Options</h4>
<p><code>-functions : Comma separated list of functions whose resources read/writes should be promoted to function inputs/outputs.</code></p>
<h3><code>-tf-promote-var-handles-to-args</code>: Promote tf.VarHandleOps to function arguments.</h3>
<p>See joint description in promote resources to args.### <code>-tf-readonly-references-to-resources</code>: Convert readonly reference variables to resource variables.</p>
<h3><code>-tf-region-control-flow-to-functional</code>: Transforms region-based control flow operations to their functional counterparts</h3>
<p>This pass transforms region-based control flow operations in the TensorFlow
dialect to their functional counterparts, i.e., <code>tf.IfRegion</code> is transformed to
<code>tf.If</code> and <code>tf.WhileRegion</code> is transformed to <code>tf.While</code>.</p>
<p>For example, this region-based operation</p>
<p><code>mlir
    %0 = "tf.IfRegion"(%arg0) ( {
      %1 = call @then_branch_func(%arg1) : (tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;
      "tf.Yield"(%1) : (tensor&lt;*xf32&gt;) -&gt; ()
    },  {
      %1 = call @else_branch_func(%arg1) : (tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;
      "tf.Yield"(%1) : (tensor&lt;*xf32&gt;) -&gt; ()
    }) {is_stateless = false} : (tensor&lt;i1&gt;) -&gt; tensor&lt;*xf32&gt;</code></p>
<p>will be transformed into this functional operation</p>
<p><code>mlir
  %0 = "tf.If"(%arg0, %arg1) {
    then_branch = @then_branch_func, else_branch = @else_branch_func, is_stateless = false
  } : (tensor&lt;i1&gt;, tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;</code></p>
<h3><code>-tf-remove-unused-arguments</code>: Removes unused args from private functions &amp; their callers.</h3>
<p>Removes arguments from functions that aren't used in the function
body, outside of returns. Also adjusts the callers of said functions.</p>
<p>For example, the code
  func.func @f(%arg0, %arg1) {
    SomeOpThatUsesArg0(%arg0)
    return %arg0
  }
  ...
  call @x_1(x, y)</p>
<p>would be transformed into
  func.func @f(%arg0) {
    return %arg0
  }
  ...
  call @x_1(x)</p>
<p>Note that, in the above example, both args would be removed if there
wasn't the "SomeOpThatUsesArg0(%arg0)" line.</p>
<h3><code>-tf-remove-unused-while-results</code>: Removes unused results from tf.WhileRegion ops</h3>
<p>Removes unused results from <code>tf.WhileRegion</code> ops along with the defining
ops in the body, if it is safe to do so.
Currently, the pass detects results with following properties:
- the result is unused outside of the <code>tf.WhileRegion</code> op
- the defining op of the result in the body can be safely removed
- the operand corresponding to the result is not used by any other op in
  the condition or body (in particular, there must not be intermediate
  pass-through ops like <code>tf.Identity</code>)</p>
<p>For example, the following pseudo MLIR code (types are left out for
brevity)
<code>mlir
  func.func @remove_first_result(%arg0, %arg1) {
    %0:2 = "tf.WhileRegion"(%arg0, %arg1) ({
    ^bb0(%arg2, %arg3):
      %1 = "tf.OpA"() {is_stateless = true}
      "tf.Yield"(%1)
    }, {
    ^bb0(%arg2, %arg3):
      %1 = "tf.OpB"(%arg2) {is_stateless = true}
      %2 = "tf.OpC"(%arg3) {is_stateless = true}
      "tf.Yield"(%1, %2)
    }) {is_stateless = true}
    return %0#1
  }</code>
would be transformed to
<code>mlir
  func.func @remove_first_result(%arg0, %arg1) {
    %0 = "tf.WhileRegion"(%arg1) ({
    ^bb0(%arg3):
      %1 = "tf.OpA"() {is_stateless = true}
      "tf.Yield"(%1)
    }, {
    ^bb0(%arg3):
      %1 = "tf.OpC"(%arg3) {is_stateless = true}
      "tf.Yield"(%1)
    }) {is_stateless = true}
    return %0
  }</code>
(the first result can be removed along with its defining op <code>tf.OpB</code>).</p>
<h3><code>-tf-replica-id-to-device-ordinal</code>: Set device ordinal with replica id</h3>
<p>This pass sets the device ordinal attribute of the ops using the replica id
attribute. This is run immediately after the replica_to_island pass which
sets the replica id attribute of these ops. Note for single chip usecase,
the pass will check if there is one op and sets the device ordinal attribute
to be zero.</p>
<h3><code>-tf-replicate-invariant-op-hoisting</code>: Hoists replicate invariant operations out of replicate</h3>
<p>This pass looks for replicate invariant ops in a <code>tf_device.replicate</code> op
region and hoists them out. It also makes <code>tf.Shape</code> ops replicate invariant
if possible. This currently updates or replaces <code>tf.Shape</code> ops of replicated
arguments, either tensors or resources.</p>
<p>The primary benefit of the pass is to hoist <code>num_replicas</code> <code>_TPUCompile</code>s
into a single <code>_TPUCompile</code>.</p>
<p>This pass assumes that when a <code>tf.Shape</code> directly inputs from <code>replicate</code>
params, then it is the same shape across replicas.</p>
<p>For example, the following</p>
<p><code>mlir
tf_device.replicate([%0, %1] as %ri: tensor&lt;*xi32&gt;) {n = 2 : i32} {
  %2 = "tf.Shape"(%ri) : (tensor&lt;*xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  tf_device.return
}</code></p>
<p>gets converted to</p>
<p><code>mlir
tf_device.replicate([%0, %1] as %ri: tensor&lt;*xi32&gt;) {n = 2 : i32} {
  %2 = "tf.Shape"(%0) : (tensor&lt;*xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  tf_device.return
}</code></p>
<p>and for resource variables the following</p>
<p><code>mlir
tf_device.replicate([%0, %1] as %ri: tensor&lt;*x!tf_type.resource&gt;) {n = 2 : i32} {
  %2 = "tf.ReadVariableOp"(%ri) : tensor&lt;*x!tf_type.resource&gt; -&gt; tensor&lt;*xi32&gt;
  %3 = "tf.Shape"(%2) : (tensor&lt;*xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  tf_device.return
}</code></p>
<p>gets converted to</p>
<p><code>mlir
tf_device.replicate([%0, %1] as %ri: tensor&lt;*x!tf_type.resource&gt;) {n = 2 : i32} {
  %2 = "tf.ReadVariableOp"(%ri) : tensor&lt;*x!tf_type.resource&gt; -&gt; tensor&lt;*xi32&gt;
  %3 = "tf.VariableShape"(%0) : (tensor&lt;*x!tf_type.resource&gt;) -&gt; tensor&lt;?xi32&gt;
  tf_device.return
}</code></p>
<h3><code>-tf-replicate-tensor-list-init-ops</code>: Replicate TensorList init ops for correct shape assignments in shape inference</h3>
<p>If we pass same TensorList to a while op as multiple arguments or just use
the same TensorList at multiple places and assign different
TensorListSetItem to elements of TensorList, the shape inference is then
unable to identify the Shape of these args and thus the input TensorList
shape is unidentifiable.
All of these args are supposed to be independent and not related to original
creation of TensorList.</p>
<p>This pass will create multiple instances of TensorList for each arg of the
while op and each use and thus there will be not a conflict in resolving the
shape of these different inputs.</p>
<h3><code>-tf-replicate-to-island</code>: Lowers device replicate to executor islands</h3>
<h4>Options</h4>
<p><code>-legacy-graph-export : Determines whether or not this pass should execute logic that is reserved for the legacy graph export pipeline to maintain expected invariants. In the case of this pass, that means manually propagating controls to lifted parallel execute regions to the graph fetch to ensure the ops execute, as well as determining whether or not the islands created by this pass should be split after the replicated ops have been lifted.</code></p>
<h3><code>-tf-resource-device-inference</code>: Propagates the device attribute on resources from callers to callees.</h3>
<p>A pass that propagates device assignment of resources on a module. It
performs in-function propagation, as well as cross-function propagation from
callers to callees.</p>
<p>This pass changes the module by adding "tf.device" attribute to function
arguments and adding "device" attribute to TF ops.</p>
<p>For example, given the function</p>
<p>```mlir
  !tf_res = type tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;</p>
<p>func @test(%arg0: !tf_res {tf.device = "/TPU:0"}) {
    tf_executor.graph {
      %control = tf_executor.island {
        %id0 = "tf.Identity"(%arg0) : (!tf_res) -&gt; !tf_res
        tf_executor.yield
      }
      tf_executor.fetch %control : !tf_executor.control
    }
    return
  }
```</p>
<p>Observe how the op inside the island obtains a <code>/TPU:0</code> device assignment:</p>
<p>```mlir
  !tf_res = type tensor&lt;*x!tf_type.resource&lt;tensor&lt;32xf32&gt;&gt;&gt;</p>
<p>func @test(%arg0: !tf_res {tf.device = "/TPU:0"}) {
    tf_executor.graph {
      %control = tf_executor.island {
        %0 = "tf.Identity"(%arg0) {device = "/TPU:0"} : (!tf_res) -&gt; !tf_res
        tf_executor.yield
      }
      tf_executor.fetch %control : !tf_executor.control
    }
    return
  }
```</p>
<h3><code>-tf-rewrite-tpu-embedding-ops</code>: Rewrites TPU embedding send/recv ops by adding TPU embedding deduplication data</h3>
<h3><code>-tf-shape-inference</code>: Shape inference on TF dialect and ops implementing InferTypeOpInterface</h3>
<p>Fixed point shape refinement pass that utilizes the shape functions
registered on ops using the InferTypeOpInterface as well as by bridging to
the TensorFlow op registry's shape functions. This is an interprocedural
pass that propagates information across function calls/control flow
operations where possible (the GuaranteeAllFuncsOneUsePass is often run
before this pass to enable more propagation opportunities). It refines
both the outermost element type of tensors as well as the nested component
type (e.g., for tensor lists).</p>
<p>During shape refinement this pass may insert additional cast operations as
well as fold some constant shape computations to enable more exact shape
inference. Therefore it does do some mutation of the graph. Constant folding
required to produce more exact shapes is also performed but these values
are only kept in the context rather than the ops folded/IR mutated.</p>
<h4>Options</h4>
<p><code>-max-iterations : Maximum shape inference iterations</code></p>
<h3><code>-tf-simple-device-assignment</code>: Simple device assignment in TF dialect.</h3>
<p>Assigns the default device to all ops that have an empty (or
nonexistent) device attribute.</p>
<p>For example, if we have the code</p>
<p><code>mlir
  %0 = "tf.Const"() {value = dense&lt;[[42.0]]&gt; : tensor&lt;1x1xf32&gt;} : () -&gt; tensor&lt;1x1xf32&gt;
  %1 = "tf.Const"() {device = "", value = dense&lt;[[42.0]]&gt; : tensor&lt;1x1xf32&gt;} : () -&gt; tensor&lt;1x1xf32&gt;
  %2 = "tf.Const"() {device = "baz", value = dense&lt;[[42.0]]&gt; : tensor&lt;1x1xf32&gt;} : () -&gt; tensor&lt;1x1xf32&gt;</code></p>
<p>then running this pass with 'default-device=foobar', we get:</p>
<p><code>mlir
  %0 = "tf.Const"() {device = "foobar" value = dense&lt;[[42.0]]&gt; : tensor&lt;1x1xf32&gt;} : () -&gt; tensor&lt;1x1xf32&gt;
  %1 = "tf.Const"() {device = "foobar", value = dense&lt;[[42.0]]&gt; : tensor&lt;1x1xf32&gt;} : () -&gt; tensor&lt;1x1xf32&gt;
  %2 = "tf.Const"() {device = "baz", value = dense&lt;[[42.0]]&gt; : tensor&lt;1x1xf32&gt;} : () -&gt; tensor&lt;1x1xf32&gt;</code></p>
<h4>Options</h4>
<p><code>-default-device : The default device to assign.</code></p>
<h3><code>-tf-stack-ops-decomposition</code>: Decompose stack operations into local variable operations. Needs static shapes.</h3>
<p>A pass that converts stack operations to tensor operations and read/assign
ops on local variables. A later resource lifting pass can further remove the
local variables.</p>
<p>This pass requires that the full shape of the stack can be inferred: 1) the
maximum size needs to be a constant and 2) a push op can be found with a
known shape, and all push ops need to have the same shape.</p>
<p>A stack creation op "tf.StackV2" will be turned in to two zero-initialized
variables, for the buffer and current size. Each push will be turned into
<code>mlir
  %old_val = "tf.ReadVariableOp"(%buffer)
  %old_size = "tf.ReadVariableOp"(%size)
  %offsets = "tf.ConcatV2"(%old_size, %other_dims_0s, %const0)
  %new_val = "tf.XlaDynamicUpdateSlice"(%old_val, %push_val, %offsets)
  "tf.AssignVariableOp"(%buffer, %new_val)
  %new_size = "tf.AddV2"(%old_size, %const1)
  "tf.AssignVariableOp"(%size, %new_size)</code></p>
<p>and each pop will be turned into</p>
<p><code>mlir
  %old_val = "tf.ReadVariableOp"(%buffer)
  %old_size = "tf.ReadVariableOp"(%size)
  %new_size = "tf.Sub"(%old_size, %const1)
  %offsets = "tf.ConcatV2"(%old_size, %other_dims_0s, %const0)
  %slice = "tf.Slice"(%old_val, %offsets, %slice_size_const)
  %pop_result = "tf.Reshape"(%slice, %elem_size_const)
  "tf.AssignVariableOp"(%size, %new_size)</code></p>
<p>The pass also works across control flow and functional calls.</p>
<h3><code>-tf-strip-noinline-attribute</code>: Strip the tf._noinline attribute from top-level functions.</h3>
<h3><code>-tf-strip-tf-attributes</code>: Removes TF specific attributes</h3>
<p>Removes attributes that are TF specific (start with "tf.") or that
have a value from the TF dialect. Useful after legalizing TF graphs
to other dialects, to remove any TF remnants.</p>
<h3><code>-tf-tensor-array-ops-decomposition</code>: Decompose tensor array operations into local variable operations.</h3>
<p>A pass that converts tensor array operations to tensor operations and
read/assign ops on local variables. A later resource lifting pass can further
remove the local variables.</p>
<p>This pass requires that the full shape of the tensor array can be inferred:
1) the size needs to be a constant, 2) it specifies the full element shape,
or that can be inferred from a later write, and 3) all elements have the same
shape.</p>
<h3><code>-tf-tensor-device-copy</code>: Fold the tf.Identity op and the tf.IdentityN op if the op has the same device as its operand</h3>
<h3><code>-tf-tensor-list-ops-decomposition</code>: Decomposes TensorList operations into generic operations on tensors.</h3>
<p>This pass rewrites TensorList operations into generic and non-mutating
operations on tensors. This results in operations that can be legalized to XLA.</p>
<p>The list is converted to a single large tensor that includes all list elements,
with a new first dimension for the list index. List update operations are
converted to operations that create a new tensor representing the list.</p>
<p>In the current implementation, the resulting operations are statically shaped,
which means it must be possible to infer a bound on the full shape of the
TensorList. That is, the <code>element_shape</code> and <code>num_elements</code> arguments to a
tensor list creation op are constant.</p>
<p>A tensor list creation op <code>tf.EmptyTensorList</code>/<code>tf.TensorListReserve</code> will be
turned in to a zero-initialized buffer, and the size is initialized to 0
for <code>tf.EmptyTensorList</code> or the specified size for <code>tf.TensorListReserve</code>.
Each push will be turned into <code>tf.XlaDynamicUpdateSlice</code> with the incremented
size, and each pop will be turned into a <code>tf.Slice</code> and a copy of the buffer
with decremented size. Each <code>tf.TensorListSetItem</code> will be turned into a
<code>tf.XlaDynamicUpdateSlice</code> with unchanged size, and each <code>tf.TensorListGetItem</code>
will be rewritten to a <code>tf.Slice</code>.</p>
<p>The pass also works across control flow and functional calls.</p>
<p>For example, the TensorList ops in the following function:</p>
<p><code>mlir
func @main(%arg0: tensor&lt;8x4xf32&gt;) {
  %elem_shape = "tf.Const"() {value = dense&lt;[8, 4]&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %max_size = "tf.Const"() {value = dense&lt;10&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %tl = "tf.EmptyTensorList"(%elem_shape, %max_size) : (tensor&lt;2xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;!tf_type.variant&lt;tensor&lt;8x4xf32&gt;&gt;&gt;
  %push = "tf.TensorListPushBack"(%tl, %arg0) : (tensor&lt;!tf_type.variant&lt;tensor&lt;8x4xf32&gt;&gt;&gt;, tensor&lt;8x4xf32&gt;) -&gt; tensor&lt;!tf_type.variant&lt;tensor&lt;8x4xf32&gt;&gt;&gt;
  return
}</code></p>
<p>will be transformed to:</p>
<p><code>mlir
func @main(%arg0: tensor&lt;8x4xf32&gt;) {
  // EmptyTensorList lowering
  %emptyi = "tf.Const"() {value = dense&lt;0&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %emptyf = "tf.Cast"(%emptyi) : (tensor&lt;i32&gt;) -&gt; tensor&lt;f32&gt;
  %size_shape = "tf.Const"() {value = dense&lt;[10, 8, 4]&gt; : tensor&lt;3xi32&gt;} : () -&gt; tensor&lt;3xi32&gt;
  %tl = "tf.BroadcastTo"(%emptyf, %size_shape) : (tensor&lt;f32&gt;, tensor&lt;3xi32&gt;) -&gt; tensor&lt;10x8x4xf32&gt;
  // TensorListPushBack lowering
  %index_in_list = "tf.Const"() {value = dense&lt;0&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %arg0_shape = "tf.Const"() {value = dense&lt;[1, 8, 4]&gt; : tensor&lt;3xi32&gt;} : () -&gt; tensor&lt;3xi32&gt;
  %arg0_reshaped = "tf.Reshape"(%arg0, %arg0_shape) : (tensor&lt;8x4xf32&gt;, tensor&lt;3xi32&gt;) -&gt; tensor&lt;1x8x4xf32&gt;
  %zeroi2 = "tf.Const"() {value = dense&lt;0&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %axis = "tf.Const"() {value = dense&lt;0&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %start_indices = "tf.ConcatV2"(%index_in_list, %zeroi2, %axis) : (tensor&lt;1xi32&gt;, tensor&lt;2xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;3xi32&gt;
  %push = "tf.XlaDynamicUpdateSlice"(%tl, %arg0_reshaped, %start_indices) : (tensor&lt;10x8x4xf32&gt;, tensor&lt;1x8x4xf32&gt;, tensor&lt;3xi32&gt;) -&gt; tensor&lt;10x8x4xf32&gt;
  %one = "tf.Const"() {value = dense&lt;1&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %next_index_in_list = "tf.AddV2"(%index_in_list, %one) : (tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;1xi32&gt;
  return
}</code></p>
<h3><code>-tf-tpu-annotate-dynamic-shape-inputs</code>: Annotate the inputs returned by TPUCopyWithDynamicShapeOp with dynamic shape</h3>
<p>This pass looks for the usage of the result of TPUCopyWithDynamicShapeOp
and sets the shape of these inputs to be dynamic shaped. This will ensure
that the generated HLO program is correctly reflecting the dynamic shape.</p>
<h3><code>-tf-tpu-cleanup-cluster-attributes</code>: Eliminate _replication_info and other attributes from ops in a cluster</h3>
<p>This pass eliminate <code>_replication_info</code> and <code>device</code> attribute on operations
that are contained in a tf_device.cluster op.</p>
<h3><code>-tf-tpu-cluster-formation</code>: Forms clusters from operations assigned to the same TPU computation</h3>
<p>TPU computations from the frontend are composed of a <code>tf.TPUReplicateMetadata</code>
op, a subgraph of ops (TensorFlow Dialect) each with a matching
<code>_replication_info</code> attribute relative to the associated
<code>tf.TPUReplicateMetadata</code> op, and optionally <code>tf.TPUReplicatedInput</code> and
<code>tf.TPUReplicatedOutput</code> ops feeding in inputs and outputs to and from a
replicated TPU computation. The number of times a TPU computation is
replicated is defined in the <code>tf.TPUReplicateMetadata</code> op (<code>num_replicas</code>
attribute) and operand and result sizes of <code>tf.TPUReplicatedInput</code> and
<code>tf.TPUReplicatedOutput</code> respectively must match, excluding packed tensors.
It is also assumed ops of the same TPU computation do not have ops outside
of the TPU computation that are both inputs and outputs to the same TPU
computation. Furthermore, we assume that every node has either none or both
of <code>_replication_info</code> and <code>_xla_compile_device_type</code> attributes defined.</p>
<p>This pass takes the TPU computation subgraph, moves them into a
<code>tf_device.cluster</code>, and copies over attributes from the associated
<code>tf.TPUReplicateMetadata</code> op to the newly created <code>tf_device.cluster</code>. If the
computation is replicated (<code>num_replicas</code> &gt; 1), the <code>num_replicas</code> attribute is
not copied over but instead the <code>tf_device.cluster</code> is further wrapped with a
<code>tf_device.replicate</code>, and associated <code>tf.TPUReplicatedInput</code> and
<code>tf.TPUReplicatedOutput</code> ops are replaced as the <code>tf_device.replicate</code> operands
and results. Otherwise, the single operands and results of the associated
<code>tf.TPUReplicatedInput</code> and <code>tf.TPUReplicatedOutput</code> ops are simply forwarded to
the <code>tf_device.cluster</code>.</p>
<p>For example, the following non replicated computation:</p>
<p><code>mlir
func @tpu_computation(%arg0: tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt; {
  // Metadata op for cluster `cluster` with 1 replica, 1 core per replica and
  // with topology `&lt;topology&gt;`.
  "tf.TPUReplicateMetadata"() {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_relicas = 1, num_cores_per_replica = 1, topology = "&lt;topology&gt;", device_assignment = [], padding_map = []} : () -&gt; ()
  %replicated_input = "tf.TPUReplicatedInput"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
  %identity = "tf.Identity"(%replicated_input) {_xla_compile_device_type = "TPU", _replication_info = "cluster"} : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
  %replicated_output = "tf.TPUReplicatedOutput(%identity) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
  return %replicated_output : tensor&lt;i32&gt;
}</code></p>
<p>will be transformed into:</p>
<p><code>mlir
func @tpu_computation(%arg0: tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt; {
  %cluster = "tf_device.cluster"() ( {
    %identity = "tf.Identity"(%arg0) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    tf_device.return %identity : tensor&lt;i32&gt;
  }) {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_cores_per_replica = 1, topology = "topology", device_assignment = [], padding_map = []} : () -&gt; (tensor&lt;i32&gt;)
  return %cluster : tensor&lt;i32&gt;
}</code></p>
<p>The following replicated computation:</p>
<p><code>mlir
func @tpu_computation(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;) {
  "tf.TPUReplicateMetadata"() {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_relicas = 2, num_cores_per_replica = 1, topology = "topology", device_assignment = [], padding_map = []} : () -&gt; ()
  %replicated_input = "tf.TPUReplicatedInput"(%arg0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
  %identity = "tf.Identity"(%replicated_input) {_xla_compile_device_type = "TPU", _replication_info = "cluster"} : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
  %replicated_output:2 = "tf.TPUReplicatedOutput(%identity) : (tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;)
  return %replicated_output#0, %replicated_output#1 : tensor&lt;i32&gt;, tensor&lt;i32&gt;
}</code></p>
<p>will be transformed into:</p>
<p><code>mlir
func @tpu_computation(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;) {
  %replicate:2 = tf_device.replicate([%arg0, %arg1] as %replicated_input) {n = 2 : i32} {
    %cluster = "tf_device.cluster"() ( {
      %identity = "tf.Identity"(%replicated_input) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
      tf_device.return %identity : tensor&lt;i32&gt;
    }) {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_cores_per_replica = 1, topology = "topology", device_assignment = [], padding_map = []} : () -&gt; (tensor&lt;i32&gt;)
    tf_device.return %cluster : tensor&lt;i32&gt;
  }
  return %replicate#0, %replicate#1 : tensor&lt;i32&gt;, tensor&lt;i32&gt;
}</code></p>
<h3><code>-tf-tpu-colocate-composite-resource-ops</code>: Colocate resource with composite device assignment to TPU device.</h3>
<p>Pass that co-locates resource ops that use composite device resources
(packed tensors) with the underlying physical TPU device.</p>
<p>So for example, if we have a function that does (inside a <code>tf_device.replicate</code>):</p>
<p><code>mlir
  %0 = "tf.ReadVariableOp"(%arg1) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;4xf32&gt;&gt;&gt;) -&gt; tensor&lt;4xf32&gt;</code></p>
<p>Then said <code>ReadVariableOp</code> is going to get replaced by:</p>
<p><code>mlir
  %0 = "tf_device.launch"() ( {
    %2 = "tf.ReadVariableOp"(%arg1) : (tensor&lt;*x!tf_type.resource&lt;tensor&lt;4xf32&gt;&gt;&gt;) -&gt; tensor&lt;4xf32&gt;
    tf_device.return %2 : tensor&lt;4xf32&gt;
  }) {...} : () -&gt; tensor&lt;4xf32&gt;</code></p>
<h3><code>-tf-tpu-colocate-splits</code>: Colocates each Split op with its predecessor</h3>
<p>It is beneficial for performance to assign a <code>Split</code> op to the same device
as its predecessor. This is because the weight of cut edges is always
minimized when the <code>Split</code> is with its predecessor. This colocation
constraint will be used by the placer graph optimization to assign a device
to the op.</p>
<p>This pass should run in the export pipeline after tf-replicate-to-island so
each replica has its own distinct (predecessor, Split) pair.</p>
<p>The colocation class (<code>_class</code>) of the <code>Split</code> is set to the same class as
its predecessor:</p>
<p><code>mlir
%outputs1:2, %control1 = tf_executor.island wraps "tf.IteratorGetNext"(%arg)
  {_class = ["loc:@dataset_iterator_1"]}
%outputs2:2, %control2 = tf_executor.island wraps "tf.Split"(%outputs0, %outputs1#1)
  {_class = ["loc:@dataset_iterator_1", num_split = 2 : i32}</code></p>
<h3><code>-tf-tpu-device-propagation</code>: Propagates TPU devices from ops to users</h3>
<h3><code>-tf-tpu-dynamic-layout-pass</code>: Inserts TPU layout ops to determine layout at run time.</h3>
<p>A pass that allows TPU input layout to be determined after JIT compilation.
This is done by adding run-time ops that interpret compilation result and
copy the input to device with that layout.</p>
<p>Example: original program:</p>
<p><code>mlir
  %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}
  %compile:2 = "tf._TPUCompileMlir"(...)
  %execute = "tf.TPUExecute"(%input, ..., %compile#1) {device = "/TPU:0"}</code></p>
<p>Without this pass, later TF graph partitioning passes will insert send/recv
between %input and %execute and data will be copied to device in a fixed
layout. With this pass, the program will be transformed into:</p>
<p><code>mlir
  %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}
  %compile:2 = "tf._TPUCompileMlir"(...)
  %get_layout = "tf.TPUGetLayoutOp"(%compile#1) {...}
  %copy_to_device = "tf.TPUCopyWithLayout"(%input, %get_layout)
      {device = "/TPU:0"}
  %execute = "tf.TPUExecute"(%copy_to_device, ..., %compile#1)
      {device = "/TPU:0"}</code></p>
<p>This way, %compile will determine the layout, which will be respected by
%copy_to_device. There will not be send/recv ops added by later passes,
because tf.TPUCopyWithLayout accepts a host input and produces a device
output.</p>
<h3><code>-tf-tpu-host-computation-expansion</code>: Expands host computation before and after TPU computation.</h3>
<p>This pass expands outside compilation attributes to Identity/Cast ops
at the head of TPU computation if it's only used by outside compiled ops.</p>
<h3><code>-tf-tpu-identity-pruning</code>: Removes Identity/IdentityN ops from the TPU computation</h3>
<h3><code>-tf-tpu-merge-variables-with-execute</code>: Merges device variable reads and updates into TPU execute ops</h3>
<p>This pass finds on-device resource variable reads and updates surrounding a
<code>tf.TPUExecute</code> op and merges them into a <code>tf.TPUExecuteAndUpdateVariables</code>
op. This allows the TPU execution to perform more efficient in-place
variable updates.</p>
<p>For example,</p>
<p><code>mlir
  %0 = "tf.ReadVariableOp"(%arg0)
  %1 = "tf.ReadVariableOp"(%arg1)
  %2 = "tf.TPUExecute"(%0, %1, %compile)
  %3 = "tf.AssignVariableOp"(%arg0, %2)</code></p>
<p>will be transformed into</p>
<p>```mlir
  %2 = "tf.TPUExecuteAndUpdateVariables"(%arg0, %arg1, %compile)
    { device_var_reads_indices = [0, 1],
      device_var_updates_indices = [0, -1] }
````</p>
<p>The transformation happens only for on-device variables. The above
transformation requires <code>%arg0</code>, <code>%arg1</code> to have the same device assignment
as the <code>TPUExecute</code> op.</p>
<h3><code>-tf-tpu-parallel-execute-sink-resource-write</code>: Moves tf.AssignVariableOp consumers of tf_device.parallel_execute into tf_device.parallel_execute regions</h3>
<h3><code>-tf-tpu-partitioned-op-conversion</code>: Rewrite all TPU Partitioned ops into their V2 counterparts.</h3>
<h3><code>-tf-tpu-reorder-replicate-partitioned-inputs</code>: Reorder replicated and partitioned input ops.</h3>
<p>This pass rewrites how data parallelism and model parallelism is expressed for
inputs. It reorders <code>tf.TPUPartitionedInput</code> (model parallelism) and
<code>tf.TPUReplicatedInput</code> (data parallelism) ops. It transforms a DAG where
multiple <code>tf.TPUPartitionedInput</code> ops are feeding into a single
<code>tf.TPUReplicatedInput</code> into a DAG where multiple <code>tf.TPUReplicatedInput</code> ops
are feeding into a single <code>tf.TPUPartitionedInput</code>. Transforming the IR in such
a manner will allow subsequent cluster formation pass to handle IR with both
data and model parallelism in an easier manner.</p>
<p>For example, the following:</p>
<p><code>mlir
!rtype = type tensor&lt;!tf_type.resource&lt;tensor&lt;10x3xf32&gt;&gt;&gt;
func @data_and_model_parallelism(%arg0: !rtype, %arg1: !rtype, %arg2: !rtype, %arg3: !rtype) -&gt; !rtype {
  %pi_0 = "tf.TPUPartitionedInput"(%arg0, %arg1) {_XlaSharding = "", device = "", partition_dim = -1 : i64} : (!rtype, !rtype) -&gt; !rtype
  %pi_1 = "tf.TPUPartitionedInput"(%arg2, %arg3) {_XlaSharding = "", device = "", partition_dim = -1 : i64} : (!rtype, !rtype) -&gt; !rtype
  %ri = "tf.TPUReplicatedInput"(%pi_0, %pi_1) : (!rtype, !rtype) -&gt; !rtype
  return %ri : !rtype
}</code></p>
<p>will be transformed into:</p>
<p><code>mlir
!rtype = type tensor&lt;!tf_type.resource&lt;tensor&lt;10x3xf32&gt;&gt;&gt;
func @data_and_model_parallelism(%arg0: !rtype, %arg1: !rtype, %arg2: !rtype, %arg3: !rtype) -&gt; !rtype {
  %ri_0 = "tf.TPUReplicatedInput"(%arg0, %arg2) : (!rtype, !rtype) -&gt; !rtype
  %ri_1 = "tf.TPUReplicatedInput"(%arg1, %arg3) : (!rtype, !rtype) -&gt; !rtype
  %pi = "tf.TPUPartitionedInput"(%ri_0, %ri_1) {_XlaSharding = "", device = "", partition_dim = -1 : i64} : (!rtype, !rtype) -&gt; !rtype
  return %pi : !rtype
}</code></p>
<h3><code>-tf-tpu-resource-partition</code>: Partitions unpartitioned resource read/write to partitioned resource variables.</h3>
<p>This pass creates individual resource reads/writes from the unpartitioned
resource variable (from <code>tf.TPUPartitionedInput</code>) to individual partitioned
resource variables (<code>tf.TPUPartitionedInput</code> operands). As resource op
decomposition/lifting occurs with the unpartitioned resource variables,
transforming the IR in such a manner will allow for subsequent passes to operate
on individual resource variable handles per core/device.</p>
<p>For example, the following:</p>
<p>```mlir
func @cluster(%arg0: tensor&lt;!tf_type.resource<tensor\<i32>>&gt;, %arg1: tensor&lt;!tf_type.resource<tensor\<i32>>&gt;) {
  %partitioned_variable = "tf.TPUPartitionedInput"(%arg0, %arg1) {N = 2 : i64, _XlaSharding = "", partition_dim = -1 : i64} : (tensor&lt;!tf_type.resource<tensor\<i32>>&gt;, tensor&lt;!tf_type.resource<tensor\<i32>>&gt;) -&gt; tensor&lt;!tf_type.resource<tensor\<i32>>&gt;
  %read = "tf.ReadVariableOp"(%partitioned_variable) : (tensor&lt;!tf_type.resource<tensor\<i32>>&gt;) -&gt; tensor<i32>
  %computation = "tf_device.cluster_func"(%read) {func = @computation, use_spmd_for_xla_partitioning = true} : (tensor<i32>) -&gt; tensor<i32>
  "tf.AssignVariableOp"(%partitioned_variable, %computation) : (tensor&lt;!tf_type.resource<tensor\<i32>>&gt;, tensor<i32>) -&gt; ()
  return
}</p>
<p>func @computation(%arg0: tensor<i32>) -&gt; tensor<i32> {
  return %arg0: tensor<i32>
}
```</p>
<p>will be transformed into:</p>
<p>```mlir
func @cluster(%arg0: tensor&lt;!tf_type.resource<tensor\<i32>>&gt;, %arg1: tensor&lt;!tf_type.resource<tensor\<i32>>&gt;) {
  %read0 = "tf.ReadVariableOp"(%arg0) : (tensor&lt;!tf_type.resource<tensor\<i32>>&gt;) -&gt; tensor<i32>
  %read1 = "tf.ReadVariableOp"(%arg1) : (tensor&lt;!tf_type.resource<tensor\<i32>>&gt;) -&gt; tensor<i32>
  %partitioned_input = "tf.TPUPartitionedInput"(%read0, %read1) {N = 2 : i64, _XlaSharding = "", partition_dim = -1 : i64} : (tensor<i32>, tensor<i32>) -&gt; tensor<i32>
  %computation = "tf_device.cluster_func"(%partitioned_input) {func = @computation, use_spmd_for_xla_partitioning = true} : (tensor<i32>) -&gt; tensor<i32>
  %partitioned_output:2 = "tf.TPUPartitionedOutput"(%computation) {N = 2 : i64, _XlaSharding = "", partition_dim = -1 : i64} : (tensor<i32>) -&gt; (tensor<i32>, tensor<i32>)
  "tf.AssignVariableOp"(%arg0, %partitioned_output#0) : (tensor&lt;!tf_type.resource<tensor\<i32>>&gt;, tensor<i32>) -&gt; ()
  "tf.AssignVariableOp"(%arg1, %partitioned_output#1) : (tensor&lt;!tf_type.resource<tensor\<i32>>&gt;, tensor<i32>) -&gt; ()
  return
}</p>
<p>func @computation(%arg0: tensor<i32>) -&gt; tensor<i32> {
  return %arg0: tensor<i32>
}
```</p>
<h3><code>-tf-tpu-resource-read-for-write</code>: Inserts tf.ReadVariableOp inputs to a TPU cluster for resource writes with no reads</h3>
<p>This pass materializes <code>tf.ReadVariableOp</code> inputs to an outlined TPU computation
for resource variables where only writes are present so later in the pipeline
such resource variables can be fused with generated <code>tf.TPUExecute</code> ops, which
only supports resource variable read or read + write. For all TPU computations,
resource variables are required to be initialized prior to execution. Write only
resource variable uses can be generated currently via packed tensor uses.</p>
<p>For example, the following:</p>
<p>```mlir
func @write_only_resource(%value: tensor<i32>, %resource: tensor&lt;<em>x!tf_type.resource<tensor\<i32>>&gt;) {
  %0 = "tf_device.cluster_func"(%value) {func = @cluster} : (tensor<i32>) -&gt; tensor<i32>
  "tf.AssignVariableOp"(%resource, %0) : (tensor&lt;</em>x!tf_type.resource<tensor\<i32>>&gt;, tensor<i32>) -&gt; ()
  return
}</p>
<p>func @cluster(%arg0: tensor<i32>) -&gt; tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -&gt; tensor<i32>
  return %identity : tensor<i32>
}
```</p>
<p>will be transformed into:</p>
<p>```mlir
func @write_only_resource(%value: tensor<i32>, %resource: tensor&lt;<em>x!tf_type.resource<tensor\<i32>>&gt;) {
  %resource_read = "tf.ReadVariableOp"(%resource) : (tensor&lt;</em>x!tf_type.resource<tensor\<i32>>&gt;) -&gt; tensor<i32>
  %0 = "tf_device.cluster_func"(%value, %resource_read) {func = @cluster} : (tensor<i32>, tensor<i32>) -&gt; tensor<i32>
  "tf.AssignVariableOp"(%resource, %0) : (tensor&lt;*x!tf_type.resource<tensor\<i32>>&gt;, tensor<i32>) -&gt; ()
  return
}</p>
<p>func @cluster(%arg0: tensor<i32>, %arg1: tensor<i32>) -&gt; tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -&gt; tensor<i32>
  return %identity : tensor<i32>
}
```</p>
<h3><code>-tf-tpu-rewrite</code>: Rewrites a <code>tf_device.cluster_func</code> on TPUs into TPU runtime operations.</h3>
<p>This pass rewrites a <code>tf_device.cluster_func</code> operation into a sequence of <code>tf._TPUCompileMlir</code>
and <code>tf.TPUExecute</code> operations. <code>tf._TPUCompileMlir</code> contains a MLIR module that is
functionally equivalent to the function referenced by <code>tf_device.cluster_func</code>.
This makes the module to be jit-compiled and executed on TPU.
If it is not possible to rewrite the operation or device assignment fails,
a failure will be returned.</p>
<p>Note, many parameters to the <code>tf_device.cluster_func</code> are omitted in this
and following examples.
For example, a non replicated <code>tf_device.cluster_func</code>:</p>
<p><code>mlir
func @tf_tpu_rewrite(%arg0: tensor&lt;i8&gt;) {
  %0 = "tf_device.cluster_func"(%arg0) {_xla_compile_device_type = "TPU", _replication_info = "cluster0", func = @func} : (tensor&lt;i8&gt;) -&gt; tensor&lt;i8&gt;
  return
}</code></p>
<p>will be rewritten as:</p>
<p><code>mlir
func @tf_tpu_rewrite(%arg0: tensor&lt;i8&gt;) {
  %0:2 = "tf_device.launch"() ( {
    %compilation_status, %program = "tf._TPUCompileMlir"() {mlir_module = "&lt;serialized func&gt;"} : () -&gt; (tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;)
    tf_device.return %compilation_status, %program : tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;
  }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -&gt; (tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;)
  "tf_device.launch"() ( {
    "tf.TPUCompileSucceededAssert"(%0#0) : (tensor&lt;!tf_type.string&gt;) -&gt; ()
    tf_device.return
  }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -&gt; ()
  %1 = "tf_device.launch"() ( {
    %2 = "tf.TPUExecute"(%arg0, %0#1) : (tensor&lt;i8&gt;, tensor&lt;3x!tf_type.string&gt;) -&gt; tensor&lt;i8&gt;
    tf_device.return %2 : tensor&lt;i8&gt;
  }) {device = "/job:worker/replica:0/task:0/device:TPU:0"} : () -&gt; tensor&lt;i8&gt;
  return
}</code></p>
<p>A replicated <code>tf_device.cluster_func</code>:</p>
<p><code>mlir
func @tf_tpu_rewrite(%arg0: tensor&lt;i8&gt;, %arg1: tensor&lt;i8&gt;) {
  %0:2 = tf_device.replicate([%arg0, %arg1] as %ri: tensor&lt;i8&gt;) {n = 2 : i32} {
    %1 = "tf_device.cluster_func"(%ri) {_xla_compile_device_type = "TPU", _replication_info = "cluster0", func = @func} : (tensor&lt;i8&gt;) -&gt; tensor&lt;i8&gt;
    tf_device.return %1 : tensor&lt;i8&gt;
  }
  return
}</code></p>
<p>will be rewritten as:</p>
<p><code>mlir
func @tf_tpu_rewrite(%arg0: tensor&lt;i8&gt;, %arg1: tensor&lt;i8&gt;) {
  %0:2 = tf_device.replicate([%arg0, %arg1] as %arg2: tensor&lt;i8&gt;) {devices = {TPU_REPLICATED_CORE_0 = ["/job:worker/replica:0/task:0/device:TPU:0", "/job:worker/replica:0/task:0/device:TPU:1"], TPU_REPLICATED_HOST_0 = ["/job:worker/replica:0/task:0/device:CPU:0", "/job:worker/replica:0/task:0/device:CPU:0"]}, n = 2 : i32} {
    %1:2 = "tf_device.launch"() ( {
      %compilation_status, %program = "tf._TPUCompileMlir"() {mlir_module = "&lt;serialized func&gt;"} : () -&gt; (tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;)
      tf_device.return %compilation_status, %program : tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;
    }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -&gt; (tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;)
    "tf_device.launch"() ( {
      "tf.TPUCompileSucceededAssert"(%1#0) : (tensor&lt;!tf_type.string&gt;) -&gt; ()
      tf_device.return
    }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -&gt; ()
    %2 = "tf_device.launch"() ( {
      %3 = "tf.TPUExecute"(%arg2, %1#1) : (tensor&lt;i8&gt;, tensor&lt;3x!tf_type.string&gt;) -&gt; tensor&lt;i8&gt;
      tf_device.return %3 : tensor&lt;i8&gt;
    }) {device = "TPU_REPLICATED_CORE_0"} : () -&gt; tensor&lt;i8&gt;
    tf_device.return %2 : tensor&lt;i8&gt;
  }
  return
}</code></p>
<p>A non replicated <code>tf_device.cluster_func</code> with the model parallelism:</p>
<p><code>mlir
func @tf_tpu_rewrite(%arg0: tensor&lt;8xi32&gt;) -&gt; tensor&lt;8xi32&gt; {
  %0 = "tf_device.cluster_func"(%arg0) {_xla_compile_device_type = "TPU", _replication_info = "cluster0", func = @func, num_cores_per_replica = 2, input_sharding_configuration = ["\08\01\1A\01\01\22\01\00"], output_sharding_configuration = ["\08\01\1A\01\01\22\01\00"]} : (tensor&lt;8xi32&gt;) -&gt; tensor&lt;8xi32&gt;
  return %0 : tensor&lt;8xi32&gt;
}</code></p>
<p>will be rewritten as:</p>
<p><code>mlir
func @tf_tpu_rewrite(%arg0: tensor&lt;8xi32&gt;) -&gt; tensor&lt;8xi32&gt; {
  %0:3 = "tf_device.launch"() ( {
    %compilation_status, %program:2 = "tf._TPUCompileMlir"() {mlir_module = "&lt;serialized func&gt;"} : () -&gt; (tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;)
    tf_device.return %compilation_status, %program#0, %program#1 : tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;
  }) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -&gt; (tensor&lt;!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;, tensor&lt;3x!tf_type.string&gt;)
  "tf_device.launch"() ( {
    "tf.TPUCompileSucceededAssert"(%0#0) : (tensor&lt;!tf_type.string&gt;) -&gt; ()
    tf_device.return
  }) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -&gt; ()
  %1 = "tf_device.parallel_execute"() ( {
    %2 = "tf_device.launch"() ( {
      %3 = "tf.TPUExecute"(%arg0, %0#1) : (tensor&lt;8xi32&gt;, tensor&lt;3x!tf_type.string&gt;) -&gt; tensor&lt;8xi32&gt;
      tf_device.return %3 : tensor&lt;8xi32&gt;
    }) {device = "/job:localhost/replica:0/task:0/device:TPU:0"} : () -&gt; tensor&lt;8xi32&gt;
    tf_device.return %2 : tensor&lt;8xi32&gt;
  },  {
    "tf_device.launch"() ( {
      "tf.TPUExecute"(%0#2) : (tensor&lt;3x!tf_type.string&gt;) -&gt; ()
      tf_device.return
    }) {device = "/job:localhost/replica:0/task:0/device:TPU:1"} : () -&gt; ()
    tf_device.return
  }) : () -&gt; tensor&lt;8xi32&gt;
  return %1 : tensor&lt;8xi32&gt;
}</code></p>
<h4>Options</h4>
<p><code>-tpu-compile-metadata-debug : Whether to serialize TPUCompileMetadataProto metadata in 'tf._TPUCompileMlir' op as a proto debug string</code></p>
<h3><code>-tf-tpu-sharding-identification</code>: Identifies and handles inputs/outputs of TPU computation that is sharded across logical cores.</h3>
<p>Bubbles up sharding configuration from <code>cluster_func</code> regions into
the attributes of <code>cluster_func</code>. This is done by parsing the
<code>XlaSharding</code> / <code>TPUPartitionedOutput</code> / <code>TPUPartitionedInput</code> ops inside
<code>cluster_func</code>.</p>
<p>For example, given the following <code>cluster_func</code> wrapping <code>func</code>:</p>
<p>```mlir
  func @test(%arg0: tensor&lt;<em>xi32&gt;) {
    "tf_device.cluster_func"(%arg0) {
        func = @func,
        step_marker_location = ""} : (tensor&lt;</em>xi32&gt;) -&gt; tensor&lt;*xi32&gt;
    return
  }</p>
<p>func @func(%arg0: tensor&lt;<em>xi32&gt;) -&gt; tensor&lt;</em>xi32&gt; {
    %0 = "tf.XlaSharding"(%arg0) {_XlaSharding = "\01\02\03",
                                  sharding = "\01\02\03"} : (tensor&lt;<em>xi32&gt;) -&gt; tensor&lt;</em>xi32&gt;
    %1 = "tf.A"(%0) : (tensor&lt;<em>xi32&gt;) -&gt; (tensor&lt;</em>xi32&gt;)
    return %1 : tensor&lt;*xi32&gt;
  }
```</p>
<p>Now, cluster_func receives the following <code>*_sharding_configuration</code>
attributes, and <code>func</code> receives the mhlo.sharding attribute:</p>
<p><code>mlir
  func @test(%arg0: tensor&lt;*xi32&gt;) {
    %0 = "tf_device.cluster_func"(%arg0) {
        func = @func,
        input_sharding_configuration = ["\01\02\03"],
        output_sharding_configuration = ["\08\01\1A\01\01\22\01\00"],
        step_marker_location = ""} : (tensor&lt;*xi32&gt;) -&gt; tensor&lt;*xi32&gt;
    return
  }
  func @func(%arg0: tensor&lt;*xi32&gt; {mhlo.sharding = "\01\02\03"}) -&gt;
            (tensor&lt;*xi32&gt; {mhlo.sharding = "\08\01\1A\01\01\22\01\00"}) {
    %0 = "tf.XlaSharding"(%arg0) {_XlaSharding = "\01\02\03", sharding = "\01\02\03"} : (tensor&lt;*xi32&gt;) -&gt; tensor&lt;*xi32&gt;
    %1 = "tf.A"(%0) : (tensor&lt;*xi32&gt;) -&gt; tensor&lt;*xi32&gt;
    return %1 : tensor&lt;*xi32&gt;
  }</code></p>
<h3><code>-tf-tpu-space-to-depth-pass</code>: Applies automatic space to depth transform for the first or frontier convolutions consume host inputs on TPU.</h3>
<p>Automatic space to depth transform is done by adding space to depth transform op after host input
and applying space to depth transform for the first convolution and its backprop filter on TPU.</p>
<p>For example, original program:</p>
<p><code>mlir
module {
  func @while_body {
    %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}: -&gt; tensor&lt;2x224x224x3xf32&gt;
    %device_launch = "tf_device.cluster_func"(%input,...) {func = @_func,...)
    return ...
  }
  func @_func(%input: tensor&lt;2x224x224x3xf32&gt;, %filter: tensor&lt;7x7x3x64xf32&gt;) {
    %6 = "tf.Conv2D"(%input, %filter)  {strides = [1, 2, 2, 1]}: (tensor&lt;2x230x230x3xf32&gt;, tensor&lt;7x7x3x64xf32&gt;) -&gt; tensor&lt;2x112x112x64xf32&gt;
  }
}</code></p>
<p>The program will be transformed into:</p>
<p><code>mlir
module {
  func @while_body {
    %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"} -&gt; tensor&lt;2x224x224x3xf32&gt;
    %space_to_depth = "tf.SpaceToDepth"(%input) {block_size = 2, ...}: (tensor&lt;2x224x224x3xf32&gt;) -&gt; tensor&lt;2x112x112x12xf32&gt;
    %device_launch = "tf_device.cluster_func"(%space_to_depth,...) {func = @_func,...)
    return ...
  }
  func @_func(%input: tensor&lt;2x112x112x12xf32&gt;, %filter: tensor&lt;7x7x3x64xf32&gt;) {
    %filter_transform = "tf.Pad/tf.Transpose/tf.Reshape"(%filter): tensor&lt;7x7x3x64xf32&gt;) -&gt; tensor&lt;4x4x12x64xf32&gt;
    %conv = "tf.Conv2D"(%input, %filter_transfrom) {strides = [1, 1, 1, 1]}: (tensor&lt;2x112x112x12xf32&gt;, tensor&lt;4x4x12x64xf32&gt;) -&gt; tensor&lt;2x112x112x64xf32&gt;
  }
}</code></p>
<p>This way, the first convolution with 3 feature dimension will be transformed
to 12 feature dimension, which has better performance on TPU.</p>
<h3><code>-tf-tpu-update-embedding-enqueue-op-inputs</code>: Updates inputs to TPU embedding enqueue ops depending on whether graph is in training mode or in evaluation mode.</h3>
<p>Updates inputs to TPU embedding enqueue ops depending on whether graph
is in training mode or in evaluation mode.</p>
<h3><code>-tf-tpu-validate-inputs</code>: Validates inputs to the TPU TF/XLA bridge</h3>
<p>This pass checks that the IR has valid input to TPU TF/XLA bridge.
It checks the relations of multiple ops. Properties of single ops are
checked by the 'verify' method of ops.</p>
<h3><code>-tf-tpu-variable-runtime-reformatting</code>: Adds device variable formatting op to allow compilation-guided variable formatting.</h3>
<p>A pass that takes advantage of a loop to add ops that allow the execution to
avoid repeatedly formatting variables back and forth. The desired formatting
is determined by TPU program compilation, so this pass does not include how
to reformat the variables, but only inserts general TPUReshardVariablesOps in
proper places, and TPUReshardVariablesOps interpret the compilation.</p>
<p>The core idea of this optimization is to keep track of the formatting state
of variables, and when the next desired state does not change, it can avoid
reformatting. We associate a set of variables on a device with a formatting
state, and TPUReshardVariablesOps compares the current state with a desired
state (which can be the compilation result). If they mismatch,
TPUReshardVariablesOp reformats the variables to the desired state; if they
match, TPUReshardVariablesOp is a no-op.</p>
<p>A major use of this pass is weight-update sharding in data parallelism, so we
require there is a tf_device.replicate in the loop.</p>
<p>For example, suppose we have a training loop (for simplicity we write the
loop body inine):</p>
<p><code>mlir
  %var0 = ...
  %var1 = ...
  tf.while (..., %var0, %var1) {
    tf_device.replicate ([%var0, %var1] as %rvar) {
      %compile:2 = "tf._TPUCompileMlir"()
      tf.TPUExecuteAndUpdateVariablesOp(%rvar, compile#1)
    }
  }</code></p>
<p>This pass will transform it into</p>
<p><code>mlir
  %var0 = ...
  %var1 = ...
  %state_var0 = ...
  %state_var1 = ...
  tf.while (..., %var0, %var1, %state_var0, %state_var1) {
    tf_device.replicate ([%var0, %var1] as %rvar,
                         [%state_var0, %state_var1] as %rstate) {
      %compile:2 = "tf._TPUCompileMlir"()
      tf.TPUReshardVariablesOp(%rvar, %compile#1, %rstate)
      tf.TPUExecuteAndUpdateVariablesOp(%rvar, compile#1)
    }
  }
  %default_format = tf.constant()
  tf_device.replicate ([%var0, %var1] as %rvar,
                       [%state_var0, %state_var1] as %rstate) {
    tf.TPUReshardVariablesOp(%rvar, %default_format, %rstate)
  }</code></p>
<h3><code>-tf-unroll-batch-matmul</code>: Unroll TF BatchMatMul op into Reshape, Slice, MatMul, Pack ops.</h3>
<h3><code>-tf-verify-for-export</code>: Verify module is suitable for export back to TF Graph</h3>
<p>Verifies whether all functions in module are of single tf_executor.graph and
each tf_executor.island in tf_executor.graph only has a single op.</p>
<h3><code>-tf-xla-call-module-deserialization</code>: Deserializes StableHLO functions embedded in <code>tf.XlaCallModule</code> to top level module</h3>
<p>This pass deserializes the StableHLO bytecodes embedded in tf.XlaCallModule,
then outlines the functions in the deserialized StableHLO module to the top
level MLIR module, with function renamings to avoid naming conflicts.</p>
<p>After the outlining, it updates tf.XlaCallModule's module attribute to be
empty, adds an <code>_entry_function</code> attribute referring to the entry function.
It also adds a <code>_from_xla_call_module: true</code> attribute to each lifted
StableHLO function.</p>
<h3><code>-tf-xla-call-module-serialization</code>: Serializes StableHLO functions from top-level module into <code>tf.XlaCallModule</code>'s <code>module</code> attribute</h3>
<p>This pass collects StableHLO functions referenced from <code>tf.XlaCallModule</code>'s
<code>_entry_function</code> attribute into a module, serializes the module into MLIR
bytecode, and embed the bytecode to <code>tf.XlaCallModule</code>'s <code>module</code> attribute.</p>
<p>After serialization, this pass removes the <code>_entry_function</code> attribute from
<code>tf.XlaCallModule</code>, and removes all the serialized stablehlo functions
from the top-level module.</p>
<h3><code>-tfe-legalize-tfg</code>: Legalize from TFG to the TFE dialect</h3>