<h1>AutoGraph reference</h1>
<p><a href="index.md">Index</a></p>
<h2>Debugging AutoGraph code</h2>
<p>The recommended way to debug AutoGraph code is to run it eagerly (see below).</p>
<p>AutoGraph generates a new function, rather than directly executing the input
function. Non-code elements, such as breakpoints, do not transfer to the
generated code.</p>
<p>You can step through the generated code and set breakpoints while debugging.
The converted function is cached, and breakpoints should persist for the
lifetime of the Python runtime.</p>
<p>Note: The code generated by AutoGraph code is more complex than the input code,
and is interspersed with AutoGraph boilerplate.</p>
<p>Note: Python debugging can only be used to step through the code during graph
construction time (or tracing time in the case of <code>tf.function</code>). To debug
TensorFlow execution, use Eager execution.</p>
<h3>Debugging <code>tf.function</code>: <code>tf.config.experimental_run_functions_eagerly</code></h3>
<p>When using <code>@tf.function</code>, you can temporarily toggle graph execution by using
<code>tf.config.experimental_run_functions_eagerly</code>. This will effectively run the
annotated code eagerly, without transformation. Since AutoGraph has semantics
consistent with Eager, it's an effective way to debug the code step-by-step.</p>
<p>Note: AutoGraph is compatible with Eager, but the converse is not always
true, so exercise care when making modifications to the code while debugging.</p>
<p>Consider the following code:</p>
<p><code>@tf.function
def f(a):
  pdb.set_trace()
  if a &gt; 0:
    tf.print(a, 'is positive')</code></p>
<p>Executing the line below will land the debugger in generated code, when the
function is traced:</p>
<p><code>f(1)</code></p>
<p>```</p>
<blockquote>
<p>l
     10     def tf__f(a):
     11       pdb.set_trace()
---&gt; 12       ag__.converted_call('print', tf, ag__.STD, (a,), None)
     13
     14       ...
```</p>
</blockquote>
<p>Adding a call to <code>tf.config.experimental_run_functions_eagerly</code> before executing
the function will land the debugger in the original code instead:</p>
<p><code>tf.config.run_functions_eagerly(True)
f(1)</code></p>
<p>```</p>
<blockquote>
<p>l
      8 def f(a):
      9   pdb.set_trace()
---&gt; 10   tf.print(a)
     11   if a &gt; 0:
     12     tf.print('is positive')
```</p>
</blockquote>
<h3>Using <code>print</code> and <code>tf.print</code></h3>
<p>The <code>print</code> function is not converted by AutoGraph, and can be used to inspect
the values of variables at graph construction time.</p>
<p>Mixing <code>print</code> with <code>tf.print</code> can be confusing at first because they run at
different stages. In general:</p>
<ul>
<li>all <code>print</code>s run when the TensorFlow graph is constructed</li>
<li>all <code>tf.print</code>s run when the TensorFlow graph is executed</li>
</ul>
<h4>Example: <code>print</code></h4>
<p>To see the difference between <code>print</code> and <code>tf.print</code>.</p>
<p><code>@tf.function
def f(a):
  print(a)
  if a &gt; 0:
    a = -a</code></p>
<p>When <code>a</code> is a <code>tf.Tensor</code> object, it is printed without an actual value:</p>
<p><code>f(tf.constant(1))</code>
<code>Tensor("a:0", shape=(), dtype=int32)</code></p>
<p>Similarly, when <code>a</code> is just a Python value, it is printed directly:</p>
<p><code>f(1)</code>
<code>1</code></p>
<h4>Example: <code>print</code> followed by <code>tf.print</code></h4>
<p>To see the difference between <code>print</code> and <code>tf.print</code>, let's run them together:</p>
<p><code>@tf.function
def f(a):
  print(a)
  tf.print(a)</code></p>
<p>For non-<code>Tensor</code> values, they produce similar results:</p>
<p><code>f(1)</code>
<code>1
1</code></p>
<p>For Tensor values, only <code>tf.print</code> outputs the actual value:</p>
<p><code>f(tf.constant(1))</code>
<code>Tensor("a:0", shape=(), dtype=int32)
1</code></p>
<h4>Example: <code>tf.print</code> followed by <code>print</code></h4>
<p>Remember that, in general, <em>all <code>print</code>s run before all <code>tf.prints</code></em>.
What's more, since graphs are usually built once and executed multiple times,
<code>print</code> usually runs just once when the function is first called.</p>
<p>So in the example below, even though <code>tf.print</code> appears above <code>print</code>, it will
run after it, because the graph is executed after it is built:</p>
<p><code>@tf.function
def f(a):
  tf.print('At graph execution:', a)
  print('At graph construction:', a)</code></p>
<p><code>f(tf.constant(1))</code>
<code>At graph construction: Tensor("a:0", shape=(), dtype=int32)
At graph execution: 1</code></p>
<p>Calling the function again will re-use the graph in this case:</p>
<p><code>f(tf.constant(1))</code>
<code>At graph execution: 1</code></p>