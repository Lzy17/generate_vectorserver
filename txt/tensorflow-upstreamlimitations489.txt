<h1>AutoGraph reference</h1>
<p><a href="index.md">Index</a></p>
<h2>Limitations</h2>
<p>When AutoGraph is applied to normal Python code, you should expect no change
in functionality.
However, when applied to TensorFlow control flow (for example, an if statement
with a <code>tf.Tensor</code> condition), there are certain limitations. This section
describes these limitations and practices that will allow you to avoid them.</p>
<p>Key Term: Python variables refer to Python symbols (or symbols for short) and
should not be confused with TensorFlow variables.</p>
<p>Key Term: A TensorFlow loop variable (or loop variable for short) refers to a
value (typically a <code>tf.Tensor</code>) modified by a loop. See <code>tf.while_loop</code>.</p>
<h3>Undefined and None values in TensorFlow</h3>
<p>TensorFlow does not support undefined or <code>None</code> values. All tensors must have
a value.</p>
<p>Example:</p>
<p><code>x = tf.cond(
    tf.random.uniform(()) &gt; 0.5,
    lambda: tf.constant(1),
    lambda: None)  # Error -- a Tensor cannot be None</code></p>
<p>The same restriction carries over in AutoGraph. If a variable is created inside
control flow, and used after, then it must be defined before the control flow
statement:</p>
<p><code>if tf.random.uniform(()) &gt; 0.5:
  x = tf.constant(1)
else:
  x = None
tf.print(x)  # Error -- x may be None here</code></p>
<p>For this reason, AutoGraph forbids variables to be defined in only one branch
of a TensorFlow conditional, if the variable is used afterwards:</p>
<p><code>del x
if tf.random.uniform(()) &gt; 0.5:
  x = tf.constant(1)
else:
  pass
tf.print(x)  # Error -- x may be undefined here</code></p>
<p>Note that if the variable is not used after the control flow statement, then it
is considered local to the control flow block, and is not subject to these
restrictions.</p>
<p><code>del x
if tf.random.uniform(()) &gt; 0.5:
  x = tf.constant(1)  # Okay -- x does not need to be returned from the TF cond
else:
  pass</code></p>
<p>Similarly, variables must usually be defined before a TensorFlow loop.</p>
<p>The most common example that is not allowed is a loop which initializes some
accumulator variable in the first iteration:</p>
<p><code>del x
for i in tf.range(100):  # Error -- x must be defined before the loop
  if i == 0:
    x = tf.constant(1)
  else:
    x = x + 1
tf.print(x)</code></p>
<p>When the variable is only used inside the loop and does not depend on previous
iterations, then it's ok to only be initialized inside the loop.</p>
<p><code>del x
while tf.random.uniform(()) &gt; 0.5:  # Okay -- x is not used after the loop
  x = tf.constant(1)</code></p>
<ul>
<li>New in TF 2.4 *</li>
</ul>
<p>As long as it doesn't depend on previous iterations, the variable may also be
used after the loop, however in that case the loop must execute at least one
iteration, and will raise a runtime error otherwise.</p>
<p><code>del x
for i in tf.range(10):  # Okay -- x does not depend on previous iterations
  x = tf.constant(1)
tf.print(x)</code></p>
<p><code>del x
while tf.constant(False):  # Error -- loop must initialize x!
  x = tf.constant(1)
tf.print(x)</code></p>
<p>Avoid these limitations by defining a default value before the control flow
statement:</p>
<p><code>x = tf.constant()
if tf.random.uniform(()) &gt; 0.5:
  x = tf.constant(1)
tf.print(x)  # Okay -- x is either 0 or 1</code></p>
<p>Note: <code>None</code> values and undefined symbols are allowed in Eager control flow,
because Eager execution uses Python control flow, rather than TensorFlow
control flow ops.</p>
<h4>Special case: creating Tensors in a loop</h4>
<ul>
<li>New in TF 2.4 *</li>
</ul>
<p>A very common use-case is to run a training loop that creates some outputs:</p>
<p><code>for i in tf.range(num_steps):
  outputs = train(next(data_iterator))</code></p>
<p>Often times these outputs can be nested structures of Tensors, which makes them
impractical to initialize ahead of the loop.</p>
<p>To help with this use-case, AutoGraph lets you run such loops, under certain
conditions:</p>
<ul>
<li>outputs must be a Tensor, Python numeric, or a structure of these</li>
<li>outputs must not depend on the value from a previous iteration; in other
   words, the outputs may only appear to the left of an assignment operation</li>
<li>the loop must run at least one iteration</li>
</ul>
<p>If the type of outputs is not recognized, then the usual
"outputs must be defined before the loop" is raised at graph construction.</p>
<p>AutoGraph also inserts a <code>tf.Assert</code> statement that raises a runtime error
if the loop did not execute at least one iteration.</p>
<h3>Indirect modifications and hidden side effects in TensorFlow control flow</h3>
<p>Key Point: We recommend using a functional programming style, immutable Python
collections, TensorFlow ops and collections. Only TensorFlow objects should be
used for side effects.</p>
<h4>AutoGraph analyzes code to detect modifications to Python objects</h4>
<p>Note: Modifications to TensorFlow objects, such as <code>tf.Variable</code>, are tracked
using a different mechanism (automatic control dependencies) which does not
rely on code analysis.</p>
<p>One of the most important functions of AutoGraph is to rewrite Python control
flow statements into equivalent TensorFlow ops. This process requires "wiring"
variables covered by these control flow statements into the respective ops.</p>
<p>The examples below use a <code>while</code> loop, but the same notions extend to all
control flow such as <code>if</code> and <code>for</code> statements.</p>
<p>In the example below, <code>x</code> needs to become a loop variable of the
corresponding `tf.while_loop':</p>
<p><code>while x &gt; 0:
  x = x - 1</code>
<code>x = tf.while_loop(..., loop_vars=(x,)</code></p>
<p>TF control ops support only a limited set of types for loop variables. At the
same time, the efficiency of TensorFlow graphs is influenced by the number of
loop variables, so we don't want to create them unnecessarily. AutoGraph pulls
symbols through loop variables only if necessary to minimize the number of
loop variables.</p>
<p>Note: If a symbol refers to a nested structure, such as a <code>dict</code> of <code>dict</code>s,
the entire structure is mapped to multiple loop variables - TensorFlow
automatically unpacks it.</p>
<p>For example, the symbol 'y' below is not wired through the <code>tf.while_loop</code>'s
<code>loop_vars</code> because it is not affected by the <code>while</code> loop:</p>
<p><code>y = 0
while x &gt; 0:
  x = x - 1
print(y)</code>
<code>x = tf.while_loop(..., loop_vars=(x,)  # y does not need to be a loop variable</code></p>
<p>AutoGraph uses static analysis to determine which symbols are modified by the
code, in order to transform them into control flow variables. Static analysis
is generally performed on single functions - Python's dynamic nature limits its
effectiveness across functions.</p>
<h4>Modifications of Python objects are not detected across functions</h4>
<p>Note: Modifications to TensorFlow objects, such as <code>tf.Variable</code>, are tracked
using a different mechanism (automatic control dependencies). Modifications
to <code>tf.Variable</code> objects are correctly handled even when called in other
functions.</p>
<p>Because static analysis is limited to single functions, modifications that are
performed in other functions are not visible to AutoGraph:</p>
<p>```
def change_y():
  global y
  y = y + 1</p>
<p>while x &gt; 0:
  change_y()  # Problem -- change made to y is not visible here!
```</p>
<p>This can be easily remedied using a functional programming style - writing
functions that use argument for all their inputs and return values for all their
outputs:</p>
<p>```
def change(y):
  y = y + 1
  return y</p>
<p>while x &gt; 0:
  y = change(y)  # Okay -- y can now be properly tracked!
```</p>
<p>As noted before, this limitation does not apply to most TensorFlow objects,
although it is still a good idea to use functional programming style for
better code readability:</p>
<p>```
def change(y_var):
  y_var.assign_add(1)</p>
<p>y = tf.Variable(1)
while x &gt; 0:
  change(y)  # This is still okay -- TensorFlow side effects are robust.
```</p>
<p>Keep in mind however that certain types like <code>tf.TensorArray</code> don't support
side effects and must have their result assigned, otherwise they may raise an
error:</p>
<p><code>def change(ta):
  ta.write(0, 1)  # Incorrect use of TensorArray - will raise an error</code></p>
<p>In other words, <code>tf.TensorArray</code> must be handled using functional programming
style:</p>
<p>```
def change(ta):
  ta = ta.write(0, 1)  # Modifications create a new TensorArray efficiently.
  return ta</p>
<p>ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)
while x &gt; 0:
  # TensorArray must be handled using functional programming style.
  ta = change(ta)
```</p>
<h4>Modifications of Python objects are not detected in methods</h4>
<p>A special case of hidden side effects are methods, which are commonly used
to change the value of objects:</p>
<p>```
class MyClass(object):
  def change(self):
    self.y += 1</p>
<p>c = MyClass()
while x &gt; 0:
  c.change()  # Problem -- modification to c.y is not visible here!
```</p>
<p>This can be addressed in a number of ways.</p>
<p>One possibility is to operate directly on the object properties:</p>
<p><code>c = MyClass()
while x &gt; 0:
  c.y += 1  # Okay -- c.y can now be properly tracked!</code></p>
<p>Another possibility is to rely on immutable objects with value semantics. This
may lead to many temporary objects when executing eagerly, but their number is
greatly reduced in <code>@tf.function</code>:</p>
<p>```
class MyClass(collections.namedtuple('MyClass', ('y',))):
  def change(self):
    new_y = self.y + 1
    return MyClass(new_y)</p>
<p>c = MyClass()
while x &gt; 0:
  c = c.change()  # Okay -- c is now a loop var.
```</p>
<p>It is also recommended to use a functional programming style with such immutable
objects - that is, all arguments are inputs, all changes are return values:</p>
<p><code>def use_my_class(c: MyClass) -&gt; MyClass:
  new_c = c.change()
  return new_c</code></p>
<p>Don't worry about creating a few extra objects - they are only used at trace
time, and don't exist at graph execution.</p>
<p>Note: TensorFlow control flow does not currently support arbitrary Python
objects, but it does support basic collection objects such as <code>list</code>, <code>dict</code>,
<code>tuple</code>, <code>namedtuple</code> and their subclasses. Design your objects as subclasses
of <a href="https://docs.python.org/3/library/collections.html#collections.namedtuple">namedtuple</a>,
or other types that <a href="https://www.tensorflow.org/api_docs/python/tf/nest/map_structure">tf.nest</a>
recognizes.</p>
<h4>Variables closed over by lambda functions</h4>
<h3>This limit will be deprecated after 2023-09-23</h3>
<p>AutoGraph assumes that variables that local functions close over may be used
anywhere in the parent function, because in general it is possible to hide a
function call in almost any Python statement). For this reason, these variables
are accounted within TensorFlow loops.</p>
<p>For example, the following code correctly captures <code>a</code> in the TensorFlow loop
variables:</p>
<p><code>a = 0
def f():
  tf.print(a)
for i in tf.range(3):
  a = i
f()  # Prints 2</code></p>
<p>An consequence is that these variables must be defined before the loop (see
Undefined and None values above). So the following code will raise an error,
even if the variable is never used after the loop:</p>
<p><code>def f():
  tf.print(a)
for i in tf.range(3):  # Error -- `a` must be defined before the loop.
  a = i</code></p>
<p>However, lambda functions are handled differently, for reasons of backward
compatibility. Lambda functions are assumed to be used in the statement where
they are used, or at least in the same block.</p>
<p><code>a = 0
foo(lambda: a)  # This lambda is not expected to be called anywhere else.
for i in tf.range(3):  # Okay -- `a` is local to the loop.
  a = i</code></p>
<p>Due to that reason, the following code will not work as expected for TensorFlow
loops.</p>
<p><code>a = 0
l = lambda: tf.print(a)
for i in tf.range(3):
  a = i  # `a` is considered local to the loop
l()  # Prints 0!</code></p>
<p>Note that none of these restrictions only apply to TensorFlow loops; Python
loops correctly handle closures in all cases.</p>
<h3>Python collections in TensorFlow control flow</h3>
<p>Key Point: Use TensorFlow collection classes instead of Python collections.
Python collections are okay to use when they represent a fixed structure (that
is, <code>list</code>s don't change length, <code>dict</code>s don't add or remove keys).</p>
<h4>Modifying Python collections in TensorFlow control flow is not allowed</h4>
<p>One of the advantages of eager execution is that you may use the usual Python
collections, like <code>list</code> or <code>dict</code> to hold <code>tf.Tensor</code> values. However, these
are generally not compatible with TensorFlow control flow. Specialized
collections like <code>tf.TensorArray</code> are required.</p>
<p>Consider the following example:</p>
<p>```
def fn():
  l = []</p>
<p>def loop_cond(i):
    return i &lt; 10</p>
<p>def loop_body(i):
    i = i + 1
    l.append(i)
    return i,</p>
<p>tf.while_loop(
      cond=loop_cond,
      body=loop_body,
      loop_vars=(0,))</p>
<p>return l
```</p>
<p>This code works in eager execution, which does not use the TensorFlow runtime
for the <code>tf.while_loop</code>:</p>
<p><code>fn()</code></p>
<p>However, it does not work in graph execution, because TensorFlow uses special
mechanisms to ensure the computations are correctly sequenced in the dataflow
graph:</p>
<p><code>tf.function(fn)()  # Error -- illegal tensor capture!</code></p>
<p>The equivalent AutoGraph code raises the same error:</p>
<p><code>l = []
for i in tf.range(10):
  l.append(i)  # Error -- illegal tensor capture!</code></p>
<p>Instead, use the specialized <code>tf.TensorArray</code> class:</p>
<p><code>l = tf.TensorArray(tf.int32, size=0, dynamic_size=True)
for i in tf.range(10):
  l = l.write(l.size(), i)  # Okay</code></p>
<h4>Python collections of fixed structure are allowed TensorFlow control flow</h4>
<p>An exception from the previous rule is made by Python collections that are
static, that is, they don't grow in size for the duration of the computation.</p>
<p>Caution: Use functional programming style when manipulating static collections.</p>
<p>Examples:</p>
<p><code>static_list = [tf.constant(3)]
while d.prop &gt; 0:
  static_list[0] -= 1  # Okay -- static_list does not change structure</code>
<code>static_object = MyClass()
static_object.field = tf.constant(3)
while static_object.field &gt; 0:
  static_object.field -= 1  # Okay -- static_object does not change structure</code>
<code>static_dict = {'field': tf.constant(3)}
while static_dict['field'] &gt; 0:
  static_dict['field'] -= 1  # Okay -- static_dict does not change structure</code></p>
<p>However, remember to use functional programming style when these collections
are used inside control flow.</p>
<h4>Python collections of fixed structure with dynamic index</h4>
<p>A more subtle error occurs when the collection is static, but is accessed in a
dynamic way, that is with a key that is not constant.</p>
<p>For example:</p>
<p><code>d = {'a': tf.constant(3)}
for i in tf.range(10):
  for key in d:
    d[key] += i  # Problem -- accessing `dict` using non-constant key</code></p>
<p>The code above will raises an "illegal capture" error. To remedy it, write it
in functional programming style:</p>
<p><code>d = {'a': tf.constant(3)}
for i in tf.range(10):
  d = {key: value + i for key, value in d.items()}  # Okay</code></p>
<h3>Shape and dtype consistency in TensorFlow control flow</h3>
<p>Unlike Python, TensorFlow has limited support for dynamic typing. This means
that tensors must maintain consistent shapes and dtypes across control flow
paths.</p>
<p>Note: In general, these restrictions do not apply in control flow in Eager
execution, because Eager execution uses Python control flow, rather than
TensorFlow control flow ops.</p>
<h4>Mixing dynamic computations and static shapes</h4>
<p>Key Point: Use <code>.shape</code> on tensors of static shape, and <code>.shape.rank</code> on
tensors of static rank; only use <code>tf.shape</code> and <code>tf.rank</code> when the shape or
rank is dynamic.</p>
<p>TensorFlow has optional static types and shapes: the shape of tensors may be
static (e.g. <code>my_tensor.shape=(3, 3)</code> denotes a three by three matrix) or
dynamic (e.g. <code>my_tensor.shape=(None, 3)</code> denotes a matrix with a dynamic
number of rows and three columns. When the shapes are dynamic, you can still
query it at runtime by using the <code>tf.shape()</code> function.</p>
<p>Note: <code>tf.shape</code> always returns a tensor.</p>
<p>For static shapes, TensorFlow will perform additional shape verifications at
graph construction time, that is, during tracing. These static shape
verifications are useful because they work like a compiler for example, errors
are caught early, before execution even begins.</p>
<p>For example:</p>
<p><code>x = tf.constant([1, 2, 3])
x[4]  # Tracing error! 4 is out of bounds.</code></p>
<p>To avoid tracing errors, you can add static shape verifications, which help
make your code more robust:</p>
<p><code>if x.shape[0] &gt; 4:
  val = x[4]
else:
  val = some_default_value</code></p>
<p>In the snippet above, the code is protected against index-out-of-bounds
errors. The code is also efficient because the verification <code>x.shape[0] &gt; 4</code>
will not be included in the graph.</p>
<p>But what happens if you try to perform the index verifications using dynamic
control flow? You might expect that the code works in the same way:</p>
<p><code>val = tf.cond(
  x.shape[0] &gt;= 4,
  lambda: x[4],
  lambda: some_default_value)</code></p>
<p>However, TensorFlow will not let you write code that could result in an error,
even if that code appeared in a branch of a <code>tf.cond</code> statement that would
never execute. Remember that the shape of <code>x</code> is <code>(3,)</code>, so TensorFlow performs
static shape verification.</p>
<p>This can lead to surprising behavior when using <code>tf.shape</code> on tensors with
static shape in TensorFlow:</p>
<p><code>x = tf.constant((1, 2, 3))
if tf.shape(x)[0] &gt; 4:
  val = x[4]  # Error at tracing: 4 is out of bounds!
else:
  val = some_default_value</code></p>
<p>Because <code>tf.shape</code> always evaluates to a Tensor, the <code>if</code> statement above is
converted by AutoGraph into a <code>tf.cond</code>, which performs static shape
verification of both branches.</p>
<p>What if you need to write code which can handle both static and dynamic
shapes? There are a few options in this case:</p>
<p>A first option is to always work with dynamic shapes, for instance by
using <code>input_signature</code> in <code>tf.function</code>. Many shape and shape-related checks
are skipped when the shape is dynamic:</p>
<p><code>@tf.function(input_signature=(tf.TensorSpec(shape=(None,))))
def f(x):  # x now has dynamic shape
  if tf.shape(x)[0] &gt;= 3:  # Builds a tf.cond
    val = x[4]  # Okay, bounds checks are skipped when the shape is dynamic
  else:
    val = some_default_value</code></p>
<p>A second option is to first verify whether the shape is static or dynamic.
This can be done at tracing time, allowing to use Python <code>if</code> to only trace
the code that is suitable for the situation:</p>
<p><code>if x.shape[0] is None:  # Python bool, does not use tf.cond
  # ... use x.shape here ...
else:
  # ... use tf.shape(x) here ...</code></p>
<h4>Consistency of dtype</h4>
<p>The dtypes across all code paths must be consistent in conditionals and loops.</p>
<p>For example, if a <code>tf.cond</code> (and correspondingly, an AutoGraph <code>if</code>) sets a
tensor value conditionally, then that tensor must have the same shape and dtype
in both branches of the conditional.</p>
<p>Example of illegal dtype change in a conditional:</p>
<p><code>x = tf.cond(
    tf.random.uniform(()) &gt; 0.5,
    lambda: tf.constant(1, dtype=tf.int32),
    lambda: tf.constant(1, dtype=tf.float32))  # Error -- inconsistent dtypes: int32, float32</code></p>
<p>The same restriction in AutoGraph code:</p>
<p><code>if tf.random.uniform(()) &gt; 0.5:
  x = tf.constant(1, dtype=tf.int32)
else:
  x = tf.constant(1, dtype=tf.float32)  # Error -- inconsistent dtypes: int32, float32</code></p>
<p>Example of illegal dtype change in a loop:</p>
<p>```</p>
<h1>This won't work - "x" changes dtype inside the loop.</h1>
<p>x = tf.while_loop(
    lambda _: tf.random.uniform(()) &gt; 0.5,
    lambda x: tf.constant(1, dtype=tf.float32),
    loop_vars=(tf.constant(1, dtype=tf.int32),))  # Error -- inconsistent dtypes: int32, float32
```</p>
<p>The same restriction in AutoGraph code:</p>
<p><code>x = tf.constant(0, dtype=tf.int32)
while tf.random.uniform(()) &gt; 0.5:
  x = tf.constant(0, dtype=tf.float32)   # Error -- inconsistent dtypes: int32, float32</code></p>
<h4>Consistency of shape</h4>
<p>The shapes across all code paths must be consistent in loops only. When tensors
do need to change shape across iterations, use <code>shape_invariants</code>.</p>
<p>Note: Shapes are allowed to be inconsistent in conditionals. The result will be
a partially dynamic shape.</p>
<p>In a <code>tf.while_loop</code> (and correspondingly, an AutoGraph <code>while</code> or <code>for</code> loop)
all loop variables must maintain consistent shape and dtype across iterations.
That is, every loop variable must have the same shape at the end of the loop
body as it had at the beginning of the loop body.</p>
<p>Example of illegal shape change in a loop:</p>
<p>```
def loop_body(x):  # x.shape is ()
  return tf.constant((1, 2, 3))  # Error -- inconsistent shapes: (), (3,)</p>
<p>x = tf.while_loop(
    lambda _: tf.random.uniform(()) &gt; 0.5,
    loop_body,
    loop_vars=(tf.constant(1,))
```</p>
<p>The same restriction in AutoGraph code:</p>
<p><code>x = tf.constant(1,)
while tf.random.uniform(()) &gt; 0.5:
  x = tf.constant((1, 2, 3))  # Error -- inconsistent shapes: (), (3,)</code></p>
<h3>Consistency of control flow types</h3>
<p>In AutoGraph, one can write Python control flow like <code>for i in range(10)</code>, as
well as TensorFlow control flow like <code>for i in tf.range(10)</code>.</p>
<p>However, one could also write (illegal) programs which start as Python control
flow, then turn into TensorFlow control flow. In such cases, an error will be
raised.</p>
<p>Below are a few examples, along with recommendations.</p>
<h4>Python loop, TF-dependent break or return</h4>
<p>Example:</p>
<p><code>for i in range(10):
  if tf.greater(i, 3):
    break  # error - TF break inside Python loop</code></p>
<p>The solution in this case is to change the loop type to a TF loop:</p>
<p><code>for i in tf.range(10):
  if tf.greater(i, 3):
    break  # works</code></p>
<h4>Python loop that turns into a TensorFlow loop</h4>
<p>Example:</p>
<p><code>i = 10
while i &gt; 0:
  i = tf.math.subtract(i, 1)  # error - loop would turn into a TF loop</code></p>
<p>The solution in this case is to make sure the loop type starts as a TF loop,
typically by making sure the condition is always a Tensor:</p>
<p><code>i = tf.constant(10)  # works
while i &gt; 0:
  i = tf.math.subtract(i, 1)</code></p>
<h4>TensorFlow loops never turn into Python loops</h4>
<p>Note that this is a legal case, as TensorFlow implicitly converts all Python
values to Tensor:</p>
<p><code>i = tf.constant(10)
while i &gt; 0:
  i = 0  # this is ok, will be auto-converted to Tensor</code></p>
<h3>Access to source code</h3>
<p>Key point: AutoGraph can only handle functions whose source code can be
accessed at runtime.</p>
<p>Almost all Python functions allow access to their source code. However, a few
exceptions exist:</p>
<ul>
<li>functions created in the Python interactive shell</li>
<li>functions with native bindings (these do not have Python source code)</li>
<li>functions created dynamically, using <code>exec</code> or <code>eval</code></li>
</ul>
<p>Use
<a href="https://docs.python.org/3/library/inspect.html#inspect.findsource">inspect.findsource</a>
to quickly diagnose whether the source code is available for a function.</p>
<p>For example:</p>
<p>```
import inspect</p>
<p>def simple_function():
  return 1</p>
<h1>If this raises an error, then AutoGraph prints a warning.</h1>
<h1>If it returns source code, then AutoGraph should work as well.</h1>
<p>inspect.findsource(simple_function)
```</p>
<h4>Source code of lambda functions</h4>
<h5>TF 2.4 and newer</h5>
<p>Key Point: When nesting lambda functions, use distinguishing argument names
to avoid parse errors.</p>
<p>The Python runtime exposes the source code of lambda functions, however it
may omit parts of the actual body, or include surrounding code. This may make it
impossible to parse the exact source code of the lambda function (see
https://github.com/tensorflow/tensorflow/issues/39832).</p>
<p>AutoGraph uses alternate methods to parse the source code more robustly, but
in rare cases it may be unable to distinguish between nested lambda functions
of identical signatures.</p>
<p>Example:</p>
<p><code>l = lambda x: lambda x: x + 1</code></p>
<p>AutoGraph raises an error for the code above because the parser cannot
distinguish between the two function signatures. To work around this limitation,
use distinct argument names:</p>
<p><code>l = lambda outer_x: lambda inner_x: inner_x + 1</code></p>
<h5>Before TF 2.3 and older</h5>
<p>In older versions of TensorFlow, the loading code for lambda functions is not
robust. Follow the guidance below to avoid errors.</p>
<p>Important: Declare lambda functions on single lines to make sure their source
code loads correctly.</p>
<p>The Python runtime exposes the source code of lambda functions, however it
may omit parts of the actual body, or include surrounding code. This may make it
impossible to parse the exact source code of the lambda function.</p>
<p>For example, consider the declaration of a lambda function below:</p>
<p><code>foo = (
    lambda y: lambda x: x * y
    - y
)</code></p>
<p>The Python runtime will report the following source code for <code>foo</code>:</p>
<p>```</p>
<blockquote>
<blockquote>
<blockquote>
<p>inspect.getsource(foo)
'    lambda y: lambda x: x*y \n'
```</p>
</blockquote>
</blockquote>
</blockquote>
<p>In other cases, the source code it returns is not valid Python code, resulting
in an error:</p>
<p><code>foo = (
 'bar',
 lambda: x)</code></p>
<p>The reported source code contains an invalid token <code>)</code>:</p>
<p>```</p>
<blockquote>
<blockquote>
<blockquote>
<p>inspect.getsource(foo[1])
' lambda: x)\n'
```</p>
</blockquote>
</blockquote>
</blockquote>
<p>This shortcoming can be avoided by declaring the lambda in a single assignment
or return value, and avoiding placing it inside parentheses which could cause
auto-formatting tools to break it into multiple lines:</p>
<p>```</p>
<h1>Good - single assignment</h1>
<p>my_lambda = lambda: x</p>
<h1>Good - single return</h1>
<p>return lambda x, y: x*y - y
```</p>
<p>```</p>
<h1>Bad - wrapped in parentheses</h1>
<p>my_lambda = (lambda x, y: x * y - y)</p>
<h1>Bad - inlined in another expression</h1>
<p>foo(lambda x, y: x + y, bar)
```</p>