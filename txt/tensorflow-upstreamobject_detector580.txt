<h1>Integrate object detectors</h1>
<p>Object detectors can identify which of a known set of objects might be present
and provide information about their positions within the given image or a video
stream. An object detector is trained to detect the presence and location of
multiple classes of objects. For example, a model might be trained with images
that contain various pieces of fruit, along with a <em>label</em> that specifies the
class of fruit they represent (e.g. an apple, a banana, or a strawberry), and
data specifying where each object appears in the image. See the
<a href="../../examples/object_detection/overview">introduction of object detection</a>
for more information about object detectors.</p>
<p>Use the Task Library <code>ObjectDetector</code> API to deploy your custom object detectors
or pretrained ones into your mobile apps.</p>
<h2>Key features of the ObjectDetector API</h2>
<ul>
<li>
<p>Input image processing, including rotation, resizing, and color space
    conversion.</p>
</li>
<li>
<p>Label map locale.</p>
</li>
<li>
<p>Score threshold to filter results.</p>
</li>
<li>
<p>Top-k detection results.</p>
</li>
<li>
<p>Label allowlist and denylist.</p>
</li>
</ul>
<h2>Supported object detector models</h2>
<p>The following models are guaranteed to be compatible with the <code>ObjectDetector</code>
API.</p>
<ul>
<li>
<p>The
    <a href="https://tfhub.dev/tensorflow/collections/lite/task-library/object-detector/1">pretrained object detection models on TensorFlow Hub</a>.</p>
</li>
<li>
<p>Models created by
    <a href="https://cloud.google.com/vision/automl/object-detection/docs">AutoML Vision Edge Object Detection</a>.</p>
</li>
<li>
<p>Models created by
    <a href="https://www.tensorflow.org/lite/guide/model_maker">TensorFlow Lite Model Maker for object detector</a>.</p>
</li>
<li>
<p>Custom models that meet the
    <a href="#model-compatibility-requirements">model compatibility requirements</a>.</p>
</li>
</ul>
<h2>Run inference in Java</h2>
<p>See the
<a href="https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android/">Object Detection reference app</a>
for an example of how to use <code>ObjectDetector</code> in an Android app.</p>
<h3>Step 1: Import Gradle dependency and other settings</h3>
<p>Copy the <code>.tflite</code> model file to the assets directory of the Android module
where the model will be run. Specify that the file should not be compressed, and
add the TensorFlow Lite library to the moduleâ€™s <code>build.gradle</code> file:</p>
<p>```java
android {
    // Other settings</p>
<pre><code>// Specify tflite file should not be compressed for the app apk
aaptOptions {
    noCompress "tflite"
}
</code></pre>
<p>}</p>
<p>dependencies {
    // Other dependencies</p>
<pre><code>// Import the Task Vision Library dependency (NNAPI is included)
implementation 'org.tensorflow:tensorflow-lite-task-vision'
// Import the GPU delegate plugin Library for GPU inference
implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin'
</code></pre>
<p>}
```</p>
<p>Note: starting from version 4.1 of the Android Gradle plugin, .tflite will be
added to the noCompress list by default and the aaptOptions above is not needed
anymore.</p>
<h3>Step 2: Using the model</h3>
<p>```java
// Initialization
ObjectDetectorOptions options =
    ObjectDetectorOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setMaxResults(1)
        .build();
ObjectDetector objectDetector =
    ObjectDetector.createFromFileAndOptions(
        context, modelFile, options);</p>
<p>// Run inference
List<Detection> results = objectDetector.detect(image);
```</p>
<p>See the
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/java/src/java/org/tensorflow/lite/task/vision/detector/ObjectDetector.java">source code and javadoc</a>
for more options to configure <code>ObjectDetector</code>.</p>
<h2>Run inference in iOS</h2>
<h3>Step 1: Install the dependencies</h3>
<p>The Task Library supports installation using CocoaPods. Make sure that CocoaPods
is installed on your system. Please see the
<a href="https://guides.cocoapods.org/using/getting-started.html#getting-started">CocoaPods installation guide</a>
for instructions.</p>
<p>Please see the
<a href="https://guides.cocoapods.org/using/using-cocoapods.html">CocoaPods guide</a> for
details on adding pods to an Xcode project.</p>
<p>Add the <code>TensorFlowLiteTaskVision</code> pod in the Podfile.</p>
<p><code>target 'MyAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskVision'
end</code></p>
<p>Make sure that the <code>.tflite</code> model you will be using for inference is present in
your app bundle.</p>
<h3>Step 2: Using the model</h3>
<h4>Swift</h4>
<p>```swift
// Imports
import TensorFlowLiteTaskVision</p>
<p>// Initialization
guard let modelPath = Bundle.main.path(forResource: "ssd_mobilenet_v1",
                                            ofType: "tflite") else { return }</p>
<p>let options = ObjectDetectorOptions(modelPath: modelPath)</p>
<p>// Configure any additional options:
// options.classificationOptions.maxResults = 3</p>
<p>let detector = try ObjectDetector.detector(options: options)</p>
<p>// Convert the input image to MLImage.
// There are other sources for MLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
guard let image = UIImage (named: "cats_and_dogs.jpg"), let mlImage = MLImage(image: image) else { return }</p>
<p>// Run inference
let detectionResult = try detector.detect(mlImage: mlImage)
```</p>
<h4>Objective C</h4>
<p>```objc
// Imports</p>
<h1>import <TensorFlowLiteTaskVision/TensorFlowLiteTaskVision.h></h1>
<p>// Initialization
NSString *modelPath = [[NSBundle mainBundle] pathForResource:@"ssd_mobilenet_v1" ofType:@"tflite"];</p>
<p>TFLObjectDetectorOptions *options = [[TFLObjectDetectorOptions alloc] initWithModelPath:modelPath];</p>
<p>// Configure any additional options:
// options.classificationOptions.maxResults = 3;</p>
<p>TFLObjectDetector *detector = [TFLObjectDetector objectDetectorWithOptions:options
                                                                     error:nil];</p>
<p>// Convert the input image to MLImage.
UIImage *image = [UIImage imageNamed:@"dogs.jpg"];</p>
<p>// There are other sources for GMLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
GMLImage *gmlImage = [[GMLImage alloc] initWithImage:image];</p>
<p>// Run inference
TFLDetectionResult *detectionResult = [detector detectWithGMLImage:gmlImage error:nil];
```</p>
<p>See the
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/ios/task/vision/sources/TFLObjectDetector.h">source code</a>
for more options to configure <code>TFLObjectDetector</code>.</p>
<h2>Run inference in Python</h2>
<h3>Step 1: Install the pip package</h3>
<p><code>pip install tflite-support</code></p>
<h3>Step 2: Using the model</h3>
<p>```python</p>
<h1>Imports</h1>
<p>from tflite_support.task import vision
from tflite_support.task import core
from tflite_support.task import processor</p>
<h1>Initialization</h1>
<p>base_options = core.BaseOptions(file_name=model_path)
detection_options = processor.DetectionOptions(max_results=2)
options = vision.ObjectDetectorOptions(base_options=base_options, detection_options=detection_options)
detector = vision.ObjectDetector.create_from_options(options)</p>
<h1>Alternatively, you can create an object detector in the following manner:</h1>
<h1>detector = vision.ObjectDetector.create_from_file(model_path)</h1>
<h1>Run inference</h1>
<p>image = vision.TensorImage.create_from_file(image_path)
detection_result = detector.detect(image)
```</p>
<p>See the
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/python/task/vision/object_detector.py">source code</a>
for more options to configure <code>ObjectDetector</code>.</p>
<h2>Run inference in C++</h2>
<p>```c++
// Initialization
ObjectDetectorOptions options;
options.mutable_base_options()-&gt;mutable_model_file()-&gt;set_file_name(model_path);
std::unique_ptr<ObjectDetector> object_detector = ObjectDetector::CreateFromOptions(options).value();</p>
<p>// Create input frame_buffer from your inputs, <code>image_data</code> and <code>image_dimension</code>.
// See more information here: tensorflow_lite_support/cc/task/vision/utils/frame_buffer_common_utils.h
std::unique_ptr<FrameBuffer> frame_buffer = CreateFromRgbRawBuffer(
      image_data, image_dimension);</p>
<p>// Run inference
const DetectionResult result = object_detector-&gt;Detect(*frame_buffer).value();
```</p>
<p>See the
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/cc/task/vision/object_detector.h">source code</a>
for more options to configure <code>ObjectDetector</code>.</p>
<h2>Example results</h2>
<p>Here is an example of the detection results of
<a href="https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/1">ssd mobilenet v1</a>
from TensorFlow Hub.</p>
<p><img src="images/dogs.jpg" alt="dogs" width="50%"></p>
<p><code>Results:
 Detection #0 (red):
  Box: (x: 355, y: 133, w: 190, h: 206)
  Top-1 class:
   index       : 17
   score       : 0.73828
   class name  : dog
 Detection #1 (green):
  Box: (x: 103, y: 15, w: 138, h: 369)
  Top-1 class:
   index       : 17
   score       : 0.73047
   class name  : dog</code></p>
<p>Render the bounding boxes onto the input image:</p>
<p><img src="images/detection-output.png" alt="detection output" width="50%"></p>
<p>Try out the simple
<a href="https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/examples/task/vision/desktop#object-detector">CLI demo tool for ObjectDetector</a>
with your own model and test data.</p>
<h2>Model compatibility requirements</h2>
<p>The <code>ObjectDetector</code> API expects a TFLite model with mandatory
<a href="../../models/convert/metadata">TFLite Model Metadata</a>. See examples of creating
metadata for object detectors using the
<a href="../../models/convert/metadata_writer_tutorial.ipynb#object_detectors">TensorFlow Lite Metadata Writer API</a>.</p>
<p>The compatible object detector models should meet the following requirements:</p>
<ul>
<li>
<p>Input image tensor: (kTfLiteUInt8/kTfLiteFloat32)</p>
<ul>
<li>image input of size <code>[batch x height x width x channels]</code>.</li>
<li>batch inference is not supported (<code>batch</code> is required to be 1).</li>
<li>only RGB inputs are supported (<code>channels</code> is required to be 3).</li>
<li>if type is kTfLiteFloat32, NormalizationOptions are required to be
    attached to the metadata for input normalization.</li>
</ul>
</li>
<li>
<p>Output tensors must be the 4 outputs of a <code>DetectionPostProcess</code> op, i.e:</p>
<ul>
<li>Locations tensor (kTfLiteFloat32)<ul>
<li>tensor of size <code>[1 x num_results x 4]</code>, the inner array representing
    bounding boxes in the form [top, left, right, bottom].</li>
<li>BoundingBoxProperties are required to be attached to the metadata
    and must specify <code>type=BOUNDARIES</code> and `coordinate_type=RATIO.</li>
</ul>
</li>
<li>
<p>Classes tensor (kTfLiteFloat32)</p>
<ul>
<li>tensor of size <code>[1 x num_results]</code>, each value representing the
    integer index of a class.</li>
<li>optional (but recommended) label map(s) can be attached as
    AssociatedFile-s with type TENSOR_VALUE_LABELS, containing one label
    per line. See the
    <a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/metadata/python/tests/testdata/object_detector/labelmap.txt">example label file</a>.
    The first such AssociatedFile (if any) is used to fill the
    <code>class_name</code> field of the results. The <code>display_name</code> field is
    filled from the AssociatedFile (if any) whose locale matches the
    <code>display_names_locale</code> field of the <code>ObjectDetectorOptions</code> used at
    creation time ("en" by default, i.e. English). If none of these are
    available, only the <code>index</code> field of the results will be filled.</li>
</ul>
</li>
<li>
<p>Scores tensor (kTfLiteFloat32)</p>
<ul>
<li>tensor of size <code>[1 x num_results]</code>, each value representing the
    score of the detected object.</li>
</ul>
</li>
<li>
<p>Number of detection tensor (kTfLiteFloat32)</p>
<ul>
<li>integer num_results as a tensor of size <code>[1]</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>