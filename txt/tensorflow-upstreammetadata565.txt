<h1>Adding metadata to TensorFlow Lite models</h1>
<p>TensorFlow Lite metadata provides a standard for model descriptions. The
metadata is an important source of knowledge about what the model does and its
input / output information. The metadata consists of both</p>
<ul>
<li>human readable parts which convey the best practice when using the model,
    and</li>
<li>machine readable parts that can be leveraged by code generators, such as the
    <a href="../../inference_with_metadata/codegen.md#generate-model-interfaces-with-tensorflow-lite-code-generator-codegen">TensorFlow Lite Android code generator</a>
    and the
    <a href="../../inference_with_metadata/codegen.md#use-android-studio-ml-model-binding-mlbinding">Android Studio ML Binding feature</a>.</li>
</ul>
<p>All image models published on
<a href="https://tfhub.dev/s?deployment-format=lite">TensorFlow Hub</a> have been populated
with metadata.</p>
<h2>Model with metadata format</h2>
<p><center><img src="../../images/convert/model_with_metadata.png" alt="model_with_metadata" width="70%"></center>
<center>Figure 1. TFLite model with metadata and associated files.</center></p>
<p>Model metadata is defined in
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/metadata/metadata_schema.fbs">metadata_schema.fbs</a>,
a
<a href="https://google.github.io/flatbuffers/index.html#flatbuffers_overview">FlatBuffer</a>
file. As shown in Figure 1, it is stored in the
<a href="https://github.com/tensorflow/tensorflow/blob/bd73701871af75539dd2f6d7fdba5660a8298caf/tensorflow/lite/schema/schema.fbs#L1208">metadata</a>
field of the
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs">TFLite model schema</a>,
under the name, <code>"TFLITE_METADATA"</code>. Some models may come with associated files,
such as
<a href="https://github.com/tensorflow/examples/blob/dd98bc2b595157c03ac9fa47ac8659bb20aa8bbd/lite/examples/image_classification/android/models/src/main/assets/labels.txt#L1">classification label files</a>.
These files are concatenated to the end of the original model file as a ZIP
using the ZipFile
<a href="https://pymotw.com/2/zipfile/#appending-to-files">"append" mode</a> (<code>'a'</code> mode).
TFLite Interpreter can consume the new file format in the same way as before.
See <a href="#pack-the-associated-files">Pack the associated files</a> for more
information.</p>
<p>See the instruction below about how to populate, visualize, and read metadata.</p>
<h2>Setup the metadata tools</h2>
<p>Before adding metadata to your model, you will need to a Python programming
environment setup for running TensorFlow. There is a detailed guide on how to
set this up <a href="https://www.tensorflow.org/install">here</a>.</p>
<p>After setup the Python programming environment, you will need to install
additional tooling:</p>
<p><code>sh
pip install tflite-support</code></p>
<p>TensorFlow Lite metadata tooling supports Python 3.</p>
<h2>Adding metadata using Flatbuffers Python API</h2>
<p>Note: to create metadata for the popular ML tasks supported in
<a href="../../inference_with_metadata/task_library/overview">TensorFlow Lite Task Library</a>,
use the high-level API in the
<a href="metadata_writer_tutorial.ipynb">TensorFlow Lite Metadata Writer Library</a>.</p>
<p>There are three parts to the model metadata in the
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/metadata/metadata_schema.fbs">schema</a>:</p>
<ol>
<li><strong>Model information</strong> - Overall description of the model as well as items
    such as license terms. See
    <a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L640">ModelMetadata</a>.</li>
<li><strong>Input information</strong> - Description of the inputs and pre-processing
    required such as normalization. See
    <a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L590">SubGraphMetadata.input_tensor_metadata</a>.</li>
<li><strong>Output information</strong> - Description of the output and post-processing
    required such as mapping to labels. See
    <a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L599">SubGraphMetadata.output_tensor_metadata</a>.</li>
</ol>
<p>Since TensorFlow Lite only supports single subgraph at this point, the
<a href="../../inference_with_metadata/codegen..md#generate-model-interfaces-with-tensorflow-lite-code-generator-codegen">TensorFlow Lite code generator</a>
and the
<a href="../../inference_with_metadata/codegen.md#use-android-studio-ml-model-binding-mlbinding">Android Studio ML Binding feature</a>
will use <code>ModelMetadata.name</code> and <code>ModelMetadata.description</code>, instead of
<code>SubGraphMetadata.name</code> and <code>SubGraphMetadata.description</code>, when displaying
metadata and generating code.</p>
<h3>Supported Input / Output types</h3>
<p>TensorFlow Lite metadata for input and output are not designed with specific
model types in mind but rather input and output types. It does not matter what
the model functionally does, as long as the input and output types consists of
the following or a combination of the following, it is supported by TensorFlow
Lite metadata:</p>
<ul>
<li>Feature - Numbers which are unsigned integers or float32.</li>
<li>Image - Metadata currently supports RGB and greyscale images.</li>
<li>Bounding box - Rectangular shape bounding boxes. The schema supports
    <a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L214">a variety of numbering schemes</a>.</li>
</ul>
<h3>Pack the associated files</h3>
<p>TensorFlow Lite models may come with different associated files. For example,
natural language models usually have vocab files that map word pieces to word
IDs; classification models may have label files that indicate object categories.
Without the associated files (if there are), a model will not function well.</p>
<p>The associated files can now be bundled with the model through the metadata
Python library. The new TensorFlow Lite model becomes a zip file that contains
both the model and the associated files. It can be unpacked with common zip
tools. This new model format keeps using the same file extension, <code>.tflite</code>. It
is compatible with existing TFLite framework and Interpreter. See
<a href="#pack-metadata-and-associated-files-into-the-model">Pack metadata and associated files into the model</a>
for more details.</p>
<p>The associated file information can be recorded in the metadata. Depending on
the file type and where the file is attached to (i.e. <code>ModelMetadata</code>,
<code>SubGraphMetadata</code>, and <code>TensorMetadata</code>),
<a href="../../inference_with_metadata/codegen.md">the TensorFlow Lite Android code generator</a>
may apply corresponding pre/post processing automatically to the object. See
<a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L77-L127">the \&lt;Codegen usage> section of each associate file type</a>
in the schema for more details.</p>
<h3>Normalization and quantization parameters</h3>
<p>Normalization is a common data preprocessing technique in machine learning. The
goal of normalization is to change the values to a common scale, without
distorting differences in the ranges of values.</p>
<p><a href="https://www.tensorflow.org/lite/performance/model_optimization#model_quantization">Model quantization</a>
is a technique that allows for reduced precision representations of weights and
optionally, activations for both storage and computation.</p>
<p>In terms of preprocessing and post-processing, normalization and quantization
are two independent steps. Here are the details.</p>
<p>|                         | Normalization           | Quantization             |
| :---------------------: | ----------------------- | ------------------------ |
| \                       | <strong>Float model</strong>: \      | <strong>Float model</strong>: \       |
: An example of the       : - mean\: 127.5 \        : - zeroPoint\: 0 \        :
: parameter values of the : - std\: 127.5 \         : - scale\: 1.0 \          :
: input image in          : <strong>Quant model</strong>\: \     : <strong>Quant model</strong>\: \      :
: MobileNet for float and : - mean\: 127.5 \        : - zeroPoint\: 128.0 \    :
: quant models,           : - std\: 127.5           : - scale\:0.0078125f \    :
: respectively.           :                         :                          :
| \                       | \                       | <strong>Float models</strong> does    |
: \                       : \                       : not need quantization. \ :
: \                       : <strong>Inputs</strong>\: If input   : <strong>Quantized model</strong> may  :
: \                       : data is normalized in   : or may not need          :
: When to invoke?         : training, the input     : quantization in pre/post :
:                         : data of inference needs : processing. It depends   :
:                         : to be normalized        : on the datatype of       :
:                         : accordingly. \          : input/output tensors. \  :
:                         : <strong>Outputs</strong>\: output    : - float tensors\: no     :
:                         : data will not be        : quantization in pre/post :
:                         : normalized in general.  : processing needed. Quant :
:                         :                         : op and dequant op are    :
:                         :                         : baked into the model     :
:                         :                         : graph. \                 :
:                         :                         : - int8/uint8 tensors\:   :
:                         :                         : need quantization in     :
:                         :                         : pre/post processing.     :
| \                       | \                       | <strong>Quantize for inputs</strong>: |
: \                       : \                       : \                        :
: Formula                 : normalized_input =      : q = f / scale +          :
:                         : (input - mean) / std    : zeroPoint \              :
:                         :                         : <strong>Dequantize for         :
:                         :                         : outputs</strong>\: \            :
:                         :                         : f = (q - zeroPoint) *    :
:                         :                         : scale                    :
| \                       | Filled by model creator | Filled automatically by  |
: Where are the           : and stored in model     : TFLite converter, and    :
: parameters              : metadata, as            : stored in tflite model   :
:                         : <code>NormalizationOptions</code>  : file.                    :
| How to get the          | Through the             | Through the TFLite       |
: parameters?             : <code>MetadataExtractor</code> API : <code>Tensor</code> API [1] or      :
:                         : [2]                     : through the              :
:                         :                         : <code>MetadataExtractor</code> API  :
:                         :                         : [2]                      :
| Do float and quant      | Yes, float and quant    | No, the float model does |
: models share the same   : models have the same    : not need quantization.   :
: value?                  : Normalization           :                          :
:                         : parameters              :                          :
| Does TFLite Code        | \                       | \                        |
: generator or Android    : Yes                     : Yes                      :
: Studio ML binding       :                         :                          :
: automatically generate  :                         :                          :
: it in data processing?  :                         :                          :</p>
<p>[1] The
<a href="https://github.com/tensorflow/tensorflow/blob/09ec15539eece57b257ce9074918282d88523d56/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Tensor.java#L73">TensorFlow Lite Java API</a>
and the
<a href="https://github.com/tensorflow/tensorflow/blob/09ec15539eece57b257ce9074918282d88523d56/tensorflow/lite/c/common.h#L391">TensorFlow Lite C++ API</a>.
\
[2] The <a href="#read-the-metadata-from-models">metadata extractor library</a></p>
<p>When processing image data for uint8 models, normalization and quantization are
sometimes skipped. It is fine to do so when the pixel values are in the range of
[0, 255]. But in general, you should always process the data according to the
normalization and quantization parameters when applicable.</p>
<p><a href="https://www.tensorflow.org/lite/inference_with_metadata/overview">TensorFlow Lite Task Library</a>
can handle normalization for you if you set up <code>NormalizationOptions</code> in
metadata. Quantization and dequantization processing is always encapsulated.</p>
<h3>Examples</h3>
<p>Note: The export directory specified has to exist before you run the script; it
does not get created as part of the process.</p>
<p>You can find examples on how the metadata should be populated for different
types of models here:</p>
<h4>Image classification</h4>
<p>Download the script
<a href="https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/metadata/metadata_writer_for_image_classifier.py">here</a>
, which populates metadata to
<a href="https://tfhub.dev/tensorflow/lite-model/mobilenet_v1_0.75_160_quantized/1/default/1">mobilenet_v1_0.75_160_quantized.tflite</a>.
Run the script like this:</p>
<p><code>sh
python ./metadata_writer_for_image_classifier.py \
    --model_file=./model_without_metadata/mobilenet_v1_0.75_160_quantized.tflite \
    --label_file=./model_without_metadata/labels.txt \
    --export_directory=model_with_metadata</code></p>
<p>To populate metadata for other image classification models, add the model specs
like
<a href="https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/metadata/metadata_writer_for_image_classifier.py#L63-L74">this</a>
into the script. The rest of this guide will highlight some of the key sections
in the image classification example to illustrate the key elements.</p>
<h3>Deep dive into the image classification example</h3>
<h4>Model information</h4>
<p>Metadata starts by creating a new model info:</p>
<p>```python
from tflite_support import flatbuffers
from tflite_support import metadata as _metadata
from tflite_support import metadata_schema_py_generated as _metadata_fb</p>
<p>""" ... """
"""Creates the metadata for an image classifier."""</p>
<h1>Creates model info.</h1>
<p>model_meta = _metadata_fb.ModelMetadataT()
model_meta.name = "MobileNetV1 image classifier"
model_meta.description = ("Identify the most prominent object in the "
                          "image from a set of 1,001 categories such as "
                          "trees, animals, food, vehicles, person etc.")
model_meta.version = "v1"
model_meta.author = "TensorFlow"
model_meta.license = ("Apache License. Version 2.0 "
                      "http://www.apache.org/licenses/LICENSE-2.0.")
```</p>
<h4>Input / output information</h4>
<p>This section shows you how to describe your model's input and output signature.
This metadata may be used by automatic code generators to create pre- and post-
processing code. To create input or output information about a tensor:</p>
<p>```python</p>
<h1>Creates input info.</h1>
<p>input_meta = _metadata_fb.TensorMetadataT()</p>
<h1>Creates output info.</h1>
<p>output_meta = _metadata_fb.TensorMetadataT()
```</p>
<h4>Image input</h4>
<p>Image is a common input type for machine learning. TensorFlow Lite metadata
supports information such as colorspace and pre-processing information such as
normalization. The dimension of the image does not require manual specification
since it is already provided by the shape of the input tensor and can be
automatically inferred.</p>
<p><code>python
input_meta.name = "image"
input_meta.description = (
    "Input image to be classified. The expected image is {0} x {1}, with "
    "three channels (red, blue, and green) per pixel. Each value in the "
    "tensor is a single byte between 0 and 255.".format(160, 160))
input_meta.content = _metadata_fb.ContentT()
input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()
input_meta.content.contentProperties.colorSpace = (
    _metadata_fb.ColorSpaceType.RGB)
input_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.ImageProperties)
input_normalization = _metadata_fb.ProcessUnitT()
input_normalization.optionsType = (
    _metadata_fb.ProcessUnitOptions.NormalizationOptions)
input_normalization.options = _metadata_fb.NormalizationOptionsT()
input_normalization.options.mean = [127.5]
input_normalization.options.std = [127.5]
input_meta.processUnits = [input_normalization]
input_stats = _metadata_fb.StatsT()
input_stats.max = [255]
input_stats.min = [0]
input_meta.stats = input_stats</code></p>
<h4>Label output</h4>
<p>Label can be mapped to an output tensor via an associated file using
<code>TENSOR_AXIS_LABELS</code>.</p>
<p>```python</p>
<h1>Creates output info.</h1>
<p>output_meta = _metadata_fb.TensorMetadataT()
output_meta.name = "probability"
output_meta.description = "Probabilities of the 1001 labels respectively."
output_meta.content = _metadata_fb.ContentT()
output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()
output_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.FeatureProperties)
output_stats = _metadata_fb.StatsT()
output_stats.max = [1.0]
output_stats.min = [0.0]
output_meta.stats = output_stats
label_file = _metadata_fb.AssociatedFileT()
label_file.name = os.path.basename("your_path_to_label_file")
label_file.description = "Labels for objects that the model can recognize."
label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS
output_meta.associatedFiles = [label_file]
```</p>
<h4>Create the metadata Flatbuffers</h4>
<p>The following code combines the model information with the input and output
information:</p>
<p>```python</p>
<h1>Creates subgraph info.</h1>
<p>subgraph = _metadata_fb.SubGraphMetadataT()
subgraph.inputTensorMetadata = [input_meta]
subgraph.outputTensorMetadata = [output_meta]
model_meta.subgraphMetadata = [subgraph]</p>
<p>b = flatbuffers.Builder(0)
b.Finish(
    model_meta.Pack(b),
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
metadata_buf = b.Output()
```</p>
<h4>Pack metadata and associated files into the model</h4>
<p>Once the metadata Flatbuffers is created, the metadata and the label file are
written into the TFLite file via the <code>populate</code> method:</p>
<p><code>python
populator = _metadata.MetadataPopulator.with_model_file(model_file)
populator.load_metadata_buffer(metadata_buf)
populator.load_associated_files(["your_path_to_label_file"])
populator.populate()</code></p>
<p>You can pack as many associated files as you want into the model through
<code>load_associated_files</code>. However, it is required to pack at least those files
documented in the metadata. In this example, packing the label file is
mandatory.</p>
<h2>Visualize the metadata</h2>
<p>You can use <a href="https://github.com/lutzroeder/netron">Netron</a> to visualize your
metadata, or you can read the metadata from a TensorFlow Lite model into a json
format using the <code>MetadataDisplayer</code>:</p>
<p>```python
displayer = _metadata.MetadataDisplayer.with_model_file(export_model_path)
export_json_file = os.path.join(FLAGS.export_directory,
                                os.path.splitext(model_basename)[0] + ".json")
json_file = displayer.get_metadata_json()</p>
<h1>Optional: write out the metadata as a json file</h1>
<p>with open(export_json_file, "w") as f:
  f.write(json_file)
```</p>
<p>Android Studio also supports displaying metadata through the
<a href="https://developer.android.com/studio/preview/features#tensor-flow-lite-models">Android Studio ML Binding feature</a>.</p>
<h2>Metadata versioning</h2>
<p>The
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/metadata/metadata_schema.fbs">metadata schema</a>
is versioned both by the Semantic versioning number, which tracks the changes of
the schema file, and by the Flatbuffers file identification, which indicates the
true version compatibility.</p>
<h3>The Semantic versioning number</h3>
<p>The metadata schema is versioned by the
<a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L53">Semantic versioning number</a>,
such as MAJOR.MINOR.PATCH. It tracks schema changes according to the rules
<a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L32-L44">here</a>.
See the
<a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L63">history of fields</a>
added after version <code>1.0.0</code>.</p>
<h3>The Flatbuffers file identification</h3>
<p>Semantic versioning guarantees the compatibility if following the rules, but it
does not imply the true incompatibility. When bumping up the MAJOR number, it
does not necessarily mean the backward compatibility is broken. Therefore, we
use the
<a href="https://google.github.io/flatbuffers/md__schemas.html">Flatbuffers file identification</a>,
<a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L61">file_identifier</a>,
to denote the true compatibility of the metadata schema. The file identifier is
exactly 4 characters long. It is fixed to a certain metadata schema and not
subject to change by users. If the backward compatibility of the metadata schema
has to be broken for some reason, the file_identifier will bump up, for example,
from “M001” to “M002”. File_identifier is expected to be changed much less
frequently than the metadata_version.</p>
<h3>The minimum necessary metadata parser version</h3>
<p>The
<a href="https://github.com/tensorflow/tflite-support/blob/4cd0551658b6e26030e0ba7fc4d3127152e0d4ae/tensorflow_lite_support/metadata/metadata_schema.fbs#L681">minimum necessary metadata parser version</a>
is the minimum version of metadata parser (the Flatbuffers generated code) that
can read the metadata Flatbuffers in full. The version is effectively the
largest version number among the versions of all the fields populated and the
smallest compatible version indicated by the file identifier. The minimum
necessary metadata parser version is automatically populated by the
<code>MetadataPopulator</code> when the metadata is populated into a TFLite model. See the
<a href="#read-the-metadata-from-models">metadata extractor</a> for more information on how
the minimum necessary metadata parser version is used.</p>
<h2>Read the metadata from models</h2>
<p>The Metadata Extractor library is convenient tool to read the metadata and
associated files from a models across different platforms (see the
<a href="https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata/java">Java version</a>
and the
<a href="https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata/cc">C++ version</a>).
You can build your own metadata extractor tool in other languages using the
Flatbuffers library.</p>
<h3>Read the metadata in Java</h3>
<p>To use the Metadata Extractor library in your Android app, we recommend using
the
<a href="https://search.maven.org/artifact/org.tensorflow/tensorflow-lite-metadata">TensorFlow Lite Metadata AAR hosted at MavenCentral</a>.
It contains the <code>MetadataExtractor</code> class, as well as the FlatBuffers Java
bindings for the
<a href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/metadata/metadata_schema.fbs">metadata schema</a>
and the
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs">model schema</a>.</p>
<p>You can specify this in your <code>build.gradle</code> dependencies as follows:</p>
<p><code>build
dependencies {
    implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0'
}</code></p>
<p>To use nightly snapshots, make sure that you have added
<a href="https://www.tensorflow.org/lite/android/lite_build#use_nightly_snapshots">Sonatype snapshot repository</a>.</p>
<p>You can initialize a <code>MetadataExtractor</code> object with a <code>ByteBuffer</code> that points
to the model:</p>
<p><code>java
public MetadataExtractor(ByteBuffer buffer);</code></p>
<p>The <code>ByteBuffer</code> must remain unchanged for the entire lifetime of the
<code>MetadataExtractor</code> object. The initialization may fail if the Flatbuffers file
identifier of the model metadata does not match that of the metadata parser. See
<a href="#metadata-versioning">metadata versioning</a> for more information.</p>
<p>With matching file identifiers, the metadata extractor will successfully read
metadata generated from all past and future schema due to the Flatbuffers'
forwards and backward compatibility mechanism. However, fields from future
schemas cannot be extracted by older metadata extractors. The
<a href="#the-minimum-necessary-metadata-parser-version">minimum necessary parser version</a>
of the metadata indicates the minimum version of metadata parser that can read
the metadata Flatbuffers in full. You can use the following method to verify if
the minimum necessary parser version condition is met:</p>
<p><code>java
public final boolean isMinimumParserVersionSatisfied();</code></p>
<p>Passing in a model without metadata is allowed. However, invoking methods that
read from the metadata will cause runtime errors. You can check if a model has
metadata by invoking the <code>hasMetadata</code> method:</p>
<p><code>java
public boolean hasMetadata();</code></p>
<p><code>MetadataExtractor</code> provides convenient functions for you to get the
input/output tensors' metadata. For example,</p>
<p><code>java
public int getInputTensorCount();
public TensorMetadata getInputTensorMetadata(int inputIndex);
public QuantizationParams getInputTensorQuantizationParams(int inputIndex);
public int[] getInputTensorShape(int inputIndex);
public int getoutputTensorCount();
public TensorMetadata getoutputTensorMetadata(int inputIndex);
public QuantizationParams getoutputTensorQuantizationParams(int inputIndex);
public int[] getoutputTensorShape(int inputIndex);</code></p>
<p>Though the
<a href="https://github.com/tensorflow/tensorflow/blob/aa7ff6aa28977826e7acae379e82da22482b2bf2/tensorflow/lite/schema/schema.fbs#L1075">TensorFlow Lite model schema</a>
supports multiple subgraphs, the TFLite Interpreter currently only supports a
single subgraph. Therefore, <code>MetadataExtractor</code> omits subgraph index as an input
argument in its methods.</p>
<h2>Read the associated files from models</h2>
<p>The TensorFlow Lite model with metadata and associated files is essentially a
zip file that can be unpacked with common zip tools to get the associated files.
For example, you can unzip
<a href="https://tfhub.dev/tensorflow/lite-model/mobilenet_v1_0.75_160_quantized/1/metadata/1">mobilenet_v1_0.75_160_quantized</a>
and extract the label file in the model as follows:</p>
<p><code>sh
$ unzip mobilenet_v1_0.75_160_quantized_1_metadata_1.tflite
Archive:  mobilenet_v1_0.75_160_quantized_1_metadata_1.tflite
 extracting: labels.txt</code></p>
<p>You can also read associated files through the Metadata Extractor library.</p>
<p>In Java, pass the file name into the <code>MetadataExtractor.getAssociatedFile</code>
method:</p>
<p><code>java
public InputStream getAssociatedFile(String fileName);</code></p>
<p>Similarly, in C++, this can be done with the method,
<code>ModelMetadataExtractor::GetAssociatedFile</code>:</p>
<p><code>c++
tflite::support::StatusOr&lt;absl::string_view&gt; GetAssociatedFile(
      const std::string&amp; filename) const;</code></p>