<h1>TensorFlow MLIR Dialects</h1>
<h2>Objective</h2>
<p><a href="https://medium.com/tensorflow/mlir-a-new-intermediate-representation-and-compiler-framework-beba999ed18d">MLIR</a>
is the intermediate representation and compiler framework we are investing in to
build the compiler infrastructure for TensorFlow. The representation for
TensorFlow exposed in this document will be what future high-level
transformations will operate on.</p>
<p>We make use of two different dialects to model TensorFlow graphs in MLIR: first
the <code>tf_executor</code> dialect that represents the execution model of the TensorFlow
executor (e.g. control dependencies, deadness propagation) and the <code>tf</code> dialect
which represent the regular operations in a TensorFlow graph (the ones that
don’t have special contract with the executor).</p>
<p>One intent of this design is that TensorFlow 2.x features can choose to target
just the <code>tf</code> dialect, allowing us to phase out the <code>tf_executor</code> dialect in
subsequent TensorFlow releases. The combination of the two dialects allows to
represent arbitrary existing TensorFlow graphs.</p>
<p>The representation in this document does not address the specific needs of
accelerators or "custom backends" for TensorFlow. We plan to provide a generic
infrastructure for replacing the TF/XLA bridge with a more flexible and reusable
system across targets. A later design proposal will address these aspects. Also
this representation does not address shape inference, an independent design
exploration is being conducted separately at the moment.</p>
<h2>TensorFlow Dialect</h2>
<p>The TensorFlow dialect in MLIR is an open dialect (it allows operations that
MLIR doesn't know about) that can contain any TensorFlow operation that does not
have a specific handling by the executor. These operations don’t operate on dead
values, don’t have control dependencies, and execute conceptually in program
order. The form used in this dialect aligns with the direction taken by
TensorFlow 2.0 with tf.function and autograph, as well as with the needs of
other frontends. This should ease the development of analyses and
transformations: optimizations operate on a simpler semantics and local graph
transformations can be validated in a local scope. Simple patterns like folding
<code>x-x</code> into a constant 0 do not need to update any control dependencies. It
should also be easily lowerable towards multiple accelerators and heterogeneous
systems in general.</p>
<p>Operations in this dialect usually operate on tensor and scalar types defined in
the standard dialect. The extra defined types are specific to TensorFlow: <code>QINT</code>
types like !tf_type.qint8 (etc), <code>QUINT</code> types like !tf_type.quint8, all of the
<code>REF</code> types like !tf_type.uint8ref, as well as !tf_type.string,
!tf_type.resource, and !tf_type.variant which correspond to the tensorflow types
of the same name.</p>
<h3>Example:</h3>
<p>Below is an example of a function operating on the TensorFlow dialect:</p>
<p><code>``mlir {.mlir}
/// This is a regular function, taking inputs by value and returning a new value.
/// The body is a regular CFG.
func some_function(%input : tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt; {
  // TensorFlow operations are not variadic: this</code>tf.add` operation always
  // takes two inputs and returns a single output. This simplifies
  // pattern-matching, verification and rewriting.
  %added = tf.Add %input, %input : tensor&lt;<em>xf32&gt;
  // Operations have sequential execution semantics in a basic block, there are
  // no control dependencies.  The compiler can reorder operations according to
  // the as-if rule ( https://en.wikipedia.org/wiki/As-if_rule ).
  %three = arith.constant splat<tensor\<f32>, 3.0>
  %mul = tf.Mul %input, %three : (tensor&lt;</em>xf32&gt;, tensor<f32>) -&gt; tensor&lt;*xf32&gt;</p>
<p>// Only control flow v2 is supported in TF dialect.
  // The tf.If operation takes three functions that accept the same
  // arguments: the condition returns a bool and the two branches must return
  // the same type, which is also the return of the tf.If.
  %value = "tf.If”(%added, %mul)
             {cond: @cond_func, true_branch: @func_foo, false_branch: @func_bar}
                 : (tensor&lt;<em>xf32&gt;, tensor&lt;</em>xf32&gt;) -&gt; tensor&lt;*xf32&gt;</p>
<p>return %value : tensor&lt;*xf32&gt;
}
```</p>
<h2>TensorFlow Executor Dialect</h2>
<p>The <code>tf_executor</code> dialect is intended to model the current TensorFlow executor
semantics and (when combined with the <code>tf</code> dialect) can represent arbitrary
TensorFlow 1.x and 2.x graphs. As such it follows the executor model, including
deadness propagation, concurrent semantics, and control dependencies. The
<code>tf_executor</code> dialect defines two dialect-specific types:</p>
<ul>
<li><code>!tf_executor.control</code> to represent control dependencies.</li>
<li><code>!tf_executor.token</code> to represent the pair of operations modeling
    NextIteration operation.</li>
</ul>
<p>The <code>tf_executor</code> dialect is closed (operations are all known to MLIR) as there
are only 8 TensorFlow ops with specific graph executor behavior and 4 additional
operations to represent islands of predictability.</p>
<p>This dialect models the TensorFlow executor semantics; as such, a large part of
the defined operations are mirroring the
<a href="https://www.tensorflow.org/api_docs/cc/group/control-flow-ops">TensorFlow Control Flow Ops</a>
and
<a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">implement Control Flow In TensorFlow</a>.
Also, almost all the operations accept a variadic number of control tokens and
return an extra control token as output. Except for <code>tf_executor.Merge</code> and
<code>tf_executor.ControlTrigger</code>, operations are propagating deadness: if any of the
input (control and non-control) is dead, all the outputs (control and
non-control) are dead as well. For <code>tf_executor.Merge</code>, the output is dead only
when either an input control token is dead or all of the regular inputs are
dead. For <code>tf_executor.ControlTrigger</code>, a live control output is always produced
even when some control inputs are dead.</p>
<h3><code>tf_executor.graph</code> Operation</h3>
<p>The <code>tf_executor.graph</code> operation contains a region with a single block that
lists the operations in a TensorFlow graph. The operations are topologically
sorted in-order (no cycles are allowed in the SSA values). The execution model
for operations in this block follows the TensorFlow executor semantics:</p>
<ol>
<li>Operations that don’t have any transitive dependencies through the SSA
    def/use chains may be executed in parallel
    (<code>tf_executor.NextIteration.Source</code> is the exception).</li>
<li>SSA values in this block can be implicitly dead. This means that every SSA
    value defined in a <code>tf_executor.graph</code> can be considered implicitly wrapped
    in a conceptual <code>dead_or&lt;T&gt;</code> structure, and includes a runtime flag
    indicating if the value is dead or present. Operations may have special case
    handling of dead values.</li>
<li>Operations in this dialect return a value of type <code>!tf_executor.control</code> as
    last returned value (exceptions are <code>tf_executor.NextIteration.sink</code> and
    <code>tf_executor.fetch</code> which don’t return any value).</li>
</ol>
<p>The <code>tf_executor.graph</code> op only allows specific <code>tf_executor</code> dialect operations
in its body: the <code>tf_executor.graph</code> verifier will reject any unknown operation.
In order to execute standard <code>tf</code> dialect operations (like <code>tf.Add</code>) they must
be wrapped in the <code>tf_executor.island</code> operation.</p>
<p>The <code>tf_executor.graph</code> operation does not accept any operands, inputs are
implicitly captured by the region, representing the feeds to the graph.</p>
<p>The region attached to <code>tf_executor.graph</code> is terminated by a
<code>tf_executor.fetch</code> operation. The non-control operands of the terminator
correspond to the result values (or fetches) of the <code>tf_executor.graph</code>
operation. The behavior is undefined if any of the operands of the
<code>tf_executor.fetch</code> is dead.</p>
<p><code>mlir {.mlir}
%fetches = tf_executor.graph : tensor&lt;*xf32&gt; {
  // Operations in the current block execute when their inputs are ready,
  // possibly concurrently.
  // Only operations in the tf_executor dialect are expected here.
  // Ops can return multiple outputs and a control token for control
  // dependencies.
  // We don’t mention the control token in the return type here, it is implicit.
  %0, %ctl0 = tf_executor.opA %feed#0, %feed#1 : tensor&lt;*xf32&gt;
  %1, %ctl1 = tf_executor.opB : tensor&lt;*xf32&gt;
  %2, %ctl2 = tf_executor.opC %1, %ctl0 : tensor&lt;*xf32&gt;
  %3, %ctl3 = tf_executor.opD %2 : tensor&lt;*xf32&gt;
  tf_executor.fetch %3 : tensor&lt;*xf32&gt;
} // end of the “tf_executor.graph" operation/region</code></p>
<h3>‘tf_executor.island’ Operation</h3>
<p>The <code>tf_executor.graph</code> operation does not allow <code>tf</code> dialect operations to be
immediately nested underneath it. The <code>tf_executor.island</code> is introduced as a
wrapper for general computation (for example, all the <code>tf</code> dialect operations):
this results in a more consistent representation which makes analysis and
transformation simpler.</p>
<p>The <code>tf_executor.island</code> operation has a single region with a single block
attached (only functional control flow is allowed). The block is terminated by a
<code>tf_executor.yield</code> operation. The operands of the terminator correspond to the
result values of the <code>tf_executor.graph</code> operation. An extra result of type
<code>!_tf_executor.control</code> is always produced by every <code>tf_executor.island</code>.</p>
<p>Within an island, execution semantics follow standard sequential behavior
consistent with the direction of TensorFlow 2.0 and autograph, and desirable for
compiler analyses and transformations. Values in an island can’t be dead. Other
nested <code>tf_executor.graph</code> operations can be present in the region (or called
functions) to re-enable the TensorFlow executor behavior for a subsection of the
code. This is important for the following reasons:</p>
<ul>
<li>Initially the functional control flow operations are calling functions
    involving nested graphs, if <code>tf_executor.graph</code> weren’t allowed in an
    island, these operations would need to have an equivalent in the
    <code>tf_executor</code> dialect.</li>
<li>Nesting also allows to form islands without involving inter-procedural
    analyzes: any function call may involve a callee with a graph.</li>
</ul>
<p>The <code>tf_executor.island</code> region allows implicit capture. If any value captured
by a <code>tf_executor.island</code> is dead, the whole region does not execute and every
produced value is marked as dead as well.</p>
<p>An arbitrary number of <code>tf_executor.control</code> operands are accepted by a
<code>tf_executor.island</code> operation. If any operand is dead, the region is not
executed and dead values are immediately returned for every result.</p>
<p>```mlir {.mlir}
// The island is capturing implicitly %0 and %1. It is also taking a control
// dependency %ctl0 as input. It produces a tensor&lt;<em>xf32&gt; value matching the
// argument of the yield terminator, as well as an extra control token.
%2, %ctl2 = tf_executor.island (%ctl0)
                  : (tensor&lt;</em>xf32&gt;, !tf_executor&lt;"control"&gt;) -&gt; tensor&lt;<em>xf32&gt; {
  %added = tf.Add %1, %0 : tensor&lt;</em>xf32&gt;
  %mul = tf.Mul %added, %1 :tensor&lt;*xf32&gt;</p>
<p>// The yield terminator operands are the result values of the island.
  tf_executor.yield %mul : tensor&lt;*xf32&gt;
}
```</p>
<p>The case where a single operation is wrapped inside an island can even be
compressed by inferring the terminator to be the returned value of the
operation. The example above if it only contained the addition with implicit
capture would be displayed as:</p>
<p><code>mlir {.mlir}
%2, %ctl2 = tf_executor.island(%ctl0) wraps tf.Add %1, %0 : tensor&lt;*xf32&gt;</code></p>
<h3><code>tf_executor.Switch</code> Operation</h3>
<p><a href="https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/switch"><code>tf_executor.Switch</code></a>:
takes two inputs,<code>predicate</code>and<code>data</code>and returns two regular
outputs,<code>true_output</code>,<code>false_output</code>. The<code>data</code>input is copied
to<code>true_output</code>if<code>predicate</code>evaluates to true otherwise it is copied
to<code>false_output</code>. The other output is marked as dead. If one of the inputs or a
control token is dead, then all of the outputs are marked as dead as well.</p>
<h3><code>tf_executor.SwitchN</code> Operation</h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/control_flow_ops.cc#L49-L53"><code>tf_executor.SwitchN</code></a>:
takes two inputs,<code>data</code>and<code>index</code>and an integer attribute<code>num_outs</code>indicating
the number of outputs. The<code>data</code>input is copied to output indicated by
the<code>index</code> input. The other outputs are marked as dead. If one of the inputs or
a control token is dead, then all of the outputs are marked as dead as well.</p>
<h3><code>tf_executor.Merge</code> Operation</h3>
<p><a href="https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/merge"><code>tf_executor.Merge</code></a>:
takes a variadic number of inputs, and returns a single output. The output is
defined as a non-dead input (selected in a non-defined way if multiple inputs
are non-dead). If all inputs are dead, the output is also dead.</p>
<h3>NextIteration: <code>tf_executor.NextIteration.Source</code> and <code>tf_executor.NextIteration.Sink</code> Operation</h3>
<p>The TensorFlow
<a href="https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/next-iteration"><code>NextIteration</code></a>
op is modeled using these two paired operations. Since <em>NextIteration</em> is
intended for modeling the loop back-edges, breaking it in two different
operations allows to keep a structural
DAG.<code>tf_executor.NextIteration.Source</code>does not take any operand and produces two
results: one regular value corresponding to the TensorFlow graph, and a second
value of type<code>tf_executor.loop_token</code>. This token is consumed by the
paired<code>tf_executor.NextIteration.Sink</code>Operation alongside the value that is
passed through the back-edge. No value is returned
by<code>tf_executor.NextIteration.Sink</code>. The type of the result of the source must
match the type of the value operand of the sink.</p>
<p><code>tf_executor.NextIteration.Source</code> is an exception in the executor model in the
sense that it executes after the paired <code>tf_executor.NextIteration.Sink</code> even
though there is no data dependency between them.</p>
<h3><code>tf_executor.LoopCond</code> Operation</h3>
<p><a href="https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/loop-cond"><code>tf_executor.LoopCond</code></a>:
forwards its boolean input to its output,
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/control_flow_ops.h#L115-L118">it acts as<code>pivot</code> for marking the loop termination condition</a>.</p>
<h3><code>tf_executor.Enter</code> Operation</h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/control_flow_ops.h##77-L79"><code>tf_executor.Enter</code></a>:
takes a single input and a<code>name</code> string attribute that identifies the execution
frame. It forwards its input to its output in the new execution frame.</p>
<h3><code>tf_executor.Exit</code> Operation</h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/control_flow_ops.h#L90-L92"><code>tf_executor.Exit</code></a>:
forwards its single input to its output, exiting the current execution frame.</p>
<h3><code>tf_executor.ControlTrigger</code> Operation</h3>
<p><a href="https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/control-trigger"><code>tf_executor.ControlTrigger</code></a>:
it is similar to
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/control_flow_ops.h#L23-L26">a no-op</a>
that acts as a placeholder for control dependencies. It always produces a live
control output even when some control inputs are dead.</p>
<h3><code>tf_executor.Send</code> Operation</h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sendrecv_ops.h#L24"><code>tf_executor.Send</code></a>:
matches TensorFlow semantics.</p>
<h3><code>tf_executor.Recv</code> Operation</h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sendrecv_ops.h#L37"><code>tf_executor.Recv</code></a>:
matches TensorFlow semantics.</p>
<h2>Example</h2>
<p>Below is an example of a loop decrementing an initial <code>%_count.init</code> integer
until it reaches 0 and returns the last value in the loop.</p>
<p><code>``mlir {.mlir}
// Loop</code>%count.init` times and return the last counter (always zero)
%fetches = tf_executor.graph {</p>
<p>%loop.init, %ctl0 = tf_executor.Enter %count.init : i32</p>
<p>%next_count, %tok = tf_executor.NextIteration.Source : i32</p>
<p>%loop.body.init, %ctlMerge = tf_executor.Merge %loop.init, %next_count : i32</p>
<p>%dec_count, %ctlAdd = tf_executor.island
    wraps tf.Add %loop.body.init, -1 : (i32, i32) -&gt; i32</p>
<p>%loop_cond, %ctlNE = tf_executor.island
    wraps tf.NotEqual %dec_count, 0 : (i32, i32) -&gt; i1</p>
<p>%true, %false, %ctlSwitch = tf_executor.Switch %loop_cond, %dec_count  : i32</p>
<p>tf_executor.NextIteration.Sink[%tok] %false : i32</p>
<p>%exit_count, %ctlExit = tf_executor.Exit %true : i32</p>
<p>tf_executor.fetch %exit_count : i32
} // end of the "tf_executor.graph" operation/region
```</p>