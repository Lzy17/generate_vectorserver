<h1>Object detection with Android</h1>
<p>This tutorial shows you how to build an Android app using TensorFlow Lite to
continuously detect objects in frames captured by a device camera. This
application is designed for a physical Android device. If you are updating an
existing project, you can use the code sample as a reference and skip ahead to
the instructions for <a href="#add_dependencies">modifying your project</a>.</p>
<p><img alt="Object detection animated demo" src="https://storage.googleapis.com/download.tensorflow.org/tflite/examples/obj_detection_cat.gif" />{: .attempt-right width="250px"}</p>
<h2>Object detection overview</h2>
<p><em>Object detection</em> is the machine learning task of identifying the presence and
location of multiple classes of objects within an image. An object detection
model is trained on a dataset that contains a set of known objects.</p>
<p>The trained model receives image frames as input and attempts to categorize
items in the images from the set of known classes it was trained to recognize.
For each image frame, the object detection model outputs a list of the objects
it detects, the location of a bounding box for each object, and a score that
indicates the confidence of the object being correctly classified.</p>
<h2>Models and dataset</h2>
<p>This tutorial uses models that were trained using the
<a href="http://cocodataset.org/">COCO dataset</a>. COCO is a large-scale object detection
dataset that contains 330K images, 1.5 million object instances, and 80 object
categories.</p>
<p>You have the option to use one of the following pre-trained models:</p>
<ul>
<li>
<p><a href="https://tfhub.dev/tensorflow/lite-model/efficientdet/lite0/detection/metadata/1">EfficientDet-Lite0</a>
    <em>[Recommended]</em> - a lightweight object detection model with a BiFPN feature
    extractor, shared box predictor, and focal loss. The mAP (mean Average
    Precision) for the COCO 2017 validation dataset is 25.69%.</p>
</li>
<li>
<p><a href="https://tfhub.dev/tensorflow/lite-model/efficientdet/lite1/detection/metadata/1">EfficientDet-Lite1</a> -
    a medium-sized EfficientDet object detection model. The mAP for the COCO
    2017 validation dataset is 30.55%.</p>
</li>
<li>
<p><a href="https://tfhub.dev/tensorflow/lite-model/efficientdet/lite2/detection/metadata/1">EfficientDet-Lite2</a> -
    a larger EfficientDet object detection model. The mAP for the COCO 2017
    validation dataset is 33.97%.</p>
</li>
<li>
<p><a href="https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2">MobileNetV1-SSD</a> -
    an extremely lightweight model optimized to work with TensorFlow Lite for
    object detection. The mAP for the COCO 2017 validation dataset is 21%.</p>
</li>
</ul>
<p>For this tutorial, the <em>EfficientDet-Lite0</em> model strikes a good balance between
size and accuracy.</p>
<p>Downloading, extraction, and placing the models into the assets folder is
managed automatically by the <code>download.gradle</code> file, which is run at build time.
You don't need to manually download TFLite models into the project.</p>
<h2>Setup and run example</h2>
<p>To setup the object detection app, download the sample from
<a href="https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android">GitHub</a>
and run it using <a href="https://developer.android.com/studio/">Android Studio</a>. The
following sections of this tutorial explore the relevant sections of the code
example, so you can apply them to your own Android apps.</p>
<h3>System requirements</h3>
<ul>
<li><strong><a href="https://developer.android.com/studio/index.html">Android Studio</a></strong>
    version 2021.1.1 (Bumblebee) or higher.</li>
<li>Android SDK version 31 or higher</li>
<li>Android device with a minimum OS version of SDK 24 (Android 7.0 -
    Nougat) with developer mode enabled.</li>
</ul>
<p>Note: This example uses a camera, so run it on a physical Android device.</p>
<h3>Get the example code</h3>
<p>Create a local copy of the example code. You will use this code to create a
project in Android Studio and run the sample application.</p>
<p>To clone and setup the example code:</p>
<ol>
<li>Clone the git repository
    <pre class="devsite-click-to-copy">
    git clone https://github.com/tensorflow/examples.git
    </pre></li>
<li>Optionally, configure your git instance to use sparse checkout, so you have only
    the files for the object detection example app:
    <pre class="devsite-click-to-copy">
    cd examples
    git sparse-checkout init --cone
    git sparse-checkout set lite/examples/object_detection/android
    </pre></li>
</ol>
<h3>Import and run the project</h3>
<p>Create a project from the downloaded example code, build the project, and then
run it.</p>
<p>To import and build the example code project:</p>
<ol>
<li>Start <a href="https://developer.android.com/studio">Android Studio</a>.</li>
<li>From the Android Studio, select <strong>File &gt; New &gt; Import Project</strong>.</li>
<li>Navigate to the example code directory containing the build.gradle file
    (<code>.../examples/lite/examples/object_detection/android/build.gradle</code>) and
    select that directory.</li>
<li>If Android Studio requests a Gradle Sync, choose OK.</li>
<li>Ensure that your Android device is connected to your computer and developer
    mode is enabled. Click the green <code>Run</code> arrow.</li>
</ol>
<p>If you select the correct directory, Android Studio creates a new project and
builds it. This process can take a few minutes, depending on the speed of your
computer and if you have used Android Studio for other projects. When the build
completes, the Android Studio displays a <code>BUILD SUCCESSFUL</code> message in the
<strong>Build Output</strong> status panel.</p>
<p>Note: The example code is built with Android Studio 4.2.2, but works with
earlier versions of Studio. If you are using an earlier version of Android
Studio you can try adjust the version number of the Android plugin so that the
build completes, instead of upgrading Studio.</p>
<p><strong>Optional:</strong> To fix build errors by updating the Android plugin version:</p>
<ol>
<li>Open the build.gradle file in the project directory.</li>
<li>
<p>Change the Android tools version as follows:</p>
<p><code>// from: classpath
'com.android.tools.build:gradle:4.2.2'
// to: classpath
'com.android.tools.build:gradle:4.1.2'</code></p>
</li>
<li>
<p>Sync the project by selecting: <strong>File &gt; Sync Project with Gradle Files</strong>.</p>
</li>
</ol>
<p>To run the project:</p>
<ol>
<li>From Android Studio, run the project by selecting <strong>Run &gt; Runâ€¦</strong>.</li>
<li>Select an attached Android device with a camera to test the app.</li>
</ol>
<p>The next sections show you the modifications you need to make to your existing
project to add this functionality to your own app, using this example app as a
reference point.</p>
<h2>Add project dependencies {:#add_dependencies}</h2>
<p>In your own application, you must add specific project dependencies to run
TensorFlow Lite machine learning models, and access utility functions that
convert data such as images, into a tensor data format that can be processed by
the model you are using.</p>
<p>The example app uses the TensorFlow Lite
<a href="../../inference_with_metadata/task_library/overview#supported_tasks">Task library for vision</a>
to enable execution of the object detection machine learning model. The
following instructions explain how to add the required library dependencies to
your own Android app project.</p>
<p>The following instructions explain how to add the required project and module
dependencies to your own Android app project.</p>
<p>To add module dependencies:</p>
<ol>
<li>
<p>In the module that uses TensorFlow Lite, update the module's <code>build.gradle</code>
    file to include the following dependencies. In the example code, this file
    is located here:
    <code>...examples/lite/examples/object_detection/android/app/build.gradle</code>
    (<a href="https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/build.gradle">code reference</a>)</p>
<p><code>dependencies {
  ...
  implementation 'org.tensorflow:tensorflow-lite-task-vision:0.4.0'
  // Import the GPU delegate plugin Library for GPU inference
  implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.0'
  implementation 'org.tensorflow:tensorflow-lite-gpu:2.9.0'
}</code></p>
<p>The project must include the Vision task library
(<code>tensorflow-lite-task-vision</code>). The graphics processing unit (GPU) library
(<code>tensorflow-lite-gpu-delegate-plugin</code>) provides the infrastructure to run
the app on GPU, and Delegate (<code>tensorflow-lite-gpu</code>) provides the
compatibility list.</p>
</li>
<li>
<p>In Android Studio, sync the project dependencies by selecting: <strong>File &gt; Sync
    Project with Gradle Files</strong>.</p>
</li>
</ol>
<h2>Initialize the ML model</h2>
<p>In your Android app, you must initialize the TensorFlow Lite machine learning
model with parameters before running predictions with the model. These
initialization parameters are consistent across object detection models and can
include settings such as minimum accuracy thresholds for predictions.</p>
<p>A TensorFlow Lite model includes a <code>.tflite</code> file containing the model code and
frequently includes a labels file containing the names of the classes predicted
by the model. In the case of object detection, classes are objects such as a
person, dog, cat, or car.</p>
<p>This example downloads several models that are specified in
<code>download_models.gradle</code>, and the <code>ObjectDetectorHelper</code> class provides a
selector for the models:</p>
<p><code>val modelName =
  when (currentModel) {
    MODEL_MOBILENETV1 -&gt; "mobilenetv1.tflite"
    MODEL_EFFICIENTDETV0 -&gt; "efficientdet-lite0.tflite"
    MODEL_EFFICIENTDETV1 -&gt; "efficientdet-lite1.tflite"
    MODEL_EFFICIENTDETV2 -&gt; "efficientdet-lite2.tflite"
    else -&gt; "mobilenetv1.tflite"
  }</code></p>
<p>Key Point: Models should be stored in the <code>src/main/assets</code> directory of your
development project. The TensorFlow Lite Task library automatically checks this
directory when you specify a model file name.</p>
<p>To initialize the model in your app:</p>
<ol>
<li>Add a <code>.tflite</code> model file to the <code>src/main/assets</code> directory of your
    development project, such as:
    <a href="https://tfhub.dev/tensorflow/lite-model/efficientdet/lite0/detection/metadata/1">EfficientDet-Lite0</a>.</li>
<li>Set a static variable for your model's file name. In the example app, you
    set the <code>modelName</code> variable to <code>MODEL_EFFICIENTDETV0</code> to use the
    EfficientDet-Lite0 detection model.</li>
<li>
<p>Set the options for model, such as the prediction threshold, results set
    size, and optionally, hardware acceleration delegates:</p>
<p><code>val optionsBuilder =
  ObjectDetector.ObjectDetectorOptions.builder()
    .setScoreThreshold(threshold)
    .setMaxResults(maxResults)</code></p>
</li>
<li>
<p>Use the settings from this object to construct a TensorFlow Lite
    <a href="https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/task/vision/detector/ObjectDetector#createFromFile(Context,%20java.lang.String)"><code>ObjectDetector</code></a>
    object that contains the model:</p>
<p><code>objectDetector =
  ObjectDetector.createFromFileAndOptions(
    context, modelName, optionsBuilder.build())</code></p>
</li>
</ol>
<p>The <code>setupObjectDetector</code> sets up the following model parameters:</p>
<ul>
<li>Detection threshold</li>
<li>Maximum number of detection results</li>
<li>Number of processing threads to use
    (<code>BaseOptions.builder().setNumThreads(numThreads)</code>)</li>
<li>Actual model (<code>modelName</code>)</li>
<li>ObjectDetector object (<code>objectDetector</code>)</li>
</ul>
<h3>Configure hardware accelerator</h3>
<p>When initializing a TensorFlow Lite model in your application, you can use
hardware acceleration features to speed up the prediction calculations of the
model.</p>
<p>TensorFlow Lite <em>delegates</em> are software modules that accelerate execution of
machine learning models using specialized processing hardware on a mobile
device, such as Graphics Processing Units (GPUs), Tensor Processing Units
(TPUs), and Digital Signal Processors (DSPs). Using delegates for running
TensorFlow Lite models is recommended, but not required.</p>
<p>The object detector is initialized using the current settings on the thread that
is using it. You can use CPU and <a href="../../android/delegates/nnapi">NNAPI</a>
delegates with detectors that are created on the main thread and used on a
background thread, but the thread that initialized the detector must use the GPU
delegate.</p>
<p>The delegates are set within the <code>ObjectDetectionHelper.setupObjectDetector()</code>
function:</p>
<p><code>when (currentDelegate) {
    DELEGATE_CPU -&gt; {
        // Default
    }
    DELEGATE_GPU -&gt; {
        if (CompatibilityList().isDelegateSupportedOnThisDevice) {
            baseOptionsBuilder.useGpu()
        } else {
            objectDetectorListener?.onError("GPU is not supported on this device")
        }
    }
    DELEGATE_NNAPI -&gt; {
        baseOptionsBuilder.useNnapi()
    }
}</code></p>
<p>For more information about using hardware acceleration delegates with TensorFlow
Lite, see <a href="../../performance/delegates">TensorFlow Lite Delegates</a>.</p>
<h2>Prepare data for the model</h2>
<p>In your Android app, your code provides data to the model for interpretation by
transforming existing data such as image frames into a Tensor data format that
can be processed by your model. The data in a Tensor you pass to a model must
have specific dimensions, or shape, that matches the format of data used to
train the model.</p>
<p>The
<a href="https://tfhub.dev/tensorflow/lite-model/efficientdet/lite0/detection/metadata/1">EfficientDet-Lite0</a>
model used in this code example accepts Tensors representing images with a
dimension of 320 x 320, with three channels (red, blue, and green) per pixel.
Each value in the tensor is a single byte between 0 and 255. So, to run
predictions on new images, your app must transform that image data into Tensor
data objects of that size and shape. The TensorFlow Lite Task Library Vision API
handles the data transformation for you.</p>
<p>The app uses an
<a href="https://developer.android.com/training/camerax/analyze"><code>ImageAnalysis</code></a> object
to pull images from the camera. This object calls the <code>detectObject</code> function
with bitmap from the camera. The data is automatically resized and rotated by
<code>ImageProcessor</code> so that it meets the image data requirements of the model. The
image is then translated into a
<a href="https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/support/image/TensorImage"><code>TensorImage</code></a>
object.</p>
<p>To prepare data from the camera subsystem to be processed by the ML model:</p>
<ol>
<li>
<p>Build an <code>ImageAnalysis</code> object to extract images in the required format:</p>
<p><code>imageAnalyzer =
    ImageAnalysis.Builder()
        .setTargetAspectRatio(AspectRatio.RATIO_4_3)
        .setTargetRotation(fragmentCameraBinding.viewFinder.display.rotation)
        .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
        .setOutputImageFormat(OUTPUT_IMAGE_FORMAT_RGBA_8888)
        .build()
        ...</code></p>
</li>
<li>
<p>Connect the analyzer to the camera subsystem and create a bitmap buffer to
    contain the data received from the camera:</p>
<p><code>.also {
  it.setAnalyzer(cameraExecutor) {
    image -&gt; if (!::bitmapBuffer.isInitialized)
    { bitmapBuffer = Bitmap.createBitmap( image.width, image.height,
    Bitmap.Config.ARGB_8888 ) } detectObjects(image)
    }
  }</code></p>
</li>
<li>
<p>Extract the specific image data needed by the model, and pass the image
    rotation information:</p>
<p><code>private fun detectObjects(image: ImageProxy) {
  //Copy out RGB bits to the shared bitmap buffer
  image.use {bitmapBuffer.copyPixelsFromBuffer(image.planes[0].buffer) }
    val imageRotation = image.imageInfo.rotationDegrees
    objectDetectorHelper.detect(bitmapBuffer, imageRotation)
  }</code></p>
</li>
<li>
<p>Complete any final data transformations and add the image data to a
    <code>TensorImage</code> object, as shown in the <code>ObjectDetectorHelper.detect()</code> method
    of the example app:</p>
<p><code>val imageProcessor = ImageProcessor.Builder().add(Rot90Op(-imageRotation / 90)).build()
// Preprocess the image and convert it into a TensorImage for detection.
val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))</code></p>
</li>
</ol>
<p>Note: When extracting image information from the Android camera subsystem, make
sure to get an image in RGB format. This format is required by the TensorFlow
Lite ImageProcessor class which you use to prepare the image for analysis by a
model. If the RGB-format image contains an Alpha channel, that transparency data
is ignored.</p>
<h2>Run predictions</h2>
<p>In your Android app, once you create a TensorImage object with image data in the
correct format, you can run the model against that data to produce a prediction,
or <em>inference</em>.</p>
<p>In the <code>fragments/CameraFragment.kt</code> class of the example app, the
<code>imageAnalyzer</code> object within the <code>bindCameraUseCases</code> function automatically
passes data to the model for predictions when the app is connected to the
camera.</p>
<p>The app uses the <code>cameraProvider.bindToLifecycle()</code> method to handle the camera
selector, preview window, and ML model processing. The <code>ObjectDetectorHelper.kt</code>
class handles passing the image data into the model. To run the model and
generate predictions from image data:</p>
<ul>
<li>
<p>Run the prediction by passing the image data to your predict function:</p>
<p><code>val results = objectDetector?.detect(tensorImage)</code></p>
</li>
</ul>
<p>The TensorFlow Lite Interpreter object receives this data, runs it against the
model, and produces a list of predictions. For continuous processing of data by
the model, use the <code>runForMultipleInputsOutputs()</code> method so that Interpreter
objects are not created and then removed by the system for each prediction run.</p>
<h2>Handle model output</h2>
<p>In your Android app, after you run image data against the object detection
model, it produces a list of predictions which your app code must handle by
executing additional business logic, displaying results to the user, or taking
other actions.</p>
<p>The output of any given TensorFlow Lite model varies in terms of the number of
predictions it produces (one or many), and the descriptive information for each
prediction. In the case of an object detection model, predictions typically
include data for a bounding box that indicates where an object is detected in
the image. In the example code, the results are passed to the <code>onResults</code>
function in <code>CameraFragment.kt</code>, which is acting as a DetectorListener on the
object detection process.</p>
<p><code>interface DetectorListener {
  fun onError(error: String)
  fun onResults(
    results: MutableList&lt;Detection&gt;?,
    inferenceTime: Long,
    imageHeight: Int,
    imageWidth: Int
  )
}</code></p>
<p>For the model used in this example, each prediction includes a bounding box
location for the object, a label for the object, and a prediction score between
0 and 1 as a Float representing the confidence of the prediction, with 1 being
the highest confidence rating. In general, predictions with a score below 50%
(0.5) are considered inconclusive. However, how you handle low-value prediction
results is up to you and the needs of your application.</p>
<p>To handle model prediction results:</p>
<ol>
<li>
<p>Use a listener pattern to pass results to your app code or user interface
    objects. The example app uses this pattern to pass detection results from
    the <code>ObjectDetectorHelper</code> object to the <code>CameraFragment</code> object:</p>
<p><code>objectDetectorListener.onResults(
// instance of CameraFragment
    results,
    inferenceTime,
    tensorImage.height,
    tensorImage.width)</code></p>
</li>
<li>
<p>Act on the results, such as displaying the prediction to the user. The
    example draws an overlay on the CameraPreview object to show the result:</p>
<p>```
override fun onResults(
  results: MutableList<Detection>?,
  inferenceTime: Long,
  imageHeight: Int,
  imageWidth: Int
) {
    activity?.runOnUiThread {
        fragmentCameraBinding.bottomSheetLayout.inferenceTimeVal.text =
            String.format("%d ms", inferenceTime)</p>
<pre><code>    // Pass necessary information to OverlayView for drawing on the canvas
    fragmentCameraBinding.overlay.setResults(
        results ?: LinkedList&lt;Detection&gt;(),
        imageHeight,
        imageWidth
    )

    // Force a redraw
    fragmentCameraBinding.overlay.invalidate()
}
</code></pre>
<p>}
```</p>
</li>
</ol>
<p>Once the model has returned a prediction result, your application can act on
that prediction by presenting the result to your user or executing additional
logic. In the case of the example code, the application draws a bounding box
around the identified object and displays the class name on screen.</p>
<h2>Next steps</h2>
<ul>
<li>Explore various uses of TensorFlow Lite in the <a href="../../examples">examples</a>.</li>
<li>Learn more about using machine learning models with TensorFlow Lite in the
    <a href="../../models">Models</a> section.</li>
<li>Learn more about implementing machine learning in your mobile application in
    the <a href="../../guide">TensorFlow Lite Developer Guide</a>.</li>
</ul>