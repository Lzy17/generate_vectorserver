<h1>TOSA Lowerings</h1>
<h2>Introduction</h2>
<h3>Overview</h3>
<p>This document provides pseudo-code lowerings from TensorFlow and TensorFlow Lite
MLIR Dialects (https://www.tensorflow.org/mlir/dialects) to the TOSA Dialect
(https://mlir.llvm.org/docs/Dialects/TOSA/).</p>
<p>The documentation is a work-in-progress: sections with missing legalizations are
in the process of being written.</p>
<h2>Syntax</h2>
<p>The pseudo-code syntax used in this document is described below.</p>
<h3>Primitive Datatypes</h3>
<p>int8: signed 8-bit integer uint8: unsigned 8-bit integer int16: signed 16-bit
integer int32: signed 32-bit integer int64: signed 32-bit integer uint32:
unsigned 32-bit integer float32: IEEE-754 32-bit floating point format float64:
IEEE-754 64-bit floating point format bool: boolean</p>
<h3>Value</h3>
<p>In pseudo-code, symbol starting with "%" indicates it’s a value. A value is
evaluated by an operator at run time, and operator can consume and can only
consume a list of values as operands. Note value’s tensor type is determined at
compile time. Only the evaluation happens at run time One can easily construct a
data flow subgraph by looking at the producer/consumer.</p>
<h3>Tensor Type</h3>
<p>Tensor type is an attribute determined by legalization at compile time,
describing the shape and element data type. It’s noted as tensor&lt;shape,
dtype&gt;, or shorthanded as tensor&lt;%t.type&gt;</p>
<h3>Operator Prototype</h3>
<p>In pseudocode an TOSA operator is prototyped as following format.</p>
<p>%&lt;output_value&gt; = tosa.&lt;OPERATOR&gt;(%&lt;input_value&gt;)
{&lt;attribute = …​}</p>
<h3>Value Attributes</h3>
<p>For the purposes of brevity and clarity in this document, the pseudocode allows
the following notation on value attribute.</p>
<p>Shorthand         | Description
----------------- | ---------------------------------------------------
<code>%t.shape</code>        | Shape vector for the tensor
<code>%t.shape[i]</code>     | Size of dimension i for the tensor
<code>%t.rank</code>         | Rank of the tensor
<code>%t.dtype</code>        | Datatype of the tensor
<code>%t.scale</code>        | Quantized scaling parameter (float64)
<code>%t.zp</code>           | Quantized zero-point (int64)
<code>%t.signed</code>       | Boolean indicating the type is signed
<code>%t.num_bits</code>     | Number of bits in the datatype
<code>%t.num_elements</code> | Number of elements in the tensor
<code>%t.type</code>         | Tuple of <code>tensor&lt;%t.shape, %t.dtype&gt;</code>
<code>%t.size</code>         | For tensor lists: the number of tensors in the list</p>
<h3>Tensor Dimension Shorthand</h3>
<p>Where the TOSA Specification allows the use of named dimensions, the following
names may be used.</p>
<p>Name | Description
---- | --------------------
<code>N</code>  | Batch dimension
<code>H</code>  | Height dimension
<code>W</code>  | Width dimension
<code>C</code>  | Channel dimension
<code>M</code>  | Depthwise multiplier</p>
<p>Each of these may be prefixed with <code>I</code> for the input dimension or <code>O</code> for the
output dimension or <code>K</code> for kernel dimensions.</p>
<h2>Common Legalization Functions</h2>
<p>The following pseudocode helper functions are used to cannonicalize arguments
from different frameworks to the TOSA dialect.</p>
<h3>.as_constant(): Matched as Constant</h3>
<p>Wherever %tensor.as_constant() is specified, a constant vector will be created
to hold the value in the %tensor at compile time. This only succeeds if %tensor
is fed by a constant type operator. If constant matching fails, the lowering
will fail and be terminated.</p>
<h2>Common Legalization Functions</h2>
<p>The following pseudo-code helper functions are used to cannonicalize arguments
from different frameworks to the TOSA dialect.</p>
<h3>get_padding_values_from_explicit_pad_attr()</h3>
<p>```
vector<int64> get_padding_values_from_explict_pad_attr(vector<int64> explicit_pad,
                                                         tensorflow::TensorFormat data_format_tf)
{
    int64 pad_before, pad_after
    vector<int64> computed_paddings</p>
<pre><code>for (int32 i = 0; i &lt; 2; i++) {
    int64 dim = GetTensorSpatialDimIndex(4, data_format_tf, i)
    pad_before = explicit_pad[dim * 2]
    pad_after  = explicit_pad[dim * 2 + 1]
    computed_paddings.push_back(pad_before)
    computed_paddings.push_back(pad_after)
}

return computed_paddings
</code></pre>
<p>}
```</p>
<h3>get_padding_values_from_pad_type()</h3>
<p>Calculate explicit padding array based on pad type</p>
<p>```
vector<int64> get_padding_values_from_pad_type(tensorflow::Padding padding, tensorflow::TensorFormat data_format,
                                        uint32 first_filter_spatial_dim, type input_type, type filter_type
                                        vector strides, vector dilations)
{
    assert(padding != tensorflow::Padding::EXPLICIT);</p>
<pre><code>vector&lt;int64&gt; computed_padding;

// Padding over H and W dimensions
for (int32 i = 0; i &lt; 2; i++) {
    int32 ifm_dim = get_tensor_spatial_dim_index(4, data_format, i);

    int32 filter_dim = first_filter_spatial_dim + i;

    int32 dim_dilation = dilations[ifm_dim];
    int32 dim_stride   = strides[ifm_dim];

    int64 op_size, pad_before_tf, pad_after_tf;

    tensorflow::GetWindowedOutputSizeVerboseV2(input_type.shape[ifm_dim], filter_type.shape[filter_dim],
                                               dim_dilation, dim_stride, padding,
                                               // Outputs
                                               &amp;op_size, &amp;pad_before_tf, &amp;pad_after_tf);
    computed_paddings.push_back(pad_before_tf);
    computed_paddings.push_back(pad_after_tf);
}

return computed_paddings;
</code></pre>
<p>}
```</p>
<h3>positive_axis()</h3>
<p>```
// Cannonicalize scalar axis attributes to a scalar positive axis attribute
int32 positive_axis(int32 axis, int32 rank)
{
   if (axis &lt; 0)
       axis += rank;</p>
<p>return axis;
}
```</p>
<h3>compute_scale_32()</h3>
<p>```
void compute_scale_32(float64 scale, int32&amp; multiplier, int32&amp; shift)
{
    /<em> Generates mantissa and shift values where mantissa is in [-1.0,-0.5] or
    [0.5, 1.0] such that
    multiplier = mantissa</em>2^shift */</p>
<pre><code>const float64 mantissa = std::frexp(scale, &amp;shift);
auto shifted_m = std::round(mantissa * (int64(1) &lt;&lt; 31));

assert(shifted_m &lt;= (int64(1) &lt;&lt; 31)); // can't be greater that 1.0
if (shifted_m == (int64(1) &lt;&lt; 31)) {
    shifted_m /= 2;
    shift++;
}
// TOSA expect right shift to be positive, and embed (1 &lt;&lt; 31) into right
// shift bits
shift = (-shift) + 31;

assert(shifted_m &lt;= std::numeric_limits&lt;int32&gt;::max());

multiplier = static_cast&lt;int32&gt;(shifted_m);
</code></pre>
<p>}
```</p>
<h3>lower_batch_to_space_nd_op()</h3>
<p>```
Value lower_batch_to_space_nd_op(Value %input, Value %block_shape, Value %crops, shape_t output_shape)
{</p>
<pre><code>vector &lt;size_t&gt; block_shape(%block_shape.rank)
vector std::pair&lt;size_t, size_t&gt; crops_arr

size_t remaining_shape_rank = %input.rank - %block.rank - 1
size_t crops_dim = %crops.shape[0]

for (int32 i = 0; i &lt; crops_dim; i++) {
    crops[i] = std::make_pair(%crops.as_constant()[i * crops_dim + 0],
                              %crops.as_constant()[i * crops_dim + 1])
}

// Step 1: Reshape input to
// [block_shape[0],
// ...
// [block_shape[M-1],
// [batch / prod(block_shape)]
// [input_shape[1],
// ...
// [input_shape[N-1]

vector &lt;size_t&gt; a1_shape(%block.rank + %input.rank)

for (int32 i = 0; i &lt; %block.rank; i++) {
    a1_shape[i] = %block.shape[i]
}

a1_shape[%block.rank] = %input.shape.[0] / %block.num_elements

for (int32 i = 1; i &lt; %input.rank; i++) {
    a1_shape[i + %block.rank] = %input.shape[i]
}

// Step 2. Permute to shape:
// [ batch / prod(block_shape) ],
// [ input_shape[1] ], [ block_shape[0] ]
//  ...
// [ input_shape[M] ], [ block_shape[M-1]
// + remaining_input_shapes input_shape[M+1 .. N-1]
vector &lt;size_t&gt; a2_perm(%block.rank + %input.rank)

a2_perm[0] = %block.rank
for (int32 i = 0; i &lt; %block.rank; i++) {
    a2_perm[1 + i * 2 + 0] = %block.rank + 1 + i
    a2_perm[1 + i * 2 + 1] = i
}

// Step 3. Reshape to
// [ batch / prod(block_shape) ],
// [input_shape[1] * block_shape[0] ],
//    ..
// [input_shape[M * block_shape[M-1],
// + remaining input shapes [input_shape[M+1.. N-1]]
vector &lt;size_t&gt; a3_shape(%input.rank)

%a3_shape[0] = %input.shape[0] / %block.num_elements
for (int32 i = 0; i &lt; %block.rank; i++) {
    a3_shape[i + 1] = %input.shape[i + 1] * %block.shape[i]
}

for (int32 i = 0; remaining_block_shape; i++) {
    a3_shape[1 + %block.rank + 1] = %input.shape[%block.rank + 1 + i]
}

// Step 4 Crop the start/end dimensions using slice
vector &lt;size_t&gt; a4_begin(%input.rank), a4_size(%input.rank)

for (int32 i = 0; i &lt; %input.rank; i++) {
    if (i == 0 || i &gt; crop_dims) {
       a4_begin[i] = 0
       a4_size[i] = output_shape[i]
    } else {
      a4_begin[i] = %crops[i-1].first
      a4_size[i] = crops[i - 1].first - crops[i - 1].second
    }
}

%a1_reshape = tosa.RESHAPE(%input) {new_shape=a1_shape}
%a2_transpose = tosa.TRANSPOSE(%a1_reshape) {perms=a2_perm}
%a3_reshape = tosa.RESHAPE(%a2_transpose) {new_shape=a3_shape}
%output = tosa.SLICE(%a3_reshape) {begin=a4_begin, size=a4_size}

return %output
</code></pre>
<p>}
```</p>
<h3>lower_concatv2_op()</h3>
<p>```
Value lower_concatv2_op(Type output_type, Value %values, int32 axis)
{
    int32 tosa_axis = positive_axis(axis)</p>
<pre><code>assert(%values.size &gt;= 2)

// Convert scalar inputs to a tensor
if (%values:0.size == 0) {
   for (int32 i = 0; i &lt; %values.size; i++) {
      %values:i = tosa.RESHAPE(%values:i) {new_shape=1}
   }
}

for (int32 i=0; i &lt; %values.size(); i++) {
    %val = %values:i
    if (%val.zp != output_type.zp || %val.scale != output_type.scale) {
        float64 rescale_scale = %val.scale / output_type.scale
        %values:i = tosa.RESCALE(%val) {scale=rescale_scale, input_zp=%values:0.zp, output_zp=output_type.zp}
    }
}

%concat_op = tosa.CONCAT(%values:0, %values:1) {axis=tosa_axis}

for (int32 i = 2; i &lt; %values.size; i++) {
    %concat_op = tosa.CONCAT(%concat_op, %values:i) {axis=tosa_axis}
}

return %concat_op
</code></pre>
<p>}
```</p>
<h3>lower_depth_to_space_op()</h3>
<p>```
Value lower_depth_to_space_op(Value %input, size_t block_size[], Format_t data_format)
{
    assert(data_format == 'NHWC')</p>
<pre><code>vector &lt;size_t&gt; a2_shape = {%input.shape[0],
                            %input.shape[1],
                            %input.shape[2],
                            block_size[0],
                            block_size[1],
                            %input.shape[3] / (block_size[0] * block_size[1])}

vector &lt;size_t&gt; a4_shape = {%input.shape[0],
                            %input.shape[1] * block_size[0],
                            %input.shape[2] * block_size[1],
                            %input.shape[3] / (block_size[0] * block_size[1])}

%a2_reshape = tosa.RESHAPE(%input) {new_shape=a2_shape}
%a3_transpose = tosa.TRANSPOSE(%a2_reshape) {perms={0, 1, 3, 2, 4, 5}}
%output = tosa.RESHAPE(%a3_transpose) {new_shape=a4_shape}

return %output
</code></pre>
<p>}
```</p>
<h3>lower_elu_op()</h3>
<p>```
Value lower_elu_op(Value %value)
{
    // elu(x) = x &lt; 0 ? (exp(x) - 1) : x
    // Create constants for 0/1 and reshape to match the rank
    // of %value
    %one_const = tosa.CONST() {value={1}}
    %zero_const = tosa.CONST() {value={0}}</p>
<pre><code>vector bcast_shape
for (int32 i = 0; i &lt; %value.rank; i++) {
    bcast_shape.push_back(1)
}

%one_reshape = tosa.RESHAPE(%one_const) {new_shape=bcast_shape}
%zero_reshape = tosa.RESHAPE(%zero_const) {new_shape=bcast_shape}

%exp_in = tosa.EXP(%value)
%sub = tosa.SUB(%exp_in, %one_reshape)
%ge  = tosa.GREATER_EQUAL(%value, %zero_reshape)
%output = tosa.SELECT(%ge, %value, %sub)
return %output
</code></pre>
<p>}
```</p>
<h3>lower_expand_dims()</h3>
<p>```
Value lower_expand_dims(Value %input, int32 axis)
{
    vector<size_t> reshape_dims</p>
<pre><code>if (axis &lt; 0 || axis &gt;= %input.rank) {
    // Insert at the end of the tensor
    axis += %input.rank
    for (int32 i = 0; i &lt; input.rank; i++) {
       reshape_dims.push_back(%input.shape[i])
    }
} else {
    for (int32 i= 0 ; i &lt; %input.rank; i++) {
        if (i == axis) {
            reshape_dims.push_back(1)
        }
        reshape_dims.push_back(%input.shape[i])
    }
}

%output = tosa.RESHAPE(%input) {new_shape=reshape_dims}
return %output
</code></pre>
<p>}
```</p>
<h3>lower_fake_quant_op()</h3>
<p>```
Value lower_fake_quant_op(Value %inputs, type output_type, float64 min, float64 max,
                            int64 num_bits, bool narrow_range)
{
    assert(num_bits == 8 || num_bits == 16)</p>
<pre><code>int64 qmax = (1L &lt;&lt; (num_bits - 1)) - 1;
int64 qmin = -(1L &lt;&lt; (num_bits - 1))

if (narrow_range) {
   qmin = qmin + 1
}

float64 scale = (max - min) / float64(qmax - qmin)

int64 zeropoint = (int64)std::round((-min) / scale + float64(qmin))

%quantized = lower_quantize_op(%inputs.type, %inputs, 1.0 / scale, zeropoint)

%dequantized = lower_dequantize_op(output_type, %quantized_op, scale, zeropoint)

return %dequantized
</code></pre>
<p>}
```</p>
<h3>lower_floor_div()</h3>
<p>```
Value lower_floor_div(Value %lhs, Value %rhs)
{
    %recip = tosa.RECIPROCAL(%rhs)
    %mul = tosa.MUL(%lhs, %recip)
    %output = tosa.FLOOR(%mul)</p>
<pre><code>return %output
</code></pre>
<p>}
```</p>
<h3>lower_floor_mod()</h3>
<p><code>Value lower_floor_mod(Value %lhs, Value %rhs)
{
    %recip = tosa.RECIPROCAL(%rhs)
    %mul = tosa.MUL(%lhs, %recip)
    %floor = tosa.FLOOR(%mul)
    %output = tosa.SUB(%mul, %floor)
    return %output
}</code></p>
<h3>lower_quantize_op()</h3>
<p><code>Value lower_quantize_op(Type output_type, Value %input, float64 scale, int64 zeropoint)
{
    %const_scale = tosa.CONST() {value={scale}}
    %const_zp = tosa.CONST() {value={zeropoint}}
    %op1_mul_in_scale = tosa.MUL(%input, %const_scale)
    %op2_add_op1_zp = tosa.ADD(%op1_mul_in_scale, %const_zp)
    %op3_cast_op2 = tosa.CAST(%op2_add_op1_zp) // f32-&gt;%output.dtype
}</code></p>
<h3>lower_dequantize_op()</h3>
<p><code>Value lower_dequantize_op(Value %input, float64 scale, int64 zeropoint)
{
    %const_scale = tosa.CONST() {value={scale}}
    %const_zp = tosa.CONST() {value={(float64)zeropoint}}
    %op1_cast_in = tosa.CAST(%input) // %input.dtype-&gt;f32
    %op2_sub_op1_zp = tosa.SUB(%op1_cast_in, %const_zp)
    %op3_mul_op2_scale = tosa.MUL(%op2_sub_op1_zp, %const_scale)
}</code></p>
<h3>lower_log_softmax_op()</h3>
<p>```
Value lower_log_softmax_op(Value %logits)
{
    %op1 = tosa.EXP(%logits)
    %op2 = tosa.REDUCE_SUM(%op1) {axis=(%logits.rank-1)}
    %op3 = tosa.RECIPROCAL(%op2)
    %op4 = tosa.MUL(%op1, %op3)
    %op5 = tosa.LOG(%op4)</p>
<pre><code>return %op5
</code></pre>
<p>}
```</p>
<h3>lower_pack_op()</h3>
<p>```
Value lower_pack_op(Value %input[], size_t axis)
{
    size_t concat_axis = positive_axis(axis)</p>
<pre><code>size_t input_tensor_rank = %input[0].rank

// Convert any rank 0 to rank 1 with reshape
if (input_tensor_rank == 0) {
   for (int32 i = 0; i &lt; %input.size; i++) {
       %input[i] = tosa.RESHAPE(%input[i], {1})
   }
</code></pre>
<p>}</p>
<p>vector<size_t> output_shape
   for (int32 i = 0; i &lt; input_tensor_rank; i++) {
       output_shape.push_back(%input[0].shape[i]
   }</p>
<p>output_shape[concat_axis] = output_shape[concat_axis] * %input.size</p>
<p>// First pair of tensors
   %concat = tosa.CONCAT(%input[0], %input[1]) {axis=concat_axis}</p>
<p>// Remaining tensors
   for (int32 i = 2; i &lt; %input.size; i++) {
      %concat = tosa.CONCAT(%concat, %input[i]) {axis=concat_axis}
   }</p>
<p>if (input_tensor_rank == 0) {
      // No reshape needed for rank 0, already done
      %output = %concat
   } else</p>
<pre><code>  %reshape = tosa.RESHAPE(%concat) {new_shape=output_shape}

  if (concat_axis == input_tensor_rank) {
     // Output shape is [A, B, C, .. n] in this case,
     // need to reshape to [N, A, B, C, ..] with perm [1, 2, 3, .. 0]
     concat_axis = 0

     vector &lt;size_t&gt; perms
     for (int32 i = 0; i &lt; %input[0].rank; i++)
        perms.push_back(i + 1)
     perms.push_back(0)

     %output = tosa.TRANSPOSE(%reshape) {perms=perms}
 } else {
     %output = %reshape
 }

 return %output
</code></pre>
<p>}
```</p>
<h3>lower_reduce_op()</h3>
<p>```
Value lower_reduce_op<tosa_op_t OP>(Value %input, shape_t output_shape, Value %axes, bool keep_dims, float64 input_scale=1.0f, int32 input_zp=0, float64 output_scale=1.0f, int32 output_zp=0)
{</p>
<pre><code>vector axes_vec = %axes.as_constant();

// Special case of no axes means no transformation
if (axes_vec.size() == 0) {
   return tosa.IDENTITY(%input)
}

bool is_quantized = isa&lt;QuantizedType&gt;(%input.dtype) ? true : false

shape_t shape = %input.shape;
%output = %input;

if (is_quantized) {
    %output = tosa.RESCALE(%output) {scale=input_scale, input_zp=input_zp, output_zp=0}
}

for (int32 i = 0; i &lt; axes_vec.size(); i++) {
    int32 axis = positive_axis(axes_vec[i], %input.rank);

    shape[axis] = 1;
    %output = tosa.OP(%output) {axis=axis}
}

if (!keep_dims) {
   %output = tosa.RESHAPE(%output) {new_shape=output_shape}
}

if (is_quantized) {
    %output = tosa.RESCALE(%output) {scale=output_scale, input_zp=0, output_zp=output_zp}
}

return %output;
</code></pre>
<p>}
```</p>
<h3>lower_resize_op()</h3>
<p>```
Value lower_resize_op(Value %images, Value %size, shape output_shape, dtype output_dtype, mode_t mode)
{
    int32 input_height  = %input.shape[1]
    int32 input_width   = %input.shape[2]
    int32 output_height = %output.shape[1]
    int32 output_width  = %output.shape[2]</p>
<pre><code>float64 in_center_h  = static_cast&lt;float64&gt;(input_height - 1) / 2.0
float64 in_center_w  = static_cast&lt;float64&gt;(input_width - 1) / 2.0
float64 out_center_h = static_cast&lt;float64&gt;(output_height - 1) / 2.0
float64 out_center_w = static_cast&lt;float64&gt;(output_width - 1) / 2.0

float64 fp_stride_y, fp_stride_x
if (align_corner &amp;&amp; output_height &gt; 1)
    fp_stride_y = static_cast&lt;float64&gt;(input_height - 1) / static_cast&lt;float64&gt;(output_height - 1)
else
    fp_stride_y = static_cast&lt;float64&gt;(input_height) / static_cast&lt;float64&gt;(output_height)
if (align_corner &amp;&amp; output_width &gt; 1)
    fp_stride_x = static_cast&lt;float64&gt;(input_width - 1) / static_cast&lt;float64&gt;(output_width - 1)
else
    fp_stride_x = static_cast&lt;float64&gt;(input_width) / static_cast&lt;float64&gt;(output_width)

float64 fp_offset_y = fp_offset_y = 0.0f
if (half_pixel_centers) {
    fp_offset_y = fp_stride_y * 0.5f - 0.5f
    fp_offset_x = fp_stride_x * 0.5f - 0.5f
}

if (dtype == float)
    %op1_resize_in = tosa.RESIZE(%input) {stride={fp_stride_y, fp_stride_x}, offset={fp_offset_y, fp_offset_x}, shift=0, resize_mode=mode}
else {
    int32 shift = 10
    float64 unit = static_cast&lt;float64&gt;(1 &lt;&lt; shift)
    int32 stride_y = fp_stride_y * unit
    int32 stride_x = fp_stride_x * unit
    int32 offset_y = fp_offset_y * unit
    int32 offset_x = fp_offset_x * unit

    %op1_resize_in = tosa.RESIZE(%input) {stride={stride_y, stride_x}, offset={offset_y, offset_x}, shift=shift, resize_mode=mode}

    if (mode == "BILINEAR") {
        %const_zero = tosa.CONST() {value={0}}
        %const_twenty = tosa.CONST() {value={20}}
        %op2_ge_op1 = tosa.GREATER_EQUAL(%op1_resize_in, %const_zero)
        %op3_abs_op1 = tosa.ABS(%op1_resize_in)
        %op4_rshift_op3 = tosa.ARITHMETIC_RIGHT_SHIFT(%op3_abs_op1, %const_twenty)
        %op5_negate_op4 = tosa.NEGATE(%op4_rshift_op3)
        %op6_select_op2_op4_op5 = tosa.SELECT(%op2_ge_op1, %op4_rshift_op3, %op5_negate_op4)
        %op7_cast_op6 = tosa.CAST(%op6_select_op2_op4_op5) // i32/i48-&gt;%output.dtype
    }
}
</code></pre>
<p>}
```</p>
<h3>lower_reversev2_op()</h3>
<p>```
Value lower_reverse_v2_op(Value %tensor, Value %axis)
{
    Value %output = %tensor</p>
<pre><code>if (%axis.num_elements == 0) {
   %output = tosa.IDENTITY(%tensor)
} else {
    for (int32 i = 0; i &lt; %axis.shape[0]; i++) {
        size_t axis_val = positive_axis(%axis.as_constant()[i])
        %output = tosa.REVERSE(%output) {axis=%axis_val}
    }
}

return %output
</code></pre>
<p>}
```</p>
<h3>lower_round_op()</h3>
<p>```
Value lower_round_op(Value %x)
{
    %half = tosa.CONST() {value={0.5}}
    %add = tosa.ADD(%x, %half)
    %output = tosa.FLOOR(%add)</p>
<pre><code>return %output
</code></pre>
<p>}
```</p>
<h3>lower_selectv2_op()</h3>
<p>```
Value lower_selectv2_op(Value %condition, Value %t, Value %e, shape output_shape)
{
    // Reshape condition so that ranks match to support
    // broadcasting (if necessary)</p>
<pre><code>if (%condition.rank != output_shape.size) {
   vector &lt;size_t&gt; cond_shape = %condition.shape
   for (int32 i = 0; i &lt; (output_shape.size - %condition.rank); i++) {
       cond_shape.push_front(1)
   }

   %condition = tosa.RESHAPE(%condition) {new_shape=cond_shape}
}

%output = tosa.SELECT(%condition, %t, %e)

return %output
</code></pre>
<p>}
```</p>
<h3>lower_shape_op()</h3>
<p>```
Value lower_shape_op(Value %input)
{
    vector <size_t> input_shape = %input.shape</p>
<pre><code>%shape = tosa.CONST() {value={input_shape}}
return %shape
</code></pre>
<p>}
```</p>
<h3>lower_space_to_batch_nd_op()</h3>
<p>```
Value lower_space_to_batch_nd_op(Value %input, Value %block_shape, Value %padding)
{</p>
<pre><code>size_t block_rank = %block.shape[0]
size_t remaining_shape_rank = %input.rank - block_rank - 1;

// Step 1. Pad based on paddings operand (flattened representation of [input.rank][2]-shaped array)
vector &lt;size_t&gt; a1_padding
a1_padding[0] = 0
a1_padding[1] = 0

for (int32 i = 0; i &lt; %padding.shape[0]; i++) {
    a1_padding[i + 2] = %padding.as_constant()[i]
}

%a1_pad = tosa.PAD(%input) {padding=a1_padding}

// Step 2. Reshape to
// [batch + padded_shape[1] / block_shape[0], block_shape[0], ...
//    padded_shape[M] / block_shape[M-1], block_shape[M-1]] +
//    remaining_shape

vector &lt;size_t&gt; a2_shape(1 + block_rank * 2 + remaining_shape_rank)
a2_shape[0] = %input.shape[0]
for (int32 i = 0; i &lt; block_rank; i++) {
    a2_shape[1 + i * 2 + 0] = %a1_pad.shape[1 + i] / block_shape.as_constant()[i]
    a2_shape[1 + i * 2 + 1] = block_shape.as_constant()[i]
}

for (int32 i = 0; i &lt; remaining_shape_rank; i++) {
    a2_shape[1 + block_rank * 2 + i] = %input.shape[1 + block_rank + i]
}

%a2_reshape = tosa.RESHAPE(%a1_pad) {new_shape=a2_shape}

// Step 3 transpose to
//  block-shape +
//  [batch] +
//  [padded_shape[1] / block_shape[0],
// ...
//  [padded_shape[M] / block_shape[M-1]] +
//  remaining_shape
vector &lt;size_t&gt; a3_perm(%a2_reshape.rank)
size_t block_num_elems = 1

for (int32 i = 0; i &lt; block_rank; i++) {
    a3_perm[i] = 1 + 2 * i + 1
    a3_perm[block_rank + 1 + i] = 2 * i + 1
    block_num_elems *= %block.as_constant()[i]
}

a3_perm[block_rank] = 0
for (int32 i = (1 + block_rank * 2); i &lt; %a2_reshape.rank; i++) {
    a3_perm[i] = i
}

%a3_reshape = tosa.RESHAPE(%a2_reshape) {perm=a3_perm}

// Step 4. Reshape transposed tensor to
// [ batch * prod(block_shape)] +
// [ padded_shape[1] / block_shape[0],
//   ...,
// padded_shape[M] / block_shape[M-1]] +
// remaining_shape

vector &lt;size_t&gt; a4_shape(%input.rank)
a4_shape[0] = batch_size * block_num_elements

for (int32 i = 0; i &lt; block_rank; i++) {
    a4_shape[i + 1] = %a1_pad.shape[i + 1] / %block.as_constant()[i]
}

for (int32 i = 0; i &lt; remaining_block_shape; i++) {
    a4_shape[1 + block_rank + i] = %input.shape[1 + block_rank + i]
}

%output = tosa.RESHAPE(%a3_reshape) {new_shape=a4_shape}

return %output
</code></pre>
<p>}
```</p>
<h3>lower_space_to_depth_op()</h3>
<p>```
Value lower_space_to_depth_op(Value %input, size_t block_size[], Format_t data_format)
{
    assert(data_format == 'NHWC')</p>
<pre><code>vector &lt;size_t&gt; a2_shape = {%input.shape[0],
                            %input.shape[1] / block_size[0],
                            %block_size[0],
                            %input_shape[2] / block_size[1],
                            %block_size[1],
                            %input_shape[3]}
%a2_reshape = tosa.RESHAPE(%input) {new_shape=a2_shape}
%a3_transpose = tosa.TRANSPOSE(%a2_reshape) {perm={0, 1, 3, 2, 4, 5}}

vector &lt;size_t&gt; a4_shape = {%input.shape[0],
                            %input_shape[1] / block_size[0],
                            %input_shape[2] / block_size[1],
                            %input_shape[3] * block_size[0] * block_size[1]}
%output = tosa.RESHAPE(%a3_transpose) {new_shape=%a4_shape}
return %output
</code></pre>
<p>}
```</p>
<h3>lower_split_op()</h3>
<p>```
Value lower_split_op(Value %value, size_t axis, size_t num_split)
{
    Value %output[]</p>
<pre><code>size_t slice_size = %value.shape[axis] / num_split

for (int32 i = 0; i &lt; num_split; i++) {
    vector &lt;size_t&gt; begin_vals, size_vals

    for (int32 j = 0; j &lt; %value.rank; j++) {
        if (j == axis) {
           begin_vals.push_back(slice_size * i)
           size_vals.push_back(slice_size)
        } else {
           begin_vals.push_back(0)
           size_vals.push_bac(%value.shape[j])
        }

        %output[i] = tosa.SLICE(%value) {start=begin_vals, size=size_vals}
    }

}

%output_list = tosa.IDENTITYN(%output)
return %output_list
</code></pre>
<p>}
```</p>
<h3>lower_splitv_op()</h3>
<p>```
Value lower_splitv_op(Value %value, vector <size_t> size_split, size_t axis)
{
   Value %output[]</p>
<p>size_t curr_split_start = 0</p>
<p>for (int32 i = 0; i &lt; size_split.size(); i++) {
       vector <size_t> begin_vals, size_vals</p>
<pre><code>   for (int32 j = 0; j &lt; %value.rank; j++) {
       if (j == axis) {
          begin_vals.push_back(curr_split_start)
          size_vals.push_back(size_split[i])
       } else {
          begin_vals.push_back(0)
          size_vals.push_back(input.shape[j])
       }
   }

   %output[i] = tosa.SLICE(%value) {start=begin_vals, size=size_vals}

   curr_split_start += size_split[i]
</code></pre>
<p>}</p>
<pre><code>%output_list = tosa.IDENTITYN(%output)
return %output_list
</code></pre>
<p>}
```</p>
<h3>lower_squeeze_op()</h3>
<p>```
Value lower_squeeze_op(Value %input, vector<size_t> squeeze_dims)
{
    vector <size_t> reshape_dims</p>
<pre><code>if (squeeze_dims.size() == 0) {
   // Remove all 1-dims
   for (int32 i = 0; i &lt; %input.rank; i++) {
       if (%input.shape[i] != 1) {
          reshape_dims.push_back(%input_shape[i])
       }
   }
} else {
  // Remove the specified dimensions
  for (int32 i = 0; i &lt; %input.rank; i++) {
      if (!squeeze_dims.find(i) || %input.shape[i] != -1) {
          reshape_dims.push_back(%input_shape[i])
      }
  }
}

%output = tosa.RESHAPE(%input) {new_shape=reshape_dims}

return %output
</code></pre>
<p>}
```</p>
<h3>lower_strided_slice_op()</h3>
<p>```
Value lower_strided_slice_op(Value %input, Value %begin_val, Value %end_val, Value %strides_val,
                               size_t begin_mask, size_t end_mask, size_t ellipsis_mask,
                               size_t new_axis_mask, size_t shrink_axis_mask)
{
    // Note: does not implement ellipsis_mask or reverse stride at this time
    assert(ellipsis_mask == 0)</p>
<pre><code>vector &lt;size_t&gt; begin(%begin_val.as_constant()), end(%end_val.as_constant()), strides(%strides_val.as_constant())
vector &lt;size_t&gt; a1_start, a1_size, a2_shape, a3_start, a3_size, a4_shape

for (int32 i = 0; i &lt; %input.rank; i++) {
    if (begin_mask &amp; (1 &lt;&lt; i)) {
       begin[i] = 0
    }

    if (end_mask &amp; (1 &lt;&lt; i)) {
       end[i] = %input.shape[i]
    }

    // Wrap around index if begin and end are negative
    if (begin[i] &lt; 0) {
       begin[i] += %input.shape[i]
    }

    if (end[i] &lt; 0) {
       end[i] += %input.shape[i]
    }

    a1_start[i] = begin[i]
    a1_size[i] = end[i] - begin[i]

    a2_shape[i*2 + 0] = a1_size[i] / strides[i]
    a2_shape[i*2 + 1] = strides[i]

    a3_start[i*2 + 0] = 0
    a3_start[i*2 + 1] = 0

    if (shrink_axis_mask &amp; (1 &lt;&lt; i)) {
       a3_size[i*2 + 0] = 1
    } else {
       a3_size[i*2 + 0] = a1_size[i] / strides[i]
    }
    a3_size[i*2 + 1] = 1

    if (!(shrink_axis_mask &amp; (1 &lt;&lt; i))) {
       if (new_axis_mask &amp; (1 &lt;&lt; i)) {
          a4_shape.push_back(1)
       a4_shape.push_back((a1_size[i] / strides[i]))
    }
}

// Step 1: Slice the input array
%a1_slice = tosa.SLICE(%input) {start=a1_start, size=a1_size}

// Step 2: Reshape the sliced array: 2x as many dimensions as %input
%a2_reshape = tosa.RESHAPE(%a1_slice) {new_shape=a2_shape}

// Step 3: Take a slice of the [0] index along each of the strided dimensions (even dimensions)
%a3_slice = tosa.SLICE(%a2_reshape) {start=a3_start, size=a3_size}

// Step 4: Reshape the now-strided tensor back down to the desired number of dimensions
%output = tosa.RESHAPE(%a3_slice) {new_shape=a4_shape}

return %output
</code></pre>
<p>}
```</p>
<h3>lower_unpack_op()</h3>
<p>```
Value lower_unpack_op(Value %value, size_t axis, uint64_t num)
{
    axis = positive_axis(axis)</p>
<pre><code>Value %output_arr[]

// Step 1: transpose 'axis' to left-most dimension, if necessary
Value %transposed_value

if (axis != 0) {
   vector &lt;size_t&gt; perms

   perms.push_back(axis)
   for (int32 i = 0; i &lt; %input.rank; i++) {
       if (i != axis)
          perms.push_back(i)
   }

   %transposed_value = tosa.TRANSPOSE(%value) {perms=perms}
</code></pre>
<p>} else {
      %transposed_value = %value
   }</p>
<p>// Step 2: Slice [N, A, B, C] into [N] [A, B, C]
   for (int32 i = 0; i &lt; %transposed_value.rank; i++) {
       vector <size_t> begin_vals, size_vals, shape_vals</p>
<pre><code>   begin_vals.push_back(i)
   size_vals.push_back(1)

   for (int32 j = 1; j &lt; %transposed_value.rank; j++) {
       begin_vals.push_back(0)
       size_vals.push_back(transposed_value.shape[j])
       shape_vals.push_back(transposed_value.shape[j])
   }

   %slice = %tosa.SLICE(%transposed_value) {begin=begin_vals, size=size_vals}
   %output_arr[i] = %tosa.RESHAPE(%slice) {new_shape=shape_vals} {begin=begin_vals, size=size_vals}
</code></pre>
<p>}</p>
<p>// Combine array of sliced tensors into a list of tensors
   %output = tosa.IDENTITYN(%output_arr)
   return %output
}
```</p>
<h3>get_transpose_conv2d_padding_values_from_pad_type()</h3>
<p>```
vector<int64> get_transpose_conv2d_padding_values_from_pad_type(tensorflow::Padding padding, tensorflow::TensorFormat data_format,
                                                         uint32 first_filter_spatial_dim, type input_type, type filter_type
                                                         vector strides, vector dilations)
{
    int64 pad_before, pad_after;
    vector<int64> computed_padding</p>
<pre><code>for (int32 i = 0; i &lt; 2; i++) {
    int64 ifm_dim = GetTensorSpatialDimIndex(4, data_format, i);
    int64 ofm_dim = GetTensorSpatialDimIndex(4, data_format, i);
    int64 filter_dim = first_filter_spatial_dim + 1

    int64 ifm_size = input_shape[ifm_dim]
    int64 ofm_size = output_dims[ofm_dim]
    int64 filter_size = filter.shape[filter_dim]
    int64 dim_dilation = dilations[i]
    int64 dim_stride = strides[i]
    int32 effective_filter_size = (filter_size - 1) * dim_dilation + 1
    int32 total_padding = ((ifm_size - 1) * dim_stride + effective_filter_size - ofm_size)
    total_padding = total_padding &gt; 0 ? total_padding : 0

    pad_before = total_padding / 2
    pad_after = total_padding - pad_before

    computed_padding.push_back(pad_before)
}

return computed_padding
</code></pre>
<p>}
```</p>
<h3>lower_fused_activation()</h3>
<p>```
Value lower_fused_activation(Value %input, string activation)
{
    bool is_quantized = isa<QuantizedType>(%input.dtype) ? true : false</p>
<pre><code>if (is_quantized) {
    if (activation == "NONE") {
        return %input
    }
    else if (activation == "RELU") {
        int32 quantized_0 = %input.zp
        int32 quantized_max = %input.storage_max
        return tosa.CLAMP(%input) {min_int=quantized_0, max_int=quantized_max}
    }
    else if (activation == "RELU6") {
        int32 quantized_0 = %input.zp
        int32 quantized_6 = %input.zp + (6.0 / %input.scale)
        return tosa.CLAMP(%input) {min_int=quantized_0, max_int=quantized_6}
    }
    else if (activation == "RELU_N1_TO_1") {
        int32 quantized_n1 = %input.zp + (-1.0 / %input.scale)
        int32 quantized_1 = %input.zp + (1.0 / %input.scale)
        return tosa.CLAMP(%input) {min_int=quantized_n1, max_int=quantized_1}
    }
}
else {
    if (activation == "NONE") {
        return %input
    }
    else if (activation == "RELU") {
        return tosa.RELUN(%input) {max_fp=numeric_limit&lt;float32&gt;::max()}
    }
    else if (activation == "RELU6") {
        return tosa.RELUN(%input) {max_fp=6.0}
    }
    else if (activation == "RELU_N1_TO_1") {
        return tosa.CLAMP(%input) {min_fp=-1.0, max_fp=1.0}
    }
    else if (activation == "TANH") {
        return tosa.TANH(%input)
    }
}
</code></pre>
<p>}
```</p>
<h3>get_table_const_tensor()</h3>
<p>```
Value get_table_const_tensor(function func)
{
    array<int16, 513> table_array
    for (int32 i = -256; i &lt;= 256; i++) {
        table_array[i] = func(i)
    }</p>
<pre><code>return tosa.CONST() {value=table_array}
</code></pre>
<p>}
```</p>
<h3>lower_gather_op()</h3>
<p>```
Value lower_gather_op(Value %params, Value %indices, int32 batch_dims, int32 axis)
{
    assert batch_dims &lt;= %indices.rank
    assert axis &gt;= batch_dims</p>
<pre><code>int32 N = W = K = C = 1

for (int32 i = 0; i &lt; batch_dims; i++) N *= %params.shape[i]
for (int32 i = batch_dims; i &lt; %indices.rank; i++) W *= %indices.shape[i]
K = %params.shape[axis]
for (int32 i = batch_dims; i &lt; axis; i++) C *= %params.shape[i]
for (int32 i = (axis + 1); i &lt; %params.rank; i++) C *= %params.shape[i]

vector&lt;int32&gt; params_idx_batch, params_idx_left, params_idx_indices, params_idx_right
for (int32 i = 0; i &lt; %params.rank; i++) {
    if (i &lt; batch_dims &amp;&amp; i &lt; axis)
        params_idx_batch.push_back(i)
    else if (i &lt; axis)
        params_idx_left.push_back(i)
    else if (i &lt; (axis + 1))
        params_idx_indices.push_back(i)
    else
        params_idx_right.push_back(i)
}

vector&lt;int32&gt; params_perm = {params_idx_batch, params_idx_left, params_idx_indices, params_idx_right}
vector&lt;int32&gt; result_perm
for (int32 i = 0; i &lt; batch_dims; i++)
    result_perm.push_back(i)
for (int32 i = 0; i &lt; params_idx_left.size(); i++)
    result_perm.push_back(params_idx_left[i])
for (int32 i = batch_dims; i &lt; %indices.rank; i++)
    result_perm.push_back(i)
for (int32 i = 0; i &lt; params_idx_right.size(); i++)
    result_perm.push_back(params_idx_right[i])

%const_params_perm = tosa.CONST() {value=params_perm}
%const_result_perm = tosa.CONST() {value=result_perm}

%op1_transpose_params = tosa.TRANSPOSE(%params, %const_params_perm)
%op2_reshape_op1 = tosa.RESHAPE(%op1_transpose_params) {shape={N,K,C}}
%op3_reshape_indices = tosa.RESHAPE(%indices) {shape={N,W}}
%op4_gather_op2_op3 = tosa.GATHER(%op2_reshape_op1, %op3_reshape_indices)
%op5_reshape_op4 = tosa.RESHAPE(%op4_gather_op2_op3) {shape={N,W,C}}
%op6_transpose_op5 = tosa.TRANSPOSE(%op5_reshape_op4, %const_result_perm)
</code></pre>
<p>}
```</p>
<h3>lower_gather_nd_op()</h3>
<p>```
Value lower_gather_nd_op(Value %params, Value %indices)
{
    int32 N = W = K = C = ND = 1</p>
<pre><code>ND = %indices.shape[%indices.rank - 1]

assert ND &lt; %params.rank

for (int32 i = 0; i &lt; (%indices.rank - 1); i++) W *= %indices.shape[i]
for (int32 i = 0; i &lt; ND; i++) K = %params.shape[i]
for (int32 i = ND; i &lt; %params.rank; i++) C *= %params.shape[i]

vector&lt;int32&gt; flatten_coeff_vec
for (int32 i = 0; i &lt; ND; i++) flatten_coeff_vec.push_back(i)
flatten_coeff_vec.push_back(1)

%const_flatten_coeff = tosa.CONST() {value=flatten_coeff_vec}
%op1_reshape_params = tosa.RESHAPE(%params) {shape={N,K,C}}
%op2_reshape_indices = tosa.RESHAPE(%indices) {shape={W,ND}}
%op3_mul_op2_flatten_coeff = tosa.MUL(%op2_reshape_indices, %const_flatten_coeff)
%op4_rsum_op3 = tosa.REDUCE_SUM(%op3_mul_op2_flatten_coeff) {axis=1}
%op5_reshape_op4 = tosa.RESHAPE(%op4_rsum_op3) {shape={N,W}}
%op6_gather_op1_op5 = tosa.GATHER(%op1_reshape_params, %op5_reshape_op4)
%op7_reshape_op6 = tosa.RESHAPE(%op6_gather_op1_op5) {shape={N,W,C}}
</code></pre>
<p>}
```</p>
<h3>lower_one_hot_op()</h3>
<p>```
Value lower_one_hot_op(Value %indices, Value %depth, Value %on_value, Value %off_value, int32 axis)
{
    int32 N = W = C = 1
    int32 K = %depth.as_constant()
    int32 left_dim = right_dim = 1
    for(int32 i : %indices.rank) {
        int32 dim = %indices.shape[i]
        N <em>= dim
        if (i &gt;= axis)
            right_dim </em>= dim
        else
            left_dim *= dim
    }</p>
<pre><code>%perm_const = tosa.CONST() {value={0, 2, 1}}
%op1_reshape_on_value = tosa.RESHAPE(%on_value) {shape={1, 1, 1}}
%op2_tile_op1 = tosa.TILE(%op1_reshape_on_value) {multiples={N, W, C}}
%op3_reshape_off_value = tosa.RESHAPE(%off_value) {shape={1, 1, 1}}
%op4_tile_op1 = tosa.TILE(%op3_reshape_off_value) {multiples={N, K, C}}
%op5_reshape_indices = tosa.RESHAPE(%indices) {shape={N, W}}
%op6_scatter_op4_op5_op2 = tosa.SCATTER(%op4_tile_op1, %op5_reshape_indices, %op2_tile_op1)
%op7_reshape_op6 = tosa.RESHAPE(%op6_scatter_op4_op5_op2) {shape={left_dim, right_dim, K}}
%op8_transpose_op7 = tosa.TRANSPOSE(%op7_reshape_op6, %perm_const)
%op9_reshape_op8 = tosa.RESHAPE(%op8_transpose_op7) {shape=%output.shape}
</code></pre>
<p>}</p>
<h2>MLIR Passes Management</h2>
<p>Legalization is built on multiple MLIR passes.</p>
<p>| MLIR Pass Name            | Input Dialect | Output Dialect | Description     |
| ------------------------- | ------------- | -------------- | --------------- |
| legalize_tf               | TensorFlow    | TOSA           | Legalize        |
:                           :               :                : TensorFlow      :
:                           :               :                : dialect to TOSA :
:                           :               :                : dialect         :
| fuse_tf_bias              | TensorFlow    | TOSA           | Mapping         |
:                           :               :                : tf.BiasAdd +    :
:                           :               :                : tf.Conv2D to    :
:                           :               :                : tosa.CONV2D     :
| legalize_tfl              | TensorFlow    | TOSA           | Legalize        |
:                           : Lite          :                : TensorFlow Lite :
:                           :               :                : dialect to TOSA :
:                           :               :                : dialect         :
| convert_tfl_uint8         | TensorFlow    | TensorFlow     | Convert         |
:                           : Lite          : Lite           : quantized uint8 :
:                           :               :                : graph to int8   :
:                           :               :                : graph           :</p>
<p>TF to TOSA legalization could be summarized by following pseudocode:</p>
<p>```</p>
<p>void legalize_tf_to_tosa(mlir::Module module) { mlir::PassManager pm</p>
<p>```
// other MLIR passes to optimize TF</p>
<p>pm.addPass(fuse_tf_bias)
pm.addPass(legalize_tf)</p>
<p>// other MLIR passes to optimize TOSA
```</p>
<p>} ```</p>
<p>TFLite to TOSA legalization could be summarized by following pseudocode:</p>
<p>```
void legalize_tfl_to_tosa(mlir::Module module)
{
    mlir::PassManager pm</p>
<pre><code>// other MLIR passes to optimize TFLite

pm.addPass(convert_tfl_uint8)
pm.addPass(legalize_tfl)

// other MLIR passes to optimize TOSA
</code></pre>
<p>}
```</p>
<p>Each of the passes is described in more detail in the subsequent chapters.</p>
<h2>TensorFlow MLIR Dialect Legalization (legalize_tf)</h2>
<h3>tf.Abs</h3>
<p>This operator is trivially lowered to tosa.ABS</p>
<h3>tf.AddN</h3>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.AddN(%inputs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.ADD(%inputs:0, %inputs:1)
for (int32 i = 2; i &lt; %inputs.size; i++) {
    %output = tosa.ADD(%inputs:i, %output)
}</code></p>
<h3>tf.Add</h3>
<p>Element-wise addition.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Add(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.ADD.</p>
<h3>tf.Addv2</h3>
<p>Element-wise addition.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Addv2(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.ADD.</p>
<h3>tf.All</h3>
<p>Computes the "logical and" of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.all(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reduce_op&lt;tosa.REDUCE_ALL&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tf.Any</h3>
<p>Computes the "logical or" of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.any(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reduce_op&lt;tosa.REDUCE_ANY&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tf.ArgMax</h3>
<p>Returns the index with the largest value across the given axis of the input
tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ArgMax(%input, %dimension)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>int64 axis = positive_axis(%dimension)
%output = tosa.ARGMAX(%input) {axis=axis}</code></p>
<h3>tf.ArgMin</h3>
<p>Returns the index with the smallest value across the given axis of the input
tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ArgMin(%input, %dimension)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.Assert</h3>
<p>Asserts that the given condition is true.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Assert(%condition, %summarize)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.AssignAddVariableOp</h3>
<p>Adds a value to the current value of a variable.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.AssignAddVariableOp(%resource, %value, %dtype)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.AssignSubVariableOp</h3>
<p>Subtracts a value to the current value of a variable.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.AssignSubVariableOp(%resource, %value, %dtype)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.AssignVariableOp</h3>
<p>Assigns a new value to a variable.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.AssignVariableOp(%resource, %value, %dtype)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.AvgPool</h3>
<p>Performs average pooling on the input.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.AvgPool(%value) {ksize, strides, padding, data_format}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
assert(data_format == "NHWC")</p>
<p>tosa_padding =
     get_padding_values_from_pad_type(%input, ksize, padding, data_format,
                                      FORMAT_OHWI, strides, {1, 1, 1, 1})
%output = tosa.AVG_POOL2D(%value) {ksize=ksize, strides=strides, padding=tosa_padding}
```</p>
<h3>tf.BatchMatMul</h3>
<p>Multiplies slices of two tensors in batches.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.BatchMatMul(%x, %y, %adj_x, %adj_y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.BatchMatMulV2</h3>
<p>Multiplies slices of two tensors in batches.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.BatchMatMulV2(%x, %y, %adj_x, %adj_y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.BatchNormWithGlobalNormalization</h3>
<p>✗ Deprecated operator.</p>
<h3>tf.BatchToSpaceND</h3>
<p>BatchToSpaceND for N-D tensors of type T.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.BatchToSpaceND(%input, %block_shape, %crops)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_batch_to_space_nd_op(%input, %block_shape, %crops, output.shape)</code></p>
<h3>tf.BiasAddGrad</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.BiasAdd</h3>
<p>Add bias to value.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.BiasAdd(%bias, %value) {data_format}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>assert(data_format == 'NHWC')
%output = tosa.ADD(%value, %bias)</code></p>
<h3>tf.BitCast</h3>
<p>Bitcasts a tensor from one type to another without copying data.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.BitCast(%input, %dtype)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>No TOSA lowering defined.</p>
<h3>tf.BitwiseAnd</h3>
<p>This operator is trivially lowered to tosa.BITWISE_AND.</p>
<h3>tf.BitwiseOr</h3>
<p>This operator is trivially lowered to tosa.BITWISE_OR.</p>
<h3>tf.BroadcastGradientArgs</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.BroadcastTo</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Cast</h3>
<p>This operator is trivially lowered to tosa.CAST.</p>
<h3>tf.Ceil</h3>
<p>This operator is trivially lowered to tosa.CEIL.</p>
<h3>tf.CheckNumerics</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.ComplexAbs</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Complex</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.ConcatOffset</h3>
<p>No TOSA lowering defined. Training profile: TOSA lowering not yet defined.</p>
<h3>tf.Concat</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.ConcatV2</h3>
<p>Concatenates tensors along one dimension.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ConcatV2(%values, %axis)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_concatv2_op(%values, %axis)</code></p>
<h3>tf.Conj</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Const</h3>
<p>This operator is trivially lowered to tosa.CONST.</p>
<h3>tf.Conv2DBackpropFilter</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Conv2DBackpropInput</h3>
<p>Computes the gradients of convolution with respect to the input.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Conv2DBackpropInput(%input_sizes, %filter, %out_backprop) {strides, use_cudnn_on_gpu, padding, explicit_paddings, data_format, dilations}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
// Transpose filter from HWIO to OHWI
%tosa_filter = tosa.TRANSPOSE(%filter) {perms={2, 0, 1, 3}}</p>
<p>vector output_shape</p>
<p>for (int32 i = 0; i &lt; input_sizes.size(); i++) {
   output_shape.push_back(input_size[i])
}</p>
<p>if (%padding == "EXPLICIT") {
   tosa_padding =
       get_padding_values_from_explicit_pad_attr(explict_padding, data_format)
} else {
    tosa_padding =
        get_transpose_conv2d_padding_values_from_pad_type(%input_sizes, %filter, output_shape, padding, data_format, FORMAT_HWIO, strides, dilations)
}</p>
<p>// Create a zero bias tensor
%zero_bias = tosa.CONST() {value={0}}
%output = tosa.TRANSPOSE_CONV2D(%out_backprop) {weight=%tosa_filter, bias=%zero_bias, outpad=tosa_pading, stride=strides, dilation==dilations, out_shape=out_shape}
```</p>
<h3>tf.Conv2D</h3>
<p>Computes a 2-D convolution given 4-D input and filter tensors.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Conv2D(%input, %filter) {strides, padding, explicit_paddings, data_format, dilations}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
assert(data_format == "NHWC")</p>
<p>// Transpose filter from HWIO to OHWI
%filter_tranpose = tosa.TRANSPOSE(%filter {perms={3, 0, 1, 2}}</p>
<p>if (padding == "EXPLICIT") {
   tosa_padding =
       get_padding_values_from_explicit_pad_attr(explict_padding, data_format)
} else {
    %tosa_padding =
        get_padding_values_from_pad_type(%input, %filter.shape, padding, data_format,
                                         FORMAT_HWIO, strides, dilations)
}</p>
<p>// Create a zero bias tensor
%zero_bias = tosa.CONST() {value={0}}</p>
<p>%output = tosa.CONV2D(%input, %filter_transpose, %zero_bias) {padding=tosa_padding, stride=strides, dilation=dilations}
```</p>
<h3>tf.Conv3D</h3>
<p>TOSA lowering to tosa.CONV3D to be defined.</p>
<h3>tf.Cos</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.CrossReplicaSum</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.DepthToSpace</h3>
<p>DepthToSpace for tensors of type T.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.DepthToSpace(%input) {block_size, data_format}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_depth_to_space_op(%input, block_size, data_format)</code></p>
<h3>tf.DepthwiseConv2dNative</h3>
<p>Computes a 2-D depthwise convolution given 4-D input and filter tensors.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.DepthwiseConv2dNative(%input, %filter) {strides, padding, data_format, dilations}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
if (padding == "EXPLICIT") {
   tosa_padding =
       get_padding_values_from_explicit_pad_attr(explict_padding, data_format)
} else {
    tosa_padding =
        get_padding_values_from_pad_type(%input, %filter.shape, padding, data_format,
                                         FORMAT_HWIO, strides, dilations)
}</p>
<p>bias_dim = %filter.shape[2] * %filter.shape[3]</p>
<p>// Create a zero-bias tensor
%zero_bias = tosa.CONST() {value={0} * bias_dim}</p>
<p>%output = tosa.DEPTHWISE_CONV2D(%input, %filter, %zero_bias) {stride=strides, dilation=dilations, padding=padding}
```</p>
<h3>tf.DivNoNan</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Div</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.DynamicStitch</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Einsum</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Elu</h3>
<p>Computes exponential linear: exp(features) - 1 if &lt;0, features otherwise</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Elu(%features)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_elu_op(%features)</code></p>
<h3>tf.EmptyTensorList</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Equal</h3>
<p>Returns the truth value of (x == y) element-wise with broadcasting.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Equal(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.EQUAL.</p>
<h3>tf.Exp</h3>
<p>This operator is trivially lowered to tosa.EXP.</p>
<h3>tf.ExpandDims</h3>
<p>Inserts a dimension of 1 into a tensor’s shape</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ExpandDims(%input, %axis)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_expand_dims(%input, %axis.to_constant())</code></p>
<h3>tf.FakeQuantWithMinMaxArgs</h3>
<p>Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.FakeQuantWithMinMaxArgs(%inputs) {min, max, num_bits, narrow_range}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_fake_quant_op(%inputs, %min, %max, %num_bits, %narrow_range)</code></p>
<h3>tf.FakeQuantWithMinMaxVars</h3>
<p>Fake-quantize the 'inputs' tensor of type float via global flats scalars min.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.FakeQuantWithMinMaxVars(%inputs, %min, %max) {num_bits, narrow_range}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_fake_quant_op(%inputs, %output.type, %min.to_constant(), %max.to_constant(), num_bits, narrow_range)</code></p>
<h3>tf.FakeQuantWithMinMaxVarsPerChannel</h3>
<p>Fake-quantize the 'inputs' tensor of type float and one of the shapes [d].</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.FakeQuantWithMinMaxVarsPerChannel(%inputs, %min, %max) {num_bits, narrow_range}</code></p>
<p>No TOSA lowering defined.</p>
<h3>tf.Fill</h3>
<p>Creates a tensor filled with a scalar value</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Fill(%dims, %value)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
int64 total_size = 1</p>
<p>for (int32 i = 0; i &lt; %dims.shape[0]; i++) {
    total_size *= %dims[i]
}</p>
<p>vector&lt;%value.dtype&gt; fill_arr(total_size, %value)</p>
<p>%output = tosa.CONST() {value={fill_arr}}
```</p>
<h3>tf.FloorDiv</h3>
<p>Returns x // y element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.FloorDiv(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_floor_div(%lhs, %rhs)</code></p>
<h3>tf.FloorMod</h3>
<p>Returns element-wise remainder of division when x &lt; 0 xor x &lt; y is true.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.FloorMod(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_floor_mod(%lhs, %rhs)</code></p>
<h3>tf.Floor</h3>
<p>This operator is trivially lowered to tosa.FLOOR.</p>
<h3>tf.FusedBatchNormGrad</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.FusedBatchNormGradV2</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.FusedBatchNormGradV3</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.FusedBatchNorm</h3>
<p>Batch normalization.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p>```
%output = tf.FusedBatchNorm(%x, %scale, %offset, %mean, %variance) {epsilon, data_format, is_training}</p>
<p>assert(data_format == 'NHWC')
assert(is_training == false)</p>
<p>%epsilon_const = tosa.CONST() {value={epsilon}}</p>
<p>%op1 = tosa.SUB(%x, %bmean)
%op2 = tosa.ADD(%variance, %epsilon_const)
%op3 = tosa.RSQRT(%op2)
%op4 = tosa.MUL(%op1, %op3)
%op5 = tosa.MUL(%op4, %scale)
%output = tosa.ADD(%op5, %offset)
```</p>
<h3>tf.FusedBatchNormV3</h3>
<p>Batch normalization.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.FusedBatchNormV3(%x, %scale, %offset, %mean, %variance) {epsilon, data_format, is_training}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
assert(data_format == 'NHWC')
assert(is_training == false)</p>
<p>%epsilon_const = tosa.CONST() {value={epsilon}}</p>
<p>%op1 = tosa.SUB(%x, %bmean)
%op2 = tosa.ADD(%variance, %epsilon_const)
%op3 = tosa.RSQRT(%op2)
%op4 = tosa.MUL(%mean, %op3)
%op5 = tosa.MUL(%op4, %scale)
%output = tosa.ADD(%op5, %offset)
```</p>
<h3>tf.GatherNd</h3>
<p>Gather slices from params into a Tensor with shape specified by indices.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.GatherNd(%params, %indices)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_gather_nd_op(%params, %indices)</code></p>
<h3>tf.Gather</h3>
<p>Gathers slices from params according to indices.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Gather(%params, %indices)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_gather_op(%params, %indices, 0, 0)</code></p>
<h3>tf.GatherV2</h3>
<p>Gathers slices from params axis according to indices.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.GatherV2(%params, %indices, %axis) {batch_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_gather_op(%params, %indices, batch_dims, %axis.to_constant())</code></p>
<h3>tf.GreaterEqual</h3>
<p>Returns the truth value of (x &gt;= y) element-wise with broadcasting.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.GreaterEqual(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.GREATER_EQUAL.</p>
<h3>tf.Greater</h3>
<p>RetruReturns the truth value of (x &gt; y) element-wise with broadcasting.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Greater(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.GREATER.</p>
<h3>tf.HashTableV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.IdentityN</h3>
<p>Returns a list of tensors with the same shapes and contents as the input.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.IdentityN(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.IDENTITYN(%input)</code></p>
<h3>tf.Identity</h3>
<p>Returns a tensor with the same shape and contents as the input.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Identity(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.IDENTITY(%input)</code></p>
<h3>tf.If</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Imag</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.InfeedDequeueTuple</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Invert</h3>
<p>This operator is trivially lowered to tosa.BITWISE_NOT.</p>
<h3>tf.InvertPermutation</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.IsFinite</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.IteratorGetNext</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.L2Loss</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.LRN</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.LeakyRelu</h3>
<p>Computes rectified linear: max(features, features * alpha).</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LeakyRelu(%features) {alpha}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%alpha_tensor = tosa.CONST() {value={alpha}}
%features_alpha = tosa.MUL(%features, %alpha_tensor)
%greater = tosa.GREATER(%features, %features_alpha)
%output = tosa.SELECT(%greater, %features, %features_alpha)</code></p>
<h3>tf.LeftShift</h3>
<p>Computes the bitwise left-shift of x by y bits, element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LeftShift(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.LOGICAL_LEFT_SHIFT.</p>
<h3>tf.LegacyCall</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.LessEqual</h3>
<p>Returns the truth value of (x ⇐ y) element-wise with broadcasting.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LessEqual(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output_greater = tosa.GREATER(%x, %y)
%output = tosa.LOGICAL_NOT(%output_greater)</code></p>
<h3>tf.Less</h3>
<p>Returns the truth value of (x &lt; y) element-wise with broadcasting.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LessEqual(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output_greater_equal = tosa.GREATER_EQUAL(%x, %y)
%output = tosa.LOGICAL_NOT(%output_greater_equal)</code></p>
<h3>tf.LiNSpace</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Log1p</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Log</h3>
<p>This operator is trivially lowered to tosa.LOG.</p>
<h3>tf.LogSoftmax</h3>
<p>Computes log softmax activations.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LogSoftmax(%logits)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_log_softmax_op(%logits)</code></p>
<h3>tf.LogicalAnd</h3>
<p>Returns the truth value of x AND y, element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LogicalAnd(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.LOGICAL_AND.</p>
<h3>tf.LogicalNot</h3>
<p>This operator is trivially lowered to tosa.LOGICAL_NOT.</p>
<h3>tf.LogicalOr</h3>
<p>Returns the truth value of x OR y, element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LogicalOr(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.LOGICAL_OR.</p>
<h3>tf.LookupTableFindV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.LookupTableInputV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.LookupTableSizeV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MatMul</h3>
<p>Multiply the matrix a by the matrix b</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.MatMul(%a, %b)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.MATMUL(%a, %b)</code></p>
<h3>tf.MatrixDiag</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MatrixDiagV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MatrixDiagV3</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MatrixSetDiag</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MatrixSetDiagV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MatrixSetDiagV3</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Max</h3>
<p>Computes the maximum of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Max(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reduce_op&lt;tosa.REDUCE_MAX&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tf.MaxPoolGrad</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.MaxPool</h3>
<p>Performs max pooling on the input.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.MaxPool(%input) {ksize, strides, padding, data_format}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
assert(data_format == "NHWC")</p>
<p>tosa_padding =
     get_padding_values_from_pad_type(%input, ksize, padding, data_format,
                                      FORMAT_OHWI, strides, {1, 1, 1, 1})
%output = tosa.MAX_POOL2D(%value) {ksize=ksize, strides=strides, padding=tosa_padding}
```</p>
<h3>tf.Maximum</h3>
<p>This operator is trivially lowered to tosa.MAXIMUM.</p>
<h3>tf.Mean</h3>
<p>Computes the mean of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Mean(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
int32 num_elements_on_axis = 1
for (int32 axis : %reduction_indices) {
    num_elements_on_axis *= %input.shape[axis]
}
float32 div_scale = 1.0 / num_elements_on_axis</p>
<p>%cst_div_scale = tosa.CONST() {value={div_scale}}
%op1_rsum_in = lower_reduce_op<tosa.REDUCE_SUM>(%input, %output.shape, %reduction_indices, keep_dims)
%op2_mul_op1 = tosa.MUL(%op1_rsum_in, %cst_div_scale)
```</p>
<h3>tf.Min</h3>
<p>Computes the minimum of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Min(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reduce_op&lt;tosa.REDUCE_MIN&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tf.Minimum</h3>
<p>This operator is trivially lowered to tosa.MAXIMUM.</p>
<h3>tf.MirrorPad</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MlirPassthroughOp</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.MulNoNan</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Mul</h3>
<p>Returns the product of x and y, element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Mul(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong> This operator is trivially lowered to tosa.MUL.</p>
<h3>tf.Neg</h3>
<p>This operator is trivially lowered to tosa.NEGATE.</p>
<h3>tf.NoOp</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.NonMaxSuppressionV4</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.NonMaxSuppressionV5</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.NotEqual</h3>
<p>Returns the truth value of (x != y) element-wise with broadcasting.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.NotEqual(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%equal = tosa.EQUAL(%x, %y)
%output = tosa.NOT(%equal)</code></p>
<h3>tf.OneHot</h3>
<p>OneHot operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tf.OneHot(%indices, %depth, %on_value, %off_value) {axis}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_one_hot_op(%indices, %depth, %on_value, %off_value, axis)</code></p>
<h3>tf.OutputEnqueueTuple</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Pack</h3>
<p>Packs a list of N rank-R tensors into one rank-(R+1) tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Pack(%values) {axis}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_pack_op(%values, axis)</code></p>
<h3>tf.Pad</h3>
<p>This operator is trivially lowered to tosa.PAD.</p>
<h3>tf.PadV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.ParseExampleV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.PartitionedCall</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Placeholder</h3>
<p>Not seen in practice. No lowering needed.</p>
<h3>tf.PlaceholderWithDefault</h3>
<p>Not seen in practice. No lowering needed.</p>
<h3>tf.Pow</h3>
<p>This operator is trivially lowered to tosa.POW.</p>
<h3>tf.PreventGradient</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.Prod</h3>
<p>Computes the product of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Prod(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reduce_op&lt;tosa.REDUCE_PRODUCT&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tf.QuantizeAndDequantize</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.QuantizeAndDequantizeV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.QuantizeAndDequantizeV3</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.RFFT</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.RandomShuffle</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.RandomStandardNormal</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.RandomUniform</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Range</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Rank</h3>
<p>Returns the rank of the tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Rank(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.CONST() {value={%input.rank}}</code></p>
<h3>tf.ReadVariableOp</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.RealDiv</h3>
<p>Returns x / y element-wise for real types.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.RealDiv(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%recip = tosa.RECIPROCAL(%y)
%output = tosa.MUL(%x, %recip)</code></p>
<h3>tf.Real</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Reciprocal</h3>
<p>This operator is trivially lowered to tosa.RECIPROCAL.</p>
<h3>tf.Relu6</h3>
<p>Computes rectified linear 6: min(max(features, 0), 6).</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Relu6(%features)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.RELUN(%features) {max_val=6}</code></p>
<h3>tf.ReluGrad</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.Relu</h3>
<p>Computes rectified linear 6: max(features, 0)</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Relu(%features)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.RELUN(%features) {max_val=0}</code></p>
<h3>tf.Reshape</h3>
<p>Reshapes a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Reshape(%tensor, %shape)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.RESHAPE(%tensor) {new_shape=%shape.as_constant}</code></p>
<h3>tf.ResizeBilinear</h3>
<p>Resizes images to size using bilinear interpolation.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ResizeBilinear(%images, %size) {align_corners, half_pixel_centers}</code></p>
<p>inferred from output shape. <strong>TOSA Lowering</strong></p>
<p><code>%output = lower_resize_op(%images, %size, float, "BILINEAR")</code></p>
<h3>tf.ResizeNearestNeighbor</h3>
<p>Resizes images to size using nearest neighbor interpolation.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ResizeNearestNeighbor(%images, %size) {align_corners, half_pixel_centers}</code></p>
<p>inferred from output shape. <strong>TOSA Lowering</strong></p>
<p><code>%output = lower_resize_op(%images, %size, %output, float, "NEAREST_NEIGHBOR")</code></p>
<h3>tf.ResourceApplyAdam</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.ResourceApplyGradientDescent</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.ResourceApplyKerasMomentum</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.ResourceGather</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.ResourceScatterUpdate</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.ReverseSequence</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.ReverseV2</h3>
<p>Reverses specific dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ReverseV2(%tensor, %axis)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reversev2_op(%tensor, %axis)</code></p>
<h3>tf.RightShift</h3>
<p>Computes the bitwise left-shift of x by y bits, element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.LeftShift(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>if (is_unsigned(%x.dtype)) {
  %output = tosa.LOGICAL_RIGHT_SHIFT(%x, %y)
} else {
  %output = tosa.ARITHMETIC_RIGHT_SHIFT(%x, %y)
}</code></p>
<h3>tf.Round</h3>
<p>Rounds the values of a tensor to the nearest integer, element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Round(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_round_op(%x)</code></p>
<h3>tf.RsqrtGrad</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.Rsqrt</h3>
<p>This operator is trivially lowered to tosa.RSQRT.</p>
<h3>tf.SegmentMax</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SegmentMean</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SegmentMin</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SegmentProd</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SegmentSum</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Select</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SelectV2</h3>
<p>Selects elements from t or e depending on condition.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.SelectV2(%condition, %t, %e)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_selectv2_op(%condition, %t, %e, %output.shape)</code></p>
<h3>tf.ShapeN</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Shape</h3>
<p>Returns the shape of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Shape(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_shape_op(%input)</code></p>
<h3>tf.Sigmoid</h3>
<p>This operator is trivially lowered to tosa.SIGMOID.</p>
<h3>tf.Sign</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Sin</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Size</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Slice</h3>
<p>Returns a slice from input.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Slice(%input, %begin, %size)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>```
vector <size_t> output_size
try {
  output_size = %size.as_constant()
} except(ConversionFailed) {
  output_size = %output.shape
}</p>
<p>%output = tosa.SLICE(%input) {start=begin, size=output_size}
```</p>
<h3>tf.Snapshot</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SoftmaxCrossEntropyWithLogits</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.Softmax</h3>
<p>Computes softmax activations</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Softmax(%logits)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%op1 = tosa.EXP(%logits)
%op2 = tosa.REDUCE_SUM(op1) {reduce_axis=(%logits.rank - 1)}
%op3 = tosa.RECIPROCAL(%op2)
%output = tosa.MUL(%op1, %op3)</code></p>
<h3>tf.Softplus</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SpaceToBatchND</h3>
<p>SpaceToBatch for N-D tensors of type T.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.SpaceToBatchND(%input, %block_shape, %paddings)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_space_to_batch_nd_op(%input, %block_shape, %paddings)</code></p>
<h3>tf.SpaceToDepth</h3>
<p>SpaceToDepth for tensors of type T.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.SpaceToDepth(%input) {block_size, data_format}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_space_to_depth_op(%input, block_size, data_format)</code></p>
<h3>tf.SparseMatMul</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SparseSoftmaxCrossEntropyWithLogits</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.SparseToDense</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Split</h3>
<p>Splits a tensor into num_split tensors along one dimension</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Split(%split_dim, %value) {num_split}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_split_op(%value, %split_dim.as_constant(), num_split)</code></p>
<h3>tf.SplitV</h3>
<p>Splits a tensor into num_split tensors along one dimension</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.SplitV(%value, %size_splits, %split_dim) {num_split}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_splitv_op(%value, %size_splits.as_constant(), %split_dim.as_constant())</code></p>
<h3>tf.Sqrt</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Square</h3>
<p>Computes the square of x, element-wise.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Square(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.MUL(%x, %x)</code></p>
<h3>tf.SquareDifference</h3>
<p>Computes (x-y)*(x-y) element-wise</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.SquareDifference(%x, %y)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%diff = tosa.SUB(%x, %y)
%output = tosa.MUL(%diff, %diff)</code></p>
<h3>tf.Squeeze</h3>
<p>Removes dimensions of size 1 from the shape of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Squeeze(%input) {squeeze_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_squeeze_op(%input, squeeze_dims)</code></p>
<h3>tf.StatefulPartitionedCall</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.StopGradient</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.StridedSliceGrad</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.StridedSlice</h3>
<p>Return a strided slice from input.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.StridedSlice(%input, %begin, %end, %strides) {begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_strided_slice_op(%input, %begin, %end, %strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask)</code></p>
<h3>tf.Sub</h3>
<p>This operator is trivially lowered to tosa.SUB.</p>
<h3>tf.Sum</h3>
<p>Computes the sum of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Sum(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reduce_op&lt;tosa.REDUCE_SUM&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tf.TPUCompilationResult</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUCopyWithLayout</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUExecuteAndUpdateVariables</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUExecute</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUGetLayout</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUReplicateMetadata</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUReplicatedInput</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUReplicatedOutput</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TPUReshardVariables</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TanhGrad</h3>
<p>Training profile: TOSA lowering not yet defined.</p>
<h3>tf.Tanh</h3>
<p>This operator is trivially lowered to tosa.TANH.</p>
<h3>tf.TensorListFromTensor</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorListGetItem</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorListLength</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorListPushBack</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorListReserve</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorListResize</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorListSetItem</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorListStack</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TensorScatterUpdate</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Tile</h3>
<p>Constructs a tensor by tiling a given tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Tile(%input, %multiples)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.TILE(%input) {multiples=%multiples.as_constant()}</code></p>
<h3>tf.ToBool</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.TopKV2</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Transpose</h3>
<p>Shuffle dimensions of x according to a permutation.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Transpose(%x, %perm)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.TRANSPOSE(%x) {perm=%perm.as_constant()}</code></p>
<h3>tf.TruncateDiv</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Unique</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Unpack</h3>
<p>Unpacks a given dimension of a rank-R tensor into num rank-(R-1) tensors.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.Unpack(%value) {axis, num}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_unpack_op(%value, axis, num)</code></p>
<h3>tf.UnsortedSegmentMax</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.UnsortedSegmentMin</h3>
<p>No TOSA lowering defined. === tf.UnsortedSegmentProd</p>
<p>No TOSA lowering defined. === tf.UnsortedSegmentSum</p>
<p>No TOSA lowering defined.</p>
<h3>tf.VarHandle</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.VariableShape</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Where</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.While</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.Xdivy</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.XlaDynamicUpdateSlice</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.XlaSharding</h3>
<p>No TOSA lowering defined.</p>
<h3>tf.ZerosLike</h3>
<p>Returns a tensor of zeros with the same shape and type as x.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.ZerosLike(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.CONST() {value={0} * %x.num_elements}</code></p>
<h2>TensorFlow Lite MLIR Dialect Legalization (legalize_tfl)</h2>
<h3>tfl.abs</h3>
<p>This operator is trivially lowered to tosa.ABS</p>
<h3>tfl.add_n</h3>
<p>add_n operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%sum = tfl.add_n(%inputs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.ADD(%inputs:0, %inputs:1)
for (int32 i = 2 i &lt; %inputs.size i++) {
    %output = tosa.ADD(%inputs:i, %output)
}</code></p>
<h3>tfl.add</h3>
<p>Element-wise addition operation.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.add(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%result = tosa.ADD(%lhs, %rhs)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p>```
float64 max_scale_2x = 2.0 * max(%lhs.scale, %rhs.scale)
float64 lhs_scale = float64(1 &lt;&lt; input_shift) * %lhs.scale / max_scale_2x
float64 rhs_scale = float64(1 &lt;&lt; input_shift) * %rhs.scale / max_scale_2x
float64 output_scale = max_scale_2x / (%output.scale * float64(1 &lt;&lt; input_shift))</p>
<p>```</p>
<p>Legalization:</p>
<p><code>%op1_rescale_lhs = tosa.RESCALE(%lhs) {scale=lhs_scale, input_zp=%lhs.zp, output_zp=0} // %lhs.dtype-&gt;i32
%op2_rescale_rhs = tosa.RESCALE(%rhs) {scale=rhs_scale, input_zp=%rhs.zp, output_zp=0} // %rhs.dtype-&gt;i32
%op3_add_op1_op2 = tosa.ADD(%op1_rescale_lhs, %op2_rescale_rhs)
%op4_rescale_op3 = tosa.RESCALE(%op3_add_op1_op2) {scale=output_scale} // i32-&gt;%output.dtype</code></p>
<h3>tfl.arg_max</h3>
<p>ArgMax operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.arg_max(%input, %dim)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%result = tosa.ARGMAX(%input) {axis=positive_axis(%dim_const.as_constant(), %input.rank)}</code></p>
<h3>tfl.arg_min</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.average_pool_2d</h3>
<p>Average_pool_2d operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.average_pool_2d(%input) {filter_height, filter_width, padding, stride_h, stride_w, fused_activation_function}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Prepare:</p>
<p><code>tosa_padding =
     get_padding_values_from_pad_type(padding, NHWC, 1,
                                      %input.type, tensor&lt;{filter_height, filter_width}, tosa.int32&gt;,
                                      {1, stride_h, stride_w, 1}, {1, 1, 1, 1})</code></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%avgpool2d = tosa.AVG_POOL2D(%input) {kernel={filter_height, filter_width}, stride={stride_h, stride_w}, padding=tosa_padding}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%avgpool2d, fused_activation)
}
else {
    %result = %avgpool2d
}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%avgpool2d = tosa.AVG_POOL2D(%input) {kernel={filter_height, filter_width}, stride={stride_h, stride_w}, padding=tosa_padding, quantization_info={input_zp=%input.zp, output_zp=%output.zp}}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%avgpool2d, fused_activation)
}
else {
    %result = %avgpool2d
}</code></p>
<h3>tfl.basic_lstm</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.batch_to_space_nd</h3>
<p>BatchToSpaceNd operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.batch_to_space_nd(%input, %block_shape, %indices)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%result = convert_batch_to_space_nd_op(%input, %block_shape, %indices)</code></p>
<h3>tfl.cast</h3>
<p>This operator is trivially lowered to tosa.CAST</p>
<h3>tfl.ceil</h3>
<p>Ceil operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.ceil(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p><code>%result = tosa.CEIL(%x)</code></p>
<h3>tfl.concatenation</h3>
<p>Concatenation operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.concatenation(%values) {axis}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%result = lower_concatv2_op(%values, axis)</code></p>
<h3>tfl.pseudo_const</h3>
<p>This operator is trivially lowered to tosa.CONST</p>
<h3>tfl.conv_2d</h3>
<p>Convolution operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.conv_2d(%input, %filter, %bias) {dilation_h_factor, dilation_w_factor, fused_activation_function, padding, stride_h, stride_w}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Prepare:</p>
<p><code>tosa_padding =
     get_padding_values_from_pad_type(padding, NHWC, 1,
                                      %input.type, %filter.type,
                                      {1, stride_h, stride_w, 1}, {1, dilation_h_factor, dilation_w_factor, 1})</code></p>
<p>Legalization:</p>
<p><code>%conv2d = tosa.CONV2D(%input, %filter, %bias) {padding=tosa_padding, stride={stride_h, stride_w}, dilation={dilation_h_factor, dilation_w_factor}}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%conv2d, fused_activation_function)
}
else {
    %result = %conv2d
}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p>```
float64 output_rescale_scale = (%input.scale * %filter.scale) / %output.scale</p>
<p>tosa_padding =
     get_padding_values_from_pad_type(padding, NHWC, 1,
                                      %input.type, %filter.type,
                                      {1, stride_h, stride_w, 1}, {1, dilation_h_factor, dilation_w_factor, 1})
```</p>
<p>Legalization:</p>
<p><code>%conv2d = tosa.CONV2D(%input, %filter, %bias) {padding=tosa_padding, stride={stride_h, stride_w}, dilation={dilation_h_factor, dilation_w_factor}, quantization_info={input_zp=%input.zp, weight_zp=%filter.zp}}
%rescale = tosa.RESCALE(%conv2d) {scale=output_rescale_scale, input_zp=0, output_zp=%output.zp} // %conv2d.dtype-&gt;%output.dtype
if(fused_activation != NONE) {
    %result = convert_fused_activation(%rescale, fused_activation_function)
}
else {
    %result = %rescale
}</code></p>
<h3>tfl.convolution_2d_transpose_bias</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.cos</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.densify</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.depth_to_space</h3>
<p>DepthToSpace operator.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.depth_to_space(%input) {block_size}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_depth_to_space_op(%input, block_size, "NHWC")</code></p>
<h3>tfl.depthwise_conv_2d</h3>
<p>Depthwise-separable convolution operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.depthwise_conv_2d(%input, %filter, %bias) {dilation_h_factor, dilation_w_factor, fused_activation_function, padding, stride_h, stride_w, depth_multiplier}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Prepare:</p>
<p><code>tosa_padding =
     get_padding_values_from_pad_type(padding, NHWC, 1,
                                      %input.type, %filter.type,
                                      {1, stride_h, stride_w, 1}, {1, dilation_h_factor, dilation_w_factor, 1})</code></p>
<p>Legalization:</p>
<p><code>%depthwise_conv2d = tosa.DEPTHWISE_CONV2D(%input, %filter, %bias) {padding=tosa_padding, stride={stride_h, stride_w}, dilation={dilation_h_factor, dilation_w_factor}}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%depthwise_conv2d, fused_activation_function)
}
else {
    %result = %depthwise_conv2d
}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p>```
float64 output_rescale_scale = (%input.scale * %filter.scale) / %output.scale</p>
<p>tosa_padding =
     get_padding_values_from_pad_type(padding, NHWC, 1,
                                      %input.type, %filter.type,
                                      {1, stride_h, stride_w, 1}, {1, dilation_h_factor, dilation_w_factor, 1})
```</p>
<p>Legalization:</p>
<p><code>%depthwise_conv2d = tosa.DEPTHWISE_CONV2D(%input, %filter, %bias) {padding=tosa_padding, stride={stride_h, stride_w}, dilation={dilation_h_factor, dilation_w_factor}, quantization_info={input_zp=%input.zp, weight_zp=%filter.zp}}
%rescale = tosa.RESCALE(%conv2d) {scale=output_rescale_scale, input_zp=0, output_zp=%output.zp} // %depthwise_conv2d.dtype-&gt;%output.dtype
if(fused_activation != NONE) {
    %result = convert_fused_activation(%rescale, fused_activation_function)
}
else {
    %result = %rescale
}</code></p>
<h3>tfl.dequantize</h3>
<p>Dequantize operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.dequantize(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%result = lower_dequantize_op(%input, %input.scale, %input.zp)</code></p>
<h3>tfl.div</h3>
<p>Division operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.div(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p><code>%rcp = tosa.RECIPROCAL(%rhs)
%mul = tosa.MUL(%lhs, %rcp)</code></p>
<h3>tfl.elu</h3>
<p>Exponential Linear Unit operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.elu(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p><code>%rcp = lower_elu_op(%x)</code></p>
<h3>tfl.embedding_lookup</h3>
<p>Embedding lookup operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.embedding_lookup(%lookup, %value)</code></p>
<h3>tfl.equal</h3>
<p>This operator is trivially lowered to tosa.EQUAL</p>
<h3>tfl.exp</h3>
<p>Natural exponentiation operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.exp(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p><code>%result = tosa.EXP(%x)</code></p>
<h3>tfl.expand_dims</h3>
<p>Inserts a dimension of 1 into a tensor’s shape.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.expand_dims(%input, %dim)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%result = lower_expand_dims(%input, %dim.as_constant())</code></p>
<h3>tfl.external_const</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.fake_quant</h3>
<p>FakeQuant operator</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.fake_quant(%input) {min, max, num_bits, narrow_range}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%result = convert_fake_quant_op(%input, min, max, num_bits, narrow_range)</code></p>
<h3>tfl.fill</h3>
<p>Fill the tensor with given value.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%res = tfl.fill(%dims, %value)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Prepare:</p>
<p><code>total_size = 1
dim_vec = %dim.as_constant()
for(int32 i = 0 i &lt; dim_vec.size() i++) {
    total_size *= dim_vec[i]
}
filled_val = %value.as_constant()[0]
output_type = tensor&lt;dim_vec, filled_val.dtype&gt;</code></p>
<p>Legalization:</p>
<p><code>%result = tosa.CONST() {value={filled_val} * total_size}</code></p>
<h3>tfl.floor_div</h3>
<p>Floor div operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.floor_div(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p><code>%recip = tosa.RECIPROCAL(%rhs)
%mul = tosa.MUL(%lhs, %recip)
%result = tosa.FLOOR(%mul)</code></p>
<h3>tfl.floor_mod</h3>
<p>Division remainder.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.floor_mod(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p><code>%recip = tosa.RECIPROCAL(%rhs)
%mul = tosa.MUL(%lhs, %recip)
%floor = tosa.FLOOR(%mul)
%result = tosa.SUB(%mul, %floor)</code></p>
<h3>tfl.floor</h3>
<p>This operator is trivially lowered to tosa.FLOOR</p>
<h3>tfl.fully_connected</h3>
<p>Fully connected op.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.fully_connected(%input, %filter, %bias) {fused_activation_function}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Prepare:</p>
<p><code>// input[N, IC] x filter[OC, IC] + bias[OC] -&gt; output[N, OC]
auto input_reshape_shape = {%input.num_elements / %filter.shape[1], %filter.shape[1]}</code></p>
<p>Legalization:</p>
<p><code>if(!(%bias)) {
    %bias_val = tosa.CONST() {value={0} * %filter.shape[3]}
}
else {
    %bias_val = %bias
}
if(%input.rank != 2) {
    %input_val = tosa.RESHAPE(%input) {shape=input_reshape_shape}
}
else {
    %input_val = %input
}
%fc = tosa.FULLY_CONNECTED(%input_val, %filter, %bias_val)
if(fused_activation != NONE) {
    %result = convert_fused_activation(%fc, fused_activation_function)
}
else {
    %result = %fc
}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p><code>auto input_reshape_shape = {%input.num_elements / %filter.shape[1], %filter.shape[1]}
float64 output_rescale_scale = (%input.scale * %filter.scale) / %output.scale</code></p>
<p>Legalization:</p>
<p><code>if(!(%bias)) {
    %bias_val = tosa.CONST() {value={0} * %filter.shape[3]}
}
else {
    %bias_val = %bias
}
if(%input.rank != 2) {
    %input_val = tosa.RESHAPE(%input) {shape=input_reshape_shape}
}
else {
    %input_val = %input
}
%fc = tosa.FULLY_CONNECTED(%input_val, %filter, %bias_val)
%rescale = tosa.RESCALE(%fc) {scale=output_rescale_scale, input_zp=0, output_zp=%output.zp} // %fc.dtype-&gt;%output.dtype
if(fused_activation != NONE) {
    %result = convert_fused_activation(%rescale, fused_activation_function)
}
else {
    %result = %rescale
}</code></p>
<h3>tfl.gather_nd</h3>
<p>Gather_nd operator.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.gather_nd(%params, %indices)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_gather_nd_op(%params, %indices)</code></p>
<h3>tfl.gather</h3>
<p>Gather operator.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.gather(%params, %indices) {axis}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_gather_op(%params, %indices, 0, axis)</code></p>
<h3>tfl.greater_equal</h3>
<p>This operator is trivially lowered to tosa.GREATER_EQUAL</p>
<h3>tfl.greater</h3>
<p>This operator is trivially lowered to tosa.GREATER</p>
<h3>tfl.hard_swish</h3>
<p>Hardswish activation function.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.hard_swish(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p><code>%const_3 = tosa.CONST() {value={3.0}}
%const_rcp6 = tosa.CONST() {value={1.0 / 6.0}}
%op1_add_in_3 = tosa.ADD(%input, %const_3)
%op2_relun_op1 = tosa.RELUN(%op1_add_in_3) {max=6.0}
%op3_mul_in_op2 = tosa.MUL(%input, %op2_relun_op1)
%op4_mul_op3_rcp6 = tosa.MUL(%op3, %const_rcp6)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p><code>float64 input_sample_grain = 1.0 / 64.0
auto hardswish_func = [input_sample_grain](int32 x) -&gt; int32 {
    float64 v = (float64)x * input_sample_grain
    float64 w = v + 3.0
    w = (w &lt; 0.0) ? 0.0 : ((w &gt; 6.0) ? 6.0 : w)
    v = (v * w) / 6.0
    return std::lround(32768.0 * v)
}
float64 input_rescale_scale = (%input.scale * 128.0) / input_sample_grain
float64 output_rescale_scale = 1.0 / (128.0 * 32768.0 * %output.scale)
int32 quantized_3 = (int32)(std::ceil(3.0 / %input.scale)) + %input.zp</code></p>
<p>Legalization:</p>
<p><code>%table_const = get_table_const_tensor(hardswish_func)
%const_3 = tosa.CONST() {value={quantized_3}}
%op1_rescale_in = tosa.RESCALE(%input) {scale=input_rescale_scale, input_zp=%input.zp, output_zp=0} // %input.dtype-&gt;i16
%op2_table_op1 = tosa.TABLE(%op1_rescale_in, %table_const)
%op3_rescale_op2 = tosa.RESCALE(%op2_table_op1) {scale=output_rescale_scale, input_zp=0, output_zp=%output.zp} // i32-&gt;%output.dtype
%op4_rescale_in = tosa.RESCALE(%input {scale=1.0, input_zp=0, output_zp=0} // %input.dtype-&gt;i32
%op5_ge_op4 = tosa.GREATER_EQUAL(%op4_rescale_in, %const_3)
%op6_select_op5_in_op3 = tosa.SELECT(%op5_ge_op4, %input, %op3_rescale_op2)</code></p>
<h3>tfl.l2_normalization</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.lstm</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.leaky_relu</h3>
<p>Leaky Relu Operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.leaky_relu(%input) {alpha}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%const_0 = tosa.CONST() {value={0.0}}
%const_alpha = tosa.CONST() {value={alpha}}
%op1_mul_in_alpha = tosa.MUL(%input, %const_alpha)
%op2_ge_in_0 = tosa.GREATER_EQUAL(%input, %const_0)
%op3_select_op2_in_op1 = tosa.SELECT(%op2_ge_in_0, %input, $op1_mul_in_alpha)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p><code>float32 scaled_alpha = (%input.scale * alpha) / %output.scale
float32 scaled_identity = %input.scale / %output.scale</code></p>
<p>Legalization:</p>
<p><code>%const_0 = tosa.CONST() {value={0}}
%op1_rescale_in = tosa.RESCALE(%input) {scale=1.0, input_zp=%input.zp} // %input.dtype-&gt;i32
%op2_ge_in_0 = tosa.GREATER_EQUAL(%input, %const_0)
%op3_rescale_in_alpha = tosa.RESCALE(%input) {scale=scaled_alpha, input_zp=%input.zp, output_zp=%output_zp} // %input.dtype-&gt;%output.dtype
%op4_rescale_in_identity = tosa.RESCALE(%input) {scale=scaled_identity, input_zp=%input.zp, output_zp=%output_zp} // %input.dtype-&gt;%output.dtype
%op5_select_op2_op3_op4 = tosa.SELECT(%op2_ge_in_0, %op4_rescale_in_identity, %op3_rescale_in_alpha)</code></p>
<h3>tfl.less_equal</h3>
<p>Less_equal operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.less_equal(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_greater_lhs_rhs = tosa.GREATER(%lhs, %rhs)
%op2_not_op1 = tosa.LOGICAL_NOT(%op1_greater_lhs_rhs)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p>```
assert (%lhs.scale == %rhs.scale) &amp;&amp; (%lhs.zp == %rhs.zp)</p>
<p>%op1_rescale_lhs = tosa.RESCALE(%lhs) {scale=1.0, input_zp=%lhs.zp, output_zp=0} // %lhs.dtype-&gt;i32
%op2_rescale_rhs = tosa.RESCALE(%rhs) {scale=1.0, input_zp=%rhs.zp, output_zp=0} // %rhs.dtype-&gt;i32
%op3_greater_op1_op2 = tosa.GREATER(%op1_rescale_lhs, %op2_rescale_rhs)
%op4_not_op3 = tosa.LOGICAL_NOT(%op3_greater_op1_op2)
```</p>
<h3>tfl.less</h3>
<p>Less operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.less(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_ge_lhs_rhs = tosa.GREATER_EQUAL(%lhs, %rhs)
%op2_not_op1 = tosa.LOGICAL_NOT(%op1_ge_lhs_rhs)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p>```
assert (%lhs.scale == %rhs.scale) &amp;&amp; (%lhs.zp == %rhs.zp)</p>
<p>%op1_rescale_lhs = tosa.RESCALE(%lhs) {scale=1.0, input_zp=%lhs.zp, output_zp=0} // %lhs.dtype-&gt;i32
%op2_rescale_rhs = tosa.RESCALE(%rhs) {scale=1.0, input_zp=%rhs.zp, output_zp=0} // %rhs.dtype-&gt;i32
%op3_ge_op1_op2 = tosa.GREATER_EQUAL(%op1_rescale_lhs, %op2_rescale_rhs)
%op4_not_op3 = tosa.LOGICAL_NOT(%op3_ge_op1_op2)
```</p>
<h3>tfl.local_response_normalization</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.log</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.log_softmax</h3>
<p>Log softmax operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.log_softmax(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%output = lower_log_softmax_op(%logits)</code></p>
<p>No TOSA lowering defined if input/output tensors are all quantized typed.</p>
<h3>tfl.logical_and</h3>
<p>This operator is trivially lowered to tosa.LOGICAL_AND</p>
<h3>tfl.logical_not</h3>
<p>This operator is trivially lowered to tosa.LOGICAL_NOT</p>
<h3>tfl.logical_or</h3>
<p>This operator is trivially lowered to tosa.LOGICAL_OR</p>
<h3>tfl.logistic</h3>
<p>Logistic operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.logistic(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_sigmoid_in = tosa.SIGMOID(%x)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p>```
float64 input_sample_grain = 1.0 / 16.0
auto sigmoid_func = <a href="int32 x">input_sample_grain</a> -&gt; int32 {
  float64 v = static_cast<float64>(x) * input_sample_grain
  v = 1.0 / (1.0 + std::exp(-v))
  return std::lround(32768.0 * v)
}</p>
<p>float32 input_rescale_scale = (%x.scale * 128.0) / input_sample_grain
float32 output_rescale_scale = 1.0 / (%y.scale * 32768.0 * 128.0);
```</p>
<p>Legalization:</p>
<p><code>%table_const = get_table_const_tensor(sigmoid_func)
%op1_rescale_in = tosa.RESCALE(%x) {scale=input_rescale_scale, input_zp=%x.zp, output_zp=0} // %x.dtype-&gt;i16
%op2_table_op1 = tosa.TABLE(%op1_rescale_in, %table_const)
%op3_rescale_op2 = tosa.RESCALE(%op2_table_op1) {scale=output_rescale_scale, input_zp=0, output_zp=%y.zp} // %int32-&gt;%y.dtype</code></p>
<h3>tfl.matrix_diag</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.matrix_set_diag</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.max_pool_2d</h3>
<p>Max Pool 2d op.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.max_pool_2d(%input) {filter_height, filter_width, padding, stride_h, stride_w, fused_activation_function}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Prepare:</p>
<p><code>tosa_padding =
     get_padding_values_from_pad_type(padding, NHWC, 1,
                                      %input.type, tensor&lt;{filter_height, filter_width}, tosa.int32&gt;,
                                      {1, stride_h, stride_w, 1}, {1, 1, 1, 1})</code></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%maxpool2d = tosa.MAX_POOL2D(%input) {kernel={filter_height, filter_width}, stride={stride_h, stride_w}, padding=tosa_padding}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%maxpool2d, fused_activation)
}
else {
    %result = %maxpool2d
}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%maxpool2d = tosa.MAX_POOL2D(%input) {kernel={filter_height, filter_width}, stride={stride_h, stride_w}, padding=tosa_padding, quantization_info={input_zp=%input.zp, output_zp=%output.zp}}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%maxpool2d, fused_activation)
}
else {
    %result = %maxpool2d
}</code></p>
<h3>tfl.max_pooling_with_argmax_2d</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.max_unpooling_2d</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.maximum</h3>
<p>This operator is trivially lowered to tosa.MAXIMUM</p>
<h3>tfl.mean</h3>
<p>Mean operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.mean(%input, %axis) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Prepare:</p>
<p><code>int32 num_elements_on_axis = 1
for (int32 axis : %reduction_indices) {
    num_elements_on_axis *= %input.shape[axis]
}
float32 div_scale = 1.0 / num_elements_on_axis</code></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%cst_div_scale = tosa.CONST() {value={div_scale}}
%op1_rsum_in = lower_reduce_op&lt;tosa.REDUCE_SUM&gt;(%input, %output.shape, %axis, keep_dims)
%op2_mul_op1 = tosa.MUL(%op1_rsum_in, %cst_div_scale)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%rsum = lower_reduce_op&lt;tosa.REDUCE_SUM&gt;(%op1_rescale_in, %output.shape, %reduction_indices, keep_dims, 1.0f, %input_zp, div_scale * %input.scale / %output.scale, %output.zp)</code></p>
<h3>tfl.minimum</h3>
<p>This operator is trivially lowered to tosa.MINIMUM</p>
<h3>tfl.mirror_pad</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.mul</h3>
<p>Mul operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.mul(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_mul_in = tosa.MUL(%lhs, %rhs)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_rescale_lhs = tosa.RESCALE(%lhs) {scale=1.0f, input_zp=%lhs.zp, output_zp=0} // %lhs.dtype-&gt;i32
%op2_rescale_rhs = tosa.RESCALE(%rhs) {scale=1.0f, input_zp=%rhs.zp, output_zp=0} // %rhs.dtype-&gt;i32
%op3_mul_op1_op2 = tosa.MUL(%op1_rescale_lhs, %op2_rescale_rhs)
%op4_rescale_op3 = tosa.RESCALE(%op3_mul_op1_op2) {scale=%lhs.scale * %rhs.scale / %output.scale, input_zp=0, output_zp=%output.zp} // i32-&gt;%output.dtype</code></p>
<h3>tfl.neg</h3>
<p>This operator is trivially lowered to tosa.NEGATE</p>
<h3>tfl.non_max_suppression_v4</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.non_max_suppression_v5</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.not_equal</h3>
<p>Not_equal operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.not_equal(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_equal_lhs_rhs = tosa.EQUAL(%lhs, %rhs)
%op2_not_op1 = tosa.LOGICAL_NOT(%op1_equal_lhs_rhs)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p>```
assert (%lhs.scale == %rhs.scale) &amp;&amp; (%lhs.zp == %rhs.zp)</p>
<p>%op1_rescale_lhs = tosa.RESCALE(%lhs) {scale=1.0f, input_zp=%lhs.zp, output_zp=0} // %lhs.dtype-&gt;i32
%op2_rescale_rhs = tosa.RESCALE(%rhs) {scale=1.0f, input_zp=%rhs.zp, output_zp=0} // %rhs.dtype-&gt;i32
%op3_equal_op1_op2 = tosa.EQUAL(%op1_rescale_lhs, %op2_rescale_rhs)
%op4_not_op3 = tosa.LOGICAL_NOT(%op3_equal_op1_op2) // i32-&gt;%output.dtype
```</p>
<h3>tfl.NumericVerify</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.one_hot</h3>
<p>OneHot operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.one_hot(%indices, %depth, %on_value, %off_value) {axis}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_one_hot_op(%indices, %depth, %on_value, %off_value, axis)</code></p>
<h3>tfl.prelu</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.pack</h3>
<p>Packs a list of tensors along a dimension into one tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tf.pack(%values) {axis}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_pack_op(%values, axis)</code></p>
<h3>tfl.pad</h3>
<p>This operator is trivially lowered to tosa.PAD</p>
<h3>tfl.padv2</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.pow</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.pseudo_qconst</h3>
<p>This operator is trivially lowered to tosa.CONST</p>
<h3>tfl.quantize</h3>
<p>Quantize operator</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.quantize(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Legalization:</p>
<p><code>if (isa&lt;QuantizedType&gt;(%input.dtype)) {
    %op1_rescale_in = tosa.RESCALE(%input) {scale=%input.scale / %output.scale, input_zp=%input.zp, output_zp=%output.zp}
}
else {
    %output = lower_quantize_op(%output.dtype, %input, %output.zp, %output.scale)
}</code></p>
<h3>tfl.range</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.rank</h3>
<p>Rank operator</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.rank(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Legalization:</p>
<p><code>%const = tosa.CONST() {value={%input.rank}}</code></p>
<h3>tfl.reduce_any</h3>
<p>Computes the "logical or" of elements across dimensions of a tensor.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.reduce_any(%input, %reduction_indices) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Legalization:</p>
<p><code>%op1_rsum_in = lower_reduce_op&lt;tosa.REDUCE_ANY&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tfl.reduce_max</h3>
<p>Max-reduction operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.reduce_max(%input, %axes) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Legalization:</p>
<p><code>%op1_rsum_in = lower_reduce_op&lt;tosa.REDUCE_MAX&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tfl.reduce_min</h3>
<p>Computes the min reduction along the specified axes.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.reduce_min(%input, %axes) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Legalization:</p>
<p><code>%op1_rsum_in = lower_reduce_op&lt;tosa.REDUCE_MIN&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tfl.reduce_prod</h3>
<p>Prod-reduction operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.reduce_prod(%input, %axes) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all float typed,</p>
<p>Legalization:</p>
<p><code>%op1_rsum_in = lower_reduce_op&lt;tosa.REDUCE_PROD&gt;(%input, %output.shape, %reduction_indices, keep_dims)</code></p>
<h3>tfl.relu_n1_to_1</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.relu6</h3>
<p>Relu6 operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.relu6(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_relun_in = tosa.RELUN(%input) {max_int=0, max_fp=6.0}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_rescale_in = tosa.RESCALE(%lhs) {scale=%x.scale / %y.scale, input_zp=%x.zp, output_zp=0} // %x.dtype-&gt;i32
%op2_relun_op1 = tosa.RELUN(%op1_rescale_in) {max_int=(6.0 / %y.scale), max_fp=0.0}
%op3_rescale_op2 = tosa.RESCALE(%op2_relun_op1) {scale=1.0, input_zp=0, output_zp=%y.zp // i32-&gt;%y.dtype</code></p>
<h3>tfl.relu</h3>
<p>Relu operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.relu(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_relun_in = tosa.RELUN(%input) {max_int=0, max_fp=std::numeric_limits&lt;float&gt;::max()}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_rescale_in = tosa.RESCALE(%lhs) {scale=%x.scale / %y.scale, input_zp=%x.zp, output_zp=0} // %x.dtype-&gt;i32
%op2_relun_op1 = tosa.RELUN(%op1_rescale_in) {max_int=std::numeric_limits&lt;int32&gt;::max(), max_fp=0.0}
%op3_rescale_op2 = tosa.RESCALE(%op2_relun_op1) {scale=1.0, input_zp=0, output_zp=%y.zp // i32-&gt;%y.dtype</code></p>
<h3>tfl.reshape</h3>
<p>This operator is trivially lowered to tosa.RESHAPE</p>
<h3>tfl.resize_bilinear</h3>
<p>ResizeBilinear Op.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.resize_bilinear(%input, %size) {aligned_corners, half_pixel_centers}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_resize_op(%input, %size, %input.dtype, "BILINEAR")</code></p>
<h3>tfl.resize_nearest_neighbor</h3>
<p>ResizeBilinear Op.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.resize_bilinear(%input, %size) {aligned_corners, half_pixel_centers}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_resize_op(%input, %size, %input.dtype, "NEAREST_NEIGHBOR")</code></p>
<h3>tfl.reverse_sequence</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.reverse_v2</h3>
<p>ReverseV2 Operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.reverse_v2(%input, %axis)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_reversev2_op(%tensor, %axis)</code></p>
<h3>tfl.round</h3>
<p>Round operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.round(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%const_half = tosa.CONST() {value={0.5}}
%op1_add_in_half = tosa.ADD(%input, %const_half)
%op2_floor_op1 = tosa.FLOOR(%op1_add_in_half)</code></p>
<h3>tfl.rsqrt</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.svdf</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.segment_sum</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.select</h3>
<p>This operator is trivially lowered to tosa.SELECT</p>
<h3>tfl.select_v2</h3>
<p>This operator is trivially lowered to tosa.SELECT</p>
<h3>tfl.shape</h3>
<p>Shape operator</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.shape(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Legalization:</p>
<p><code>%const = tosa.CONST() {value=%input.shape}</code></p>
<h3>tfl.sin</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.slice</h3>
<p>This operator is trivially lowered to tosa.SLICE</p>
<h3>tfl.softmax</h3>
<p>Softmax operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.softmax(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_exp_in = tosa.EXP(%input)
%op2_rsum_op1 = tosa.REDUCE_SUM(%op1_exp_in) {axis=(%input.rank-1)}
%op3_rcp_op2 = tosa.RECIPROCAL(%op2)
%op4_mul_op1_op3 = tosa.MUL(%op1, %op3)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p>```
float64 exp_sample_grain = 1.0 / 16.0
auto exp_func = <a href="int32 x">exp_sample_grain</a> -&gt; int32 {
  double v = static_cast<float64>(x) * exp_sample_grain
  v = v &lt; 0.0 ? std::exp(v) : 1.0
  return std::lround(32768.0 * v)
}</p>
<p>float64 one_over_one_plus_x_sample_grain = 1.0 / 256.0
auto one_over_one_plus_x_func = <a href="int32 x">one_over_one_plus_x_sample_grain</a> -&gt; int32 {
  double v = static_cast<float64>(x) * one_over_one_plus_x_sample_grain
  v = v &lt; 0.0 ? 1.0 : 1.0 / (1.0 + v)
  return std::lround(32768.0 * v)
}</p>
<p>float64 op4_rescale_scale = (%input.scale * 128.0) / exp_sample_grain
float64 op19_rescale_scale = 1.0 / (%output.scale * 256.0)
```</p>
<p>Legalization:</p>
<p>```
%const_exp_table = get_table_const_tensor(exp_func)
%const_one_over_one_plus_x_table = get_table_const_tensor(one_over_one_plus_x_func)
%const_3 = tosa.CONST() {value={3}}
%const_34 = tosa.CONST() {value={12+20-8}}
%const_2_to_31 = tosa.CONST() {value={1&lt;&lt;31}}
%const_16 = tosa.CONST() {value={16}}</p>
<p>%op1_rescale_in = tosa.RESCALE(%lhs) {scale=1.0f, input_zp=%x.zp, output_zp=0} // %x.dtype-&gt;i32
%op2_rmax_op1 = tosa.REDUCE_MAX(%op1_rescale_in) {axis=(%input.rank-1)}
%op3_sub_op1_op2 = tosa.SUB(%op1_rescale_in, %op2_relun_op1)
%op4_rescale_op3 = tosa.RESCALE(%op3_sub_op1_op2) {scale=op4_rescale_scale, input_zp=0, output_zp=0} // i32-&gt;i16
%op5_table_op4 = tosa.TABLE(%op4_rescale_op3, %const_exp_table)
%op6_rshift_op5_3 = tosa.ARITHMETIC_RIGHT_SHIFT(%op5_table_op4, %const_3)
%op7_rsum_op6 = tosa.REDUCE_SUM(%op6_rshift_op5_3) {axis=(%input.rank-1)}
%op8_clz_op7 = tosa.CLZ(%op7_rsum_op6)
%op9_sub_34_op8 = tosa.SUB(%const_34, %op8_clz_op7)
%op10_lshift_op7_op8 = tosa.LOGICAL_LEFT_SHIFT(%op7_rsum_op6, %op8_clz_op7)
%op11_sub_op10 = tosa.SUB(%op10_lshift_op7_op8, %const_2_to_31)
%op12_rshift_op11_16 = tosa.ARITHMETIC_RIGHT_SHIFT(%op11_sub_op10, %const_16)
%op13_cast_op12 = tosa.CAST(%op12_rshift_op11_16) // i32-&gt;i16
%op14_table_op13 = tosa.TABLE(%op13_cast_op12, %const_one_over_one_plus_x_table)
%op15_rescale_op14 = tosa.RESCALE(%op14_table_op13) {scale=1.0/128.0, input_zp=0, output_zp=0} // i32-&gt;i16
%op16_rescale_op5 = tosa.RESCALE(%op5_table_op4) {scale=1.0/128.0, input_zp=0, output_zp=0} // i32-&gt;i16
%op17_mul_op16_op15 = tosa.MUL(%op15_rescale_op14, %op16_rescale_op5)
%op18_rshift_op17_op9 = tosa.ARITHMETIC_RIGHT_SHIFT(%op17_mul_op16_op15, %op9_sub_34_op8)
%op19_rescale_op18 = tosa.RESCALE(%op18_rshift_op17_op9) {scale=op19_rescale_scale, input_zp=0, output_zp=%output.zp}
```</p>
<h3>tfl.space_to_batch_nd</h3>
<p>SpaceToBatchNd operator.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.space_to_batch_nd(%input, %block_shape, %paddings)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_space_to_batch_nd_op(%input, %block_shape, %paddings)</code></p>
<h3>tfl.space_to_depth</h3>
<p>SpaceToDepth operator.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.space_to_depth(%input) {block_size}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_space_to_depth_op(%input, block_size, "NHWC")</code></p>
<h3>tfl.pseudo_sparse_const</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.pseudo_sparse_qconst</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.sparse_to_dense</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.split</h3>
<p>Splits a tensor into num_split tensors along one dimension.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.split(%split_dim, %value) {num_split}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_split_op(%value, %split_dim.as_constant(), num_split)</code></p>
<h3>tfl.split_v</h3>
<p>Splits a tensor into num_split tensors along one dimension.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.split_v(%value, %size_splits, %split_dim) {num_splits}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_splitv_op(%value, %size_splits.as_constant(), %split_dim.as_constant())</code></p>
<h3>tfl.sqrt</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.square</h3>
<p>Square operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.square(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_mul_in = tosa.MUL(%x, %x)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_rescale_x = tosa.RESCALE(%x) {scale=1.0f, input_zp=%x.zp, output_zp=0} // %x.dtype-&gt;i32
%op2_mul_op1_op1 = tosa.MUL(%op1_rescale_x, %op1_rescale_x)
%op3_rescale_op2 = tosa.RESCALE(%op2_mul_op1_op1) {scale=%(x.scale * %x.scale) / %output.scale, input_zp=0, output_zp=%y.zp} // i32-&gt;%y.dtype</code></p>
<h3>tfl.squared_difference</h3>
<p>Squared difference operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.squared_difference(%lhs, %rhs)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Legalization:</p>
<p><code>%op1_sub_in = tosa.SUB(%lhs, %rhs)
%op2_mul_op1 = tosa.MUL(%op1_sub_in, %op1_sub_in)</code></p>
<h3>tfl.squeeze</h3>
<p>Removes dimensions of size 1 from the shape of a tensor.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.squeeze(%input) {squeeze_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_squeeze_op(%input, squeeze_dims)</code></p>
<h3>tfl.strided_slice</h3>
<p>StridedSlice Op.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.strided_slice(%input, %begin, %end, %strides) {begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_strided_slice_op(%input, %begin, %end, %strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask)</code></p>
<h3>tfl.sub</h3>
<p>This operator is trivially lowered to tosa.SUB</p>
<h3>tfl.sum</h3>
<p>Sum operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.sum(%input, %axis) {keep_dims}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_rsum_in = lower_reduce_op&lt;tosa.REDUCE_SUM&gt;(%input, %output.shape, %axis, keep_dims)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Legalization:</p>
<p><code>%rsum = lower_reduce_op&lt;tosa.REDUCE_SUM&gt;(%op1_rescale_in, %output.shape, %reduction_indices, keep_dims, 1.0f, %input_zp, (%input.scale / %output.scale), %output.zp)</code></p>
<h3>tfl.tanh</h3>
<p>Hyperbolic tangent operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%y = tfl.tanh(%x)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%op1_tanh_in = tosa.TANH(%x)</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p>```
float64 input_sample_grain = 1.0 / 32.0
auto tanh_func = <a href="int32 x">input_sample_grain</a> -&gt; int32 {
  float64 v = static_cast<float64>(x) * input_sample_grain
  v = std::exp(-2.0 * v)
  v = (1.0 - v) / (1.0 + v)
  return std::lround(32768.0 * v)
}</p>
<p>float32 input_rescale_scale = (%x.scale * 128.0) / input_sample_grain
float32 output_rescale_scale = 1.0 / (%y.scale * 32768.0 * 128.0);
```</p>
<p>Legalization:</p>
<p><code>%table_const = get_table_const_tensor(tanh_func)
%op1_rescale_in = tosa.RESCALE(%x) {scale=input_rescale_scale, input_zp=%x.zp, output_zp=0} // %x.dtype-&gt;i16
%op2_table_op1 = tosa.TABLE(%op1_rescale_in, %table_const)
%op3_rescale_op2 = tosa.RESCALE(%op2_table_op1) {scale=output_rescale_scale, input_zp=0, output_zp=%y.zp} // %int32-&gt;%y.dtype</code></p>
<h3>tfl.tile</h3>
<p>This operator is trivially lowered to tosa.TILE</p>
<h3>tfl.topk_v2</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.transpose_conv</h3>
<p>Transpose convolution operator.</p>
<p><strong>TensorFlow Lite Dialect</strong></p>
<p><code>%output = tfl.transpose_conv(%output_shape, %weights, %input) {padding, stride_h, stride_w}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p>Prepare:</p>
<p><code>tosa_padding =
    get_transpose_conv2d_padding_values_from_pad_type(%input.type, %weights.type, %output_shape, padding, "NHWC", FORMAT_HWIO, {stride_h, stride_w}, {1, 1})</code></p>
<p>If input/output tensors are all non-quantized typed,</p>
<p>Legalization:</p>
<p><code>%bias = tosa.CONST() {value={0.0} * %output.shape[3]}
%conv2d = tosa.TRANSPOSE_CONV2D(%input, %weight, %bias) {padding=tosa_padding, stride={stride_h, stride_w}, dilation={1, 1}}</code></p>
<p>If input/output tensors are all quantized typed,</p>
<p>Prepare:</p>
<p><code>float64 output_rescale_scale = (%input.scale * %weights.scale) / %output.scale</code></p>
<p>Legalization:</p>
<p><code>%bias = tosa.CONST() {value={0} * %output.shape[3]}
%conv2d = tosa.TRANSPOSE_CONV2D(%input, %weight, %bias) {padding=tosa_padding, stride={stride_h, stride_w}, dilation={1, 1}}
%rescale = tosa.RESCALE(%conv2d) {scale=output_rescale_scale, input_zp=0, output_zp=%output.zp} // %conv2d.dtype-&gt;%output.dtype</code></p>
<h3>tfl.transpose</h3>
<p>This operator is trivially lowered to tosa.TRANSPOSE</p>
<h3>tfl.unidirectional_sequence_lstm</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.unidirectional_sequence_rnn</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.unique</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.unpack</h3>
<p>Unpacks a tensor along a dimension into multiple tensors.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.unpack(%input) {num, axis}</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = lower_unpack_op(%input, axis, num)</code></p>
<h3>tfl.where</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.while</h3>
<p>No TOSA lowering defined.</p>
<h3>tfl.yield</h3>
<p>This operator is trivially lowered to tosa.YIELD</p>
<h3>tfl.zeros_like</h3>
<p>ZerosLike operator.</p>
<p><strong>TensorFlow Dialect</strong></p>
<p><code>%output = tfl.zeros_like(%input)</code></p>
<p><strong>TOSA Lowering</strong></p>
<p><code>%output = tosa.CONST() {value={0} * %input.num_elements}</code></p>
<h2>fuse_tf_bias</h2>
<p>Legalize (tf.Conv2D + tf.BiasAdd) to tosa.CONV2D. This is currently the only N:1
mapping in TOSA legalization.</p>
<p>From:</p>
<p><code>%conv2d = tf.Conv2D(%input, %filter) {...}
%bias_add = tf.BiasAdd(%conv2d, %bias)</code></p>
<p>To:</p>
<p><code>%conv2d = tosa.CONV2D(%input, %filter, %bias)</code></p>
<h2>convert_tfl_uint8</h2>
<p>This pass does three things:</p>
<ol>
<li>Convert const from quantized uint8 to quantized int8, with value within
    remapped as well.</li>
<li>If input placeholders is quantized uint8 typed, insert "tosa.RESCALE()
    {scale=1.0, input_zp=input_zp, output_zp=input_zp-128} // qu8-&gt;qi8" in
    between</li>
<li>If output tensor is quantized uint8 typed, insert "tosa.RESCALE()
    {scale=1.0, input_zp=output_zp+128, output_zp=output_zp} // qi8-&gt;qu8" in
    between</li>
</ol>