<h1>Lazy Tensor Tutorial</h1>
<h2>Introduction</h2>
<p>Lazy Tensor is a brand-new tracing system in PyTorch. It includes a safety guarantee not provided by other tracing systems (jit.trace) in that it retraces and recompiles if properties about the input change or uses a cached computation otherwise. It's easier to use than jit.trace and <strong>much</strong> easier to use than jit.script! Lazy Tensor traces both forward and backward passes and removes many Python features present in jit scripted and traced graphs
that are difficult for hardware vendors to support.</p>
<p>Let's kick off our introduction to Lazy Tensor with an example that illustrates the safety guarantee, as it's one of the biggest usability issues of jit.trace. Suppose we'd like to jit trace the following function.</p>
<p>```python
import torch</p>
<p>def add_two_maybe(t: torch.Tensor, maybe: torch.Tensor):
    if maybe:
        return t + 2
    return t
```</p>
<p>You may have noticed that <code>add_two_maybe</code> contains an if statement that depends on <code>maybe</code> input.
Let's jit trace the function with the following inputs.</p>
<p>```python
t = torch.ones(1)
maybe_false = torch.BoolTensor([0])
good_inputs = (t, maybe_false)
jit = torch.jit.trace(add_two_maybe, good_inputs)</p>
<h1>let's check that the results match with eager</h1>
<p>assert jit(<em>good_inputs) == add_two_maybe(</em>good_inputs)
```</p>
<p>So far, so good! We successfully traced <code>add_two_maybe</code> into <code>jit</code> and running it gives us the same result as the original function.</p>
<p>Our troubles start if we change the second input and re-run the traced function.</p>
<p><code>python
maybe_true = torch.BoolTensor([1])
assert jit(t, maybe_true) == add_two_maybe(t, maybe_true)</code></p>
<p><code>shell
Traceback (most recent call last):
  File "/home/villedepommes/github/pytorch4/test/test_tutorial.py", line 27, in &lt;module&gt;
    assert jit(t, maybe_true) == add_two_maybe(t, maybe_true)
AssertionError</code></p>
<p>Uh oh?! What really happened here? Let's print out the graph for <code>jit</code>:</p>
<p>```python</p>
<p>print(torch.jit.last_executed_optimized_graph())</p>
<h1>graph(%t : Tensor,</h1>
<h1>%maybe : Tensor):</h1>
<h1>%2 : Tensor = prim::profile<a href="%t">profiled_type=Float(1, strides=[1], requires_grad=0, device=cpu), seen_none=0</a></h1>
<h1>= prim::profile()</h1>
<h1>return (%2)</h1>
<p>```</p>
<p>We could see that the if statement disappeared and jit trace only traced the <code>else</code> path. In fact, jit trace can trace <strong>only</strong> aten operations. It's completely oblivious to any control flow operations such as <code>if</code>, <code>for</code> or an exception.
If this sounds unsafe to you, that's because it is!</p>
<p>Let's now learn how we can solve this issue with Lazy Tensors.</p>
<p>The first step is to move the inputs to the Lazy device. The Lazy device isn't any real hardware device. Your code still runs either on CPU or on GPU if you set <code>LTC_TS_CUDA="1"</code>.</p>
<p>The lazy device is however very special: it makes PyTorch "remember" every aten operation (into a graph) the user calls rather than eagerly executing it. It's lazy that way ;) get it?</p>
<p>So, the lazy device is an API that users should use to trace their models with Lazy Tensor. It's also a PyTorch device which is a very convenient way for implementing tracing based on PyTorch dispatcher.</p>
<p>First of all, we need a little bit of setup. The Lazy Tensor needs a backend to actually run traced graphs. We implemented a TorchScript-based backend to give our users end-to-end experience running their models with Lazy Tensor. It also serves as an example for hardware vendors looking to integrate with Lazy Tensor.</p>
<p><code>python
import torch._lazy
import torch._lazy.ts_backend
torch._lazy.ts_backend.init()</code></p>
<p>Now, we can run our example,</p>
<p><code>python
dev = "lazy"
t_lazy = torch.ones(1).to(dev)
maybe_false_lazy = torch.BoolTensor([0]).to(dev)
lazy_result = add_two_maybe(t_lazy, maybe_false_lazy)</code></p>
<p>This is pretty cool! Eventually, however, we would still like to execute our computation and access the result, wouldn't we?</p>
<p>There are a few ways to do it. Typically, PyTorch transparently triggers the execution when the user tries to access the result e.g., print a tensor out, move the tensor to a non-lazy device, etc.</p>
<p>Let's give it a try:</p>
<p><code>python
lazy_result = add_two_maybe(t_lazy, maybe_false_lazy)
print(lazy_result)
assert lazy_result.cpu() == add_two_maybe(t, maybe_false)</code></p>
<p>This works as expected! Let's try the case jit trace couldn't handle.</p>
<p><code>python
maybe_true_lazy = torch.BoolTensor([1]).to(dev)
lazy_result = add_two_maybe(t_lazy, maybe_true_lazy)
assert lazy_result.cpu() == add_two_maybe(t, maybe_true)</code></p>
<p>Woo-hoo! This works too!
Unfortunately, this flexibility comes with a few downsides. Remember that backends need to translate aten ops into some much lower-level operations that an accelerator understands. The translation process may be time-consuming. Although, usually, it's well worth it!</p>
<p>However, if a non-trivial model is wildly dynamic and contains loops that always run different number of times or if statements one after another that explode into different traces every time you run the model, the backend will spend non-trivial amount of time compiling each trace even though the latter is used only for a few times.</p>
<p>Alright, at this point, you should have learned the main ideas behind Lazy Tensor, most common usage patterns and APIs.
Also, you are hopefully as inspired and motivated about Lazy Tensor as I am.</p>
<p>Let's see now how we can run a full training loop with an optimizer and backward pass! We will learn a few more important concepts and APIs.</p>
<h2>MNIST MLP</h2>
<p>We will adapt the following example running MNIST_MLP from <a href="https://github.com/pytorch/examples/blob/main/mnist/main.py">pytorch/examples</a></p>
<p>Note, you can access the full version of the script <a href="https://github.com/pytorch/pytorch/blob/master/torch/csrc/lazy/test_mnist.py">here</a></p>
<p>First, we need to install one single dependency, <code>torchvision</code></p>
<p><code>pip install torchvision</code></p>
<p><code>torchvision</code> comes with MNIST dataset w/ images of handwritten digits, which we will be using for training.</p>
<p>Here's our model definition:</p>
<p>```python
class Net(nn.Module):
    def <strong>init</strong>(self):
        super().<strong>init</strong>()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)</p>
<pre><code>def forward(self, x):
    x = self.conv1(x)
    x = F.relu(x)
    x = self.conv2(x)
    x = F.relu(x)
    x = F.max_pool2d(x, 2)
    x = self.dropout1(x)
    x = torch.flatten(x, 1)
    x = self.fc1(x)
    x = F.relu(x)
    x = self.dropout2(x)
    x = self.fc2(x)
    output = F.log_softmax(x, dim=1)
    return output
</code></pre>
<p>```</p>
<p>We are using a multi-level perceptron model with two convolutions, two linear layers and activations sandwiched in between.</p>
<p>Let's set up a loader that would feed the <code>MNIST</code> dataset in <code>train</code> to our model.
We are going to run the training loop for 14 epochs which is what the original MNIST example uses.
<strong>Note, we had to move the model to the Lazy device, <code>Net().to(device)</code>. This is very similar to what we would have done had we been training this model on a GPU.</strong></p>
<p>The rest of the code is pretty standard boilerplate.</p>
<p>```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
import torch._lazy
import torch._lazy.ts_backend
import torch._lazy.metrics
torch._lazy.ts_backend.init()</p>
<p>if <strong>name</strong>  == '<strong>main</strong>':
    bsz = 64
    device = 'lazy'
    epochs = 14
    log_interval = 10
    lr = 1
    gamma = 0.7
    train_kwargs = {'batch_size': bsz}
    # if we want to use CUDA
    if "LTC_TS_CUDA" in os.environ:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True,
                       'batch_size': bsz}
        train_kwargs.update(cuda_kwargs)</p>
<pre><code>transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
    ])
dataset1 = datasets.MNIST('./data', train=True, download=True,
                    transform=transform)
train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
model = Net().to(device)
optimizer = optim.Adadelta(model.parameters(), lr=lr)
scheduler = StepLR(optimizer, step_size=1, gamma=gamma)
for epoch in range(1, epochs + 1):
    train(log_interval, model, device, train_loader, optimizer, epoch)
    scheduler.step()
</code></pre>
<p>```</p>
<p>The training loop in <code>train</code> also has one addition. Namely, <code>torch._lazy.mark_step()</code> which deserves some elaboration on our part. <code>mark_step()</code> instructs Lazy Tensor to break up the current trace and start executing it asynchronously. The current trace encompasses both forward and backward passes and provides the backends with the whole model graph w/o any pythonisms.
If we don't stop the trace after <code>optimizer_step</code> it will include two or more iterations which is way more stuff for the backends to chew through without a whole lot of benefit.</p>
<p>Another important point is that after <code>mark_step()</code> we actually continue tracing the next iteration! And... start executing the previous one at the same time! Really, nothing stops us from tracing the next iteration ...and then the one after next until we hit <code>if batch_idx % log_interval == 0:</code> where
we actually need to wait for execution to catch up, so we can print out <code>loss</code>. Remember to avoid accessing intermediate results too often if you would like to extract the maximum benefit out of Lazy Tensor.</p>
<p>Since every iteration looks exactly like the one before it, the TS backend will be re-using the same TS compilation.</p>
<p>Alright, let's run it now!</p>
<p>```python
def train(log_interval, model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad(set_to_none=True)
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        torch._lazy.mark_step()</p>
<pre><code>    if batch_idx % log_interval == 0:
        print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
            epoch, batch_idx * len(data), len(train_loader.dataset),
            100. * batch_idx / len(train_loader), loss.item()))
</code></pre>
<p>```</p>
<p>After the script downloads the dataset, the model will be trained on the Lazy device as
evidenced by the decreasing loss.</p>
<p>```shell
Train Epoch: 1 [0/60000 (0%)]   Loss: 2.343924
Train Epoch: 1 [640/60000 (1%)] Loss: 1.760821
Train Epoch: 1 [1280/60000 (2%)]        Loss: 0.802798
Train Epoch: 1 [1920/60000 (3%)]        Loss: 0.856164
Train Epoch: 1 [2560/60000 (4%)]        Loss: 0.568396
Train Epoch: 1 [3200/60000 (5%)]        Loss: 0.399044
Train Epoch: 1 [3840/60000 (6%)]        Loss: 0.457996
Train Epoch: 1 [4480/60000 (7%)]        Loss: 0.285104
Train Epoch: 1 [5120/60000 (9%)]        Loss: 0.193083
Train Epoch: 1 [5760/60000 (10%)]       Loss: 0.486165
Train Epoch: 1 [6400/60000 (11%)]       Loss: 0.163996
Train Epoch: 1 [7040/60000 (12%)]       Loss: 0.200323</p>
<p>```</p>
<p>Let's briefly mention a few more APIs before we wrap this up. Unfortunately, LT is still very early in its development which means it doesn't implement every single PyTorch op out of there.
In fact, we implement about a hundred most common ops. What happens if a model contains an op that LT does <strong>not</strong> implement. Lazy Tensor transparently (from a user) breaks up the current trace, waits until all inputs to the op are computed, computes the op on some different device, and finally moves the results onto the lazy device again and starts a new trace.
This big-little wrinkle means that <em>sometimes</em> LT can <strong>not</strong> give the backend a whole model graph which may have a negative impact on performance. You could get the list of the ops that LT could handle for your model by adding the following to your model:</p>
<p><code>python
torch._lazy.metrics.reset()
train(...)
print(torch._lazy.metrics.counter_names())</code></p>
<p>If you are seeing any ops with the prefix: <code>aten::</code></p>
<p><em>Sometimes</em> you could replace such ops with similar that LT does support. More often than not, we will have to just live with it until LT matures.</p>
<p>Another handy API is <code>torch._lazy.wait_device_ops()</code>. Remember, we said that <code>mark_step()</code> breaks up the current trace and kicks off a computation asynchronously? If downstream there are no blocking operations such as <code>print</code>, <code>item()</code>, <code>to</code>, LT will happily continue tracing.
If you would like to time how much exactly time computation and tracing took for some model without including device transfers or printing, you could stick <code>torch._lazy.wait_device_ops()</code> and <code>time.perf_counter()</code> right after it. Don't forget another <code>time.perf_counter()</code> before the trace start!</p>
<p>This concludes our brief introduction to LT. Hopefully, you'll remember the main takeaways:</p>
<ul>
<li>Backends prefer bigger graphs that preferably include both forward and backward as there's ample opportunity for performance optimizations</li>
<li>It's really tricky to produce such graphs without overburdening a user too much. Think, torch.jit.script, torch.jit.trace! Also, think ifs, fors, "Lions, and Tigers, and Bears, Oh My" We digressed.</li>
</ul>
<p>Please give LT a try and tell us what you think on GitHub! We are <strong>eager, not lazy</strong> (haha!) to hear from you!</p>